<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
lang="en"
xmlns="http://www.w3.org/1999/xhtml"
xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Learn Enough Python to Be Dangerous: Software Development, Flask Web Apps, and Beginning Data Science with Python</title>
<link rel="stylesheet" type="text/css" href="override_v1.css"/>
<link rel="stylesheet" type="text/css" href="9780138051051.css"/>
</head>
<body>
<div id="book-content">
<div id="sbo-rt-content"><section epub:type="chapter">
<h2 class="h2" id="ch11"><span epub:type="pagebreak" id="page_319"></span>Chapter 11</h2>
<h2 class="h2a">Data Science</h2>
<p class="noindent">Data science is a rapidly developing field that combines tools from computation and statistics to create insights and draw conclusions from data. That description may sound a little vague, and indeed there is no universally accepted definition of the field; for example, some people think “data science” is just a fancy term for “statistics”, while others hold that statistics is the <em>least</em> important part of data science.</p>
<p class="indent">Luckily, there <em>is</em> broad agreement that Python is an excellent tool for data science, whatever it is exactly.<sup><a id="fn11_1a" href="ch11.xhtml#fn11_1">1</a></sup> There is also a general consensus about which specific Python tools are most useful for the subject. The purpose of this chapter is to introduce some of those tools and use them to investigate some aspects of data science for which Python is especially well-suited.</p>
<p class="footnote"><a id="fn11_1" href="ch11.xhtml#fn11_1a">1.</a> Python’s main open-source rival in this space is R, which was originally developed by statisticians. Python has the advantage of being a general-purpose programming language as well, which is part of why many data scientists have come to prefer it. Nevertheless, R is undeniably powerful, and there are many resources for learning data science that actually cover both Python and R at the same time. I recommend using one of those resources if for any reason it is important for you to know R.</p>
<p class="indent">These subjects include Jupyter notebooks for interactive calculations (<a href="ch11.xhtml#sec11_1">Section 11.1</a>), NumPy for numerical computations (<a href="ch11.xhtml#sec11_2">Section 11.2</a>), Matplotlib for data visualization (<a href="ch11.xhtml#sec11_3">Section 11.3</a>), pandas for data analysis (<a href="ch11.xhtml#sec11_4">Section 11.4</a>, <a href="ch11.xhtml#sec11_5">Section 11.5</a>, and <a href="ch11.xhtml#sec11_6">Section 11.6</a>), and scikit-learn for machine learning (<a href="ch11.xhtml#sec11_7">Section 11.7</a>).<sup><a id="fn11_2a" href="ch11.xhtml#fn11_2">2</a></sup> Almost all other Python data-science tools (such as PySpark, Databricks, and others) also build on the libraries in this chapter.</p>
<p class="footnote"><a id="fn11_2" href="ch11.xhtml#fn11_2a">2.</a> All of these resources are open-source software.</p>
<p class="indent">Data science is far too big to cover in so small a space, but this chapter will give you a great foundation for learning more about the subject. <a href="ch11.xhtml#sec11_8">Section 11.8</a> includes <span epub:type="pagebreak" id="page_320"></span>some suggestions and further resources if you decide you’d like to pursue data science further.</p>
<section>
<h3 class="h3" id="sec11_1">11.1 Data Science Setup</h3>
<p class="noindent">The first step is setting up our environment for doing data-science investigations. Here is an overview of some of the most important tools for Python data science:</p>
<ul class="sq">
<li><p class="bull">IPython and Jupyter: Packages that provide the computational environment in which many Python-using data scientists work.</p></li>
<li><p class="bull">NumPy: A library that makes a variety of mathematical and statistical operations easier; it is also the basis for many features of the pandas library.</p></li>
<li><p class="bull">Matplotlib: A visualization library that makes it quick and easy to generate graphs and charts from our data.</p></li>
<li><p class="bull">pandas: A Python library created specifically to facilitate working with data. This is the bread and butter of a lot of Python data-science work.</p></li>
<li><p class="bull">scikit-learn: Probably the most popular library for machine learning in Python.</p></li>
</ul>
<p class="indent">Because the use of IPython and Jupyter is technically optional, we’ll start by installing the packages that will be needed no matter what your environment looks like. For convenience, I suggest creating a new directory and setting up a fresh virtual environment, as shown in <a href="ch11.xhtml#ch11list1">Listing 11.1</a>.</p>
<p class="listing" id="ch11list1"><strong>Listing 11.1:</strong> Setting up a data-science environment.</p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>$</strong></span> <span class="pd_green">cd</span> ~/repos
<span class="pd_blue"><strong>$</strong></span> mkdir python_data_science
<span class="pd_blue"><strong>$</strong></span> <span class="pd_green">cd</span> python_data_science/
<span class="pd_blue"><strong>$</strong></span> python3 -m venv venv
<span class="pd_blue"><strong>$</strong></span> <span class="pd_green">source</span> venv/bin/activate
<span class="pd_blue"><strong>(venv) $</strong></span></code></pre>
</div>
<p class="noindent">I also recommend putting your project under version control with Git and setting up a remote repository at GitHub or another repository host of your choice. If you go this route, you can use the <span class="pd_green-d"><code><strong>.gitignore</strong></code></span> file shown in <a href="ch11.xhtml#ch11list2">Listing 11.2</a>, which includes an extra line for ignoring unneeded Jupyter changes.</p>
<span epub:type="pagebreak" id="page_321"></span>
<p class="listing" id="ch11list2"><strong>Listing 11.2:</strong> A <span class="pd_green-d"><code><strong>.gitignore</strong></code></span> file for Python data science.<br/><code><em>.gitignore</em></code></p>
<div class="box1">
<pre class="pre"><code>venv/

*.pyc
__pycache__/

instance/

.pytest_cache/
.coverage
htmlcov/

dist/
build/
*.egg-info/

.ipynb_checkpoints

.DS_Store</code></pre>
</div>
<p class="indent">At this point, we’re ready to install the necessary packages. As with the rest of this tutorial, we’ll install exact versions for maximum future compatibility, but feel free to try the latest versions by leaving off the <span class="pd_green-d"><code><strong>==&lt;version number&gt;</strong></code></span> part. Just be prepared for unpredictable results. The full set of necessary packages is shown in <a href="ch11.xhtml#ch11list3">Listing 11.3</a>.</p>
<p class="listing" id="ch11list3"><strong>Listing 11.3:</strong> Installing packages for Python data science.</p>
<p class="codelink"><a href="ch11_images.xhtml#f321-01" id="f321-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>(venv) $</strong></span> pip install <span class="pd_blue">numpy</span><span class="pd_gray">==1</span>.23.3
<span class="pd_blue"><strong>(venv) $</strong></span> pip install <span class="pd_blue">matplotlib</span><span class="pd_gray">==3</span>.6.1
<span class="pd_blue"><strong>(venv) $</strong></span> pip install <span class="pd_blue">pandas</span><span class="pd_gray">==1</span>.5.0
<span class="pd_blue"><strong>(venv) $</strong></span> pip install scikit-learn<span class="pd_gray">==1</span>.1.2</code></pre>
</div>
<p class="indent">We saw as early as <a href="ch01.xhtml#sec1_3">Section 1.3</a> that many Python developers prefer the Conda system for managing packages. If anything, this is even more the case among Python data scientists. But as also noted in <a href="ch01.xhtml#sec1_3">Section 1.3</a>, Conda makes more extensive changes to the environment and is (at least in my experience) harder to reverse or tear down if you need to reset the system. As you gain experience with using Python on your system, I suggest taking another look at Conda to see if it meets your needs.</p>
<p class="indent"><span epub:type="pagebreak" id="page_322"></span>As noted in the introduction, I also highly recommend using Jupyter (pronounced “Jupiter”, like the planet or Roman god),<sup><a id="fn11_3a" href="ch11.xhtml#fn11_3">3</a></sup> which provides a <em>notebook interface</em> to a version of Python, typically a powerful variant known as <em>IPython</em> (Interactive Python). Notebooks consist of <em>cells</em> where you can type and execute code, seeing the results interactively (much like the REPL), which is especially convenient for visualizing plots. (Also like the REPL, Jupyter notebooks are often a good first step toward self-contained Python programs like the ones discussed in previous chapters.) After a while, your notebook might look something like <a href="ch11.xhtml#ch11fig1">Figure 11.1</a>.</p>
<figure class="image-c" id="ch11fig1">
<img src="graphics/11fig01.jpg" alt="images" width="725" height="515"/>
<figcaption>
<p class="title-f"><strong>Figure 11.1:</strong> A working Jupyter notebook.</p>
</figcaption>
</figure>
<p class="footnote"><a id="fn11_3" href="ch11.xhtml#fn11_3a">3.</a> The name is a reference to the three main languages supported by the notebook interface: Julia, Python, and R.</p>
<p class="indent">I suggest installing and using Jupyter via <em>JupyterLab</em>, which conveniently wraps multiple Jupyter notebooks and is also the interface recommended by the Jupyter project itself:</p>
<span epub:type="pagebreak" id="page_323"></span>
<p class="codelink"><a href="ch11_images.xhtml#f323-01" id="f323-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>(venv) $</strong></span> pip install <span class="pd_blue">jupyterlab<span class="pd_gray">==3</span>.4.8</span></code></pre>
<p class="noindent">JupyterLab can be started using the following command:<sup><a id="fn11_4a" href="ch11.xhtml#fn11_4">4</a></sup></p>
<p class="footnote"><a id="fn11_4" href="ch11.xhtml#fn11_4a">4.</a> It is unclear why the library and package are JupyterLab and <span class="pd_green-d"><code><strong>jupyterlab</strong></code></span> (no hyphen) while the commandline command is <span class="pd_green-d"><code><strong>jupyter-lab</strong></code></span> (with a hyphen), but that’s the way it is.</p>
<pre class="pre1"><code><span class="pd_blue"><strong>(venv) $</strong></span> jupyter-lab</code></pre>
<p class="noindent">The result of this is a Jupyter server running on the local system, typically at the address <a href="http://localhost:8889/lab">http://localhost:8889/lab</a> (though details may differ). On my system, the <span class="pd_green-d"><code><strong>jupyterlab</strong></code></span> command automatically spawns a new browser window, with a directory tree and an interface for creating a new notebook (<a href="ch11.xhtml#ch11fig2">Figure 11.2</a>).</p>
<figure class="image-c" id="ch11fig2">
<img src="graphics/11fig02.jpg" alt="images" width="725" height="516"/>
<figcaption>
<p class="title-f"><strong>Figure 11.2:</strong> A directory tree and interface for creating a new notebook.</p>
</figcaption>
</figure>
<p class="indent"><span epub:type="pagebreak" id="page_324"></span>You may also sometimes encounter the “classic” Jupyter interface, which comes from installing the <span class="pd_green-d"><code><strong>jupyter</strong></code></span> package by itself and running <span class="pd_green-d"><code><strong>jupyter notebook</strong></code></span> at the command line (<a href="ch11.xhtml#ch11fig3">Figure 11.3</a>).</p>
<figure class="image-c" id="ch11fig3">
<img src="graphics/11fig03.jpg" alt="images" width="725" height="534"/>
<figcaption>
<p class="title-f"><strong>Figure 11.3:</strong> The “classic” Jupyter interface.</p>
</figcaption>
</figure>
<p class="indent">Each Jupyter notebook runs inside an ordinary web browser and consists of cells of Python code that can be executed using the graphical user interface or (more conveniently) the keyboard shortcut Shift-Return.<sup><a id="fn11_5a" href="ch11.xhtml#fn11_5">5</a></sup> On my system, Jupyter launches in whatever directory I happen to run the <span class="pd_green-d"><code><strong>jupyter-lab</strong></code></span> command in, though this behavior may be system-dependent.</p>
<p class="footnote"><a id="fn11_5" href="ch11.xhtml#fn11_5a">5.</a> Users of <em>Mathematica</em>, from which Jupyter draws heavy inspiration, will find this notebook interface especially familiar.</p>
<p class="indent">By the way, Jupyter doesn’t autoreload modules by default, which can be annoying. The following code can be used to change this default behavior:</p>
<pre class="pre1"><code><span class="pd_gray">%</span>load_ext autoreload
<span class="pd_gray">%</span>autoreload <span class="pd_gray">2</span></code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_325"></span>Throughout the rest of this chapter, we’ll mainly be using examples from the Python prompt because I don’t want to assume you’ve installed Jupyter.<sup><a id="fn11_6a" href="ch11.xhtml#fn11_6">6</a></sup> That being said, I strongly recommend installing and learning Jupyter at some point since it is a standard tool in Python data analysis and scientific computing. In particular, Jupyter can be used on the cloud IDE recommended in <em>Learn Enough Dev Environment to Be Dangerous</em> (<a href="https://www.learnenough.com/dev-environment">https://www.learnenough.com/dev-environment</a>) by following the steps in <a href="ch11.xhtml#ch11box1">Box 11.1</a>. Another option is CoCalc, a commercial service that supports Jupyter notebooks by default.</p>
<p class="footnote"><a id="fn11_6" href="ch11.xhtml#fn11_6a">6.</a> Because the notebook interface is so instructive when used interactively, the videos that accompany this book <em>do</em> make use of Jupyter (or, more specifically, JupyterLab).</p>
<div class="box">
<p class="box-title" id="ch11box1"><strong>Box 11.1: Running Jupyter on the cloud IDE</strong></p>
<p class="sb-noindent">Perhaps surprisingly, it’s possible to get Jupyter notebooks to work on the cloud IDE recommended in <em>Learn Enough Dev Environment to Be Dangerous</em>. (At least, it was surprising to me.) The first step is to generate a configuration file as follows (be sure to run this and all commands in the <code>jupyter_data_science</code> directory created in <a href="ch11.xhtml#ch11list1">Listing 11.1</a> and inside a virtual environment):</p>
<p class="codelink"><a href="ch11_images.xhtml#f325-01" id="f325-01a">Click here to view code image</a></p>
<pre class="pre1-b"><code>$ jupyter notebook --generate-config</code></pre>
<p class="sb-noindent">This command generates a file in the <code>.jupyter</code> hidden directory under the home directory:</p>
<p class="codelink"><a href="ch11_images.xhtml#f325-02" id="f325-02a">Click here to view code image</a></p>
<pre class="pre1-b"><code>~/.jupyter/jupyter_notebook_config.py</code></pre>
<p class="sb-noindent">Using a text editor such as <code>nano</code>, <code>vim</code>, or <code>c9</code> (the last one can be installed via <code>npm install --location=global c9</code>), include the following lines at the bottom of <code>jupyter_notebook_config.py</code>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f325-03" id="f325-03a">Click here to view code image</a></p>
<pre class="pre1-b"><code>c.NotebookApp.allow_origin = "*"
c.NotebookApp.ip = "0.0.0.0"
c.NotebookApp.allow_remote_access = True</code></pre>
<p class="sb-noindent">At this point, you should be ready to run</p>
<p class="codelink"><a href="ch11_images.xhtml#f325-04" id="f325-04a">Click here to view code image</a></p>
<pre class="pre1-b"><code>$ jupyter-lab --port $PORT --ip $IP --no-browser</code></pre>
<p class="sb-noindent">at the command line.</p>
<p class="sb-noindent"><span epub:type="pagebreak" id="page_326"></span>To view the notebook, use the menu item Preview &gt; Preview Running Application. You might have to click the Pop Out Into New Window icon in the upper right of the window pane. You will probably be prompted for a token, which can be found in the output of the <code>jupyter-lab</code> command and should look something like this:</p>
<p class="codelink"><a href="ch11_images.xhtml#f326-01" id="f326-01a">Click here to view code image</a></p>
<pre class="pre1-b"><code>http://127.0.0.1:8080/?token=c33a7633b81ad52fc81</code></pre>
<p class="sb-noindent">Copy and paste the unique token for your application (i.e., everything after token=) to get access to the page. The result should be a Jupyter notebook running on the cloud IDE (<a href="ch11.xhtml#ch11fig4">Figure 11.4</a>).</p>
<figure class="image-c" id="ch11fig4">
<img src="graphics/11fig04.jpg" alt="images" width="725" height="534"/>
<figcaption>
<p class="title-f"><strong>Figure 11.4:</strong> A Jupyter notebook on the cloud IDE.</p>
</figcaption>
</figure>
</div>
<span epub:type="pagebreak" id="page_327"></span>
</section>
<section>
<h3 class="h3" id="sec11_2">11.2 Numerical Computations with NumPy</h3>
<p class="noindent">Although Python has a reputation for being a “slow language”, in fact Python is written in C, one of the fastest languages in existence. The occasional slowness of Python is mostly a consequence of the things that also make it dynamic, which often involves layers of abstraction above the underlying C code. The NumPy library makes the underlying speed directly available to numerical computations by rewriting the most time-intensive parts directly in C.</p>
<p class="indent">NumPy (pronounced “NUM-pie”, for “Numerical Python”) was originally part of the large and powerful SciPy (“SIE-pie”) library for scientific computing in Python but was factored out as a separate library because of its broad applicability. Indeed, data science is a great example: The core Python data-science library, pandas (<a href="ch11.xhtml#sec11_4">Section 11.4</a>), doesn’t need SciPy but relies heavily on NumPy for numerical computations. As a result, although a complete mastery of NumPy isn’t necessary for data science, it’s important to at least know the basics.</p>
<p class="indent">Once NumPy has been installed (<a href="ch11.xhtml#ch11list3">Listing 11.3</a>), it can be included in a program, in the REPL, or in a Jupyter notebook as usual using <span class="pd_green-d"><code><strong>import</strong></code></span>. The near-universal convention in data science and closely related communities is to import <span class="pd_green-d"><code><strong>numpy</strong></code></span> as <span class="pd_green-d"><code><strong>np</strong></code></span> for convenience:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>import</strong></span> <span class="pd_nila"><strong>numpy</strong></span> <span class="pd_green"><strong>as</strong></span> <span class="pd_nila"><strong>np</strong></span></code></pre>
<p class="noindent">(Most of the examples in this chapter include the REPL prompt <span class="pd_green-d"><code><strong>&gt;&gt;&gt;</strong></code></span>, but if you use Jupyter notebooks no prompt will be present, as seen in, e.g., <a href="ch11.xhtml#ch11fig1">Figure 11.1</a>.)</p>
<section>
<h4 class="h4" id="sec11_2_1">11.2.1 Arrays</h4>
<p class="noindent">The combination SciPy + NumPy + Matplotlib (<a href="ch11.xhtml#sec11_3">Section 11.3</a>) represents an open-source alternative to the proprietary MATLAB system. Like MATLAB, NumPy is array-based, with the core data structure being an <em>ndarray</em> (short for “<em>n</em>-dimensional array”):</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>array([<span class="pd_gray">1</span>, <span class="pd_gray">2</span>, <span class="pd_gray">3</span>])
<span class="pd_green1-d">array([1, 2, 3])</span></code></pre>
<p class="indent"><span epub:type="pagebreak" id="page_328"></span>NumPy ndarrays share many properties with regular Python lists (<a href="ch03.xhtml#ch03">Chapter 3</a>):</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a <span class="pd_gray">=</span> np<span class="pd_gray">.</span>array([<span class="pd_gray">1</span>, <span class="pd_gray">3</span>, <span class="pd_gray">2</span>])
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green">len</span>(a)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a<span class="pd_gray">.</span>sort()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a
<span class="pd_green-d">array([1, 2, 3])</span></code></pre>
<p class="noindent">In analogy with the list <span class="pd_green-d"><code><strong>range()</strong></code></span> function (first seen in <a href="ch02.xhtml#ch2list24">Listing 2.24</a>), we can create array ranges using <span class="pd_green-d"><code><strong>arange()</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f328-01" id="f328-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> r <span class="pd_gray">=</span> <span class="pd_green">rang</span>e(<span class="pd_gray">17</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> r
<span class="pd_green-d">range(0, 17)</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green">list</span>(r)
<span class="pd_green-d">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a <span class="pd_gray">=</span> np<span class="pd_gray">.</span>arange(<span class="pd_gray">17</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a
<span class="pd_green-d">array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])</span></code></pre>
<p class="indent">These similarities raise the question of why we can’t just use lists when doing data science with Python. The answer is that computations with arrays are much faster than the corresponding operations with lists. Because NumPy itself is array-based, such computations can also typically be expressed much more compactly, without the need for loops or even comprehensions.</p>
<p class="indent">In particular, NumPy arrays support <em>vectorized operations</em>, whereby we can (say) multiply every element in an array by a particular number all at once. For example, to create a list multiplying each element in a range by <span class="pd_green-d"><code><strong>3</strong></code></span>, we could use a list comprehension (<a href="ch06.xhtml#sec6_1">Section 6.1</a>) as follows:</p>
<p class="codelink"><a href="ch11_images.xhtml#f328-02" id="f328-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> [<span class="pd_gray">3 *</span> i <span class="pd_green"><strong>for</strong></span> i <span class="pd_pink"><strong>in</strong></span> r]
<span class="pd_green-d">[0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48]</span></code></pre>
<p class="noindent">With a NumPy ndarray, we can just multiply by <span class="pd_green-d"><code><strong>3</strong></code></span> directly:</p>
<p class="codelink"><a href="ch11_images.xhtml#f328-03" id="f328-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_gray">3 *</span> a
<span class="pd_green-d">array([ 0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48])</span></code></pre>
<p class="noindent">Here NumPy automatically threads the multiplication over the array elements (essentially equivalent to “scalar multiplication” on vectors). We can also apply an operation like squaring in a similar manner:</p>
<span epub:type="pagebreak" id="page_329"></span>
<p class="codelink"><a href="ch11_images.xhtml#f329-01" id="f329-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a<span class="pd_gray">**2</span>
<span class="pd_green-d">array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169,</span>
<span class="pd_green-d">196, 225, 256])</span></code></pre>
<p class="noindent">Here each element of <span class="pd_green-d"><code><strong>a</strong></code></span> has been squared without the need for a loop or comprehension.</p>
<p class="indent">As indicated above, this is not just for convenience, either; it’s a lot faster as well. We can see this by using the <span class="pd_green-d"><code><strong>timeit</strong></code></span> library to call the same code repeatedly and then time the result:</p>
<p class="codelink"><a href="ch11_images.xhtml#f329-02" id="f329-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green-d"><strong>import</strong></span> <span class="pd_nila"><strong>timeit</strong></span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> t1 <span class="pd_gray">=</span> timeit<span class="pd_gray">.</span>timeit(<span class="pd_red">"[i**2 for i in range(50)]"</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> t2 <span class="pd_gray">=</span> timeit<span class="pd_gray">.</span>timeit(<span class="pd_red">"import numpy as np; np.arange(50)**2"</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> t1, t2, t1<span class="pd_gray">/</span>t2
<span class="pd_green1-d">(9.171871625003405, 0.5006397919496521, 18.320300887960165)</span></code></pre>
<p class="noindent">Although exact results will vary, the result shown here indicates nearly a factor of 20 increase in speed for the vectorized version, which NumPy accomplishes by pushing the main loops into optimized C code. (<em>Note</em>: In a Jupyter notebook, we can use IPython to perform an even better comparison using the special <span class="pd_green-d"><code><strong>%%timeit</strong></code></span> operation (<a href="ch11.xhtml#ch11fig5">Figure 11.5</a>).)</p>
<figure class="image-c" id="ch11fig5">
<img src="graphics/11fig05.jpg" alt="images" width="725" height="229"/>
<figcaption>
<p class="title-f"><strong>Figure 11.5:</strong> Using NumPy and <span class="pd_green-d"><code><strong>timeit</strong></code></span> in a Jupyter notebook.</p>
</figcaption>
</figure>
<span epub:type="pagebreak" id="page_330"></span>
</section>
<section>
<h4 class="h4" id="sec11_2_2">11.2.2 Multidimensional Arrays</h4>
<p class="noindent">NumPy also includes support for multidimensional arrays:</p>
<p class="codelink"><a href="ch11_images.xhtml#f330-01" id="f330-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a <span class="pd_gray">=</span> np<span class="pd_gray">.</span>array([[<span class="pd_gray">1</span>, <span class="pd_gray">2</span>, <span class="pd_gray">3</span>], [<span class="pd_gray">4</span>, <span class="pd_gray">5</span>, <span class="pd_gray">6</span>]])
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a
<span class="pd_green-d">array([[1, 2, 3],</span>
       <span class="pd_green-d">[4, 5, 6]])</span></code></pre>
<p class="noindent">NumPy arrays have an attribute called <span class="pd_green-d"><code><strong>shape</strong></code></span> that returns the number of rows and columns:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a<span class="pd_gray">.</span>shape
<span class="pd_green-d">(2, 3)</span></code></pre>
<p class="noindent">Here <span class="pd_green-d"><code><strong>(2, 3)</strong></code></span> corresponds to the 2 rows (<span class="pd_green-d"><code><strong>[1, 2, 3]</strong></code></span> and <span class="pd_green-d"><code><strong>[4, 5, 6]</strong></code></span>) and the 3 columns (<span class="pd_green-d"><code><strong>[1, 4]</strong></code></span>, <span class="pd_green-d"><code><strong>[2, 5]</strong></code></span>, and <span class="pd_green-d"><code><strong>[3, 6]</strong></code></span>). You can think of this as a 2 × 3 matrix.</p>
<p class="indent">In analogy with list slicing (<a href="ch03.xhtml#sec3_3">Section 3.3</a>), NumPy supports array slicing for ndarrays of all dimensions. The colon notation introduced in <a href="ch03.xhtml#sec3_3">Section 3.3</a> is especially useful for selecting full rows or columns by using a single colon by itself:</p>
<p class="codelink"><a href="ch11_images.xhtml#f330-02" id="f330-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a[<span class="pd_gray">0</span>, :]            <span class="pd_blue1"><em># first row</em></span>
<span class="pd_green-d">array([1, 2, 3])</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a[:, <span class="pd_gray">0</span>]            <span class="pd_blue1"><em># first column</em></span>
<span class="pd_green-d">array([1, 4])</span></code></pre>
<p class="noindent">By combining colons with number ranges, we can slice out a subarray:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> A <span class="pd_gray">=</span> a[<span class="pd_gray">0</span>: <span class="pd_gray">2</span>, <span class="pd_gray">0</span>:<span class="pd_gray">2</span>]
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> A
<span class="pd_green-d">array([[1, 2],</span>
      <span class="pd_green-d">[4, 5]])</span></code></pre>
<p class="noindent">As with list slicing, you can omit the beginning or end of the range and get the same result:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> A <span class="pd_gray">=</span> a[:<span class="pd_gray">2</span>, :<span class="pd_gray">2</span>]
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> A
<span class="pd_green-d">array([[1, 2],</span>
       <span class="pd_green-d">[4, 5]])</span></code></pre>
<p class="indent"><span epub:type="pagebreak" id="page_331"></span>NumPy includes lots of support for common numerical operations such as linear algebra, in this case using super-optimized and battle-tested packages like BLAS and LAPACK. These routines are mostly written in C and Fortran, but we don’t have to know those languages because they are wrapped by Python via the <span class="pd_green-d"><code><strong>linalg</strong></code></span> library.<sup><a id="fn11_7a" href="ch11.xhtml#fn11_7">7</a></sup></p>
<p class="footnote"><a id="fn11_7" href="ch11.xhtml#fn11_7a">7.</a> Although I did end up doing a lot of C programming in graduate school anyway, I was able to achieve my childhood dream of never having to learn Fortran.</p>
<p class="indent">Let’s take a look at a quick example of NumPy’s linear algebra support. The sub-array <span class="pd_green-d"><code><strong>A</strong></code></span> that we just defined is a square matrix (the same number of rows and columns), which means that we can try calculating its matrix inverse. The inverse of an invertible matrix, written as <em>A</em><sup>−1</sup> (“A inverse”), satisfies the relations <em>AA</em><sup>−1</sup> = <em>A</em><sup>−1</sup><em>A</em> = <em>I</em>, where <em>I</em> is the <em>n</em> × <em>n</em> identity matrix (1s on the diagonal, 0s everywhere else). Matrix inversion is available in NumPy via <span class="pd_green-d"><code><strong>linalg.inv()</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f331-01" id="f331-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> Ainv <span class="pd_gray">=</span> np<span class="pd_gray">.</span>linalg<span class="pd_gray">.</span>inv(A)             <span class="pd_blue1"><em># inverse of a matrix</em></span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> Ainv
<span class="pd_green-d">array([[-1.66666667,  0.66666667],</span>
       <span class="pd_green-d">[ 1.33333333, -0.33333333]])</span></code></pre>
<p class="noindent">We can try adding and multiplying the matrices using <span class="pd_green-d"><code><strong>+</strong></code></span> and <span class="pd_green-d"><code><strong>*</strong></code></span>, respectively:</p>
<p class="codelink"><a href="ch11_images.xhtml#f331-02" id="f331-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> A <span class="pd_gray">+</span> Ainv
<span class="pd_green-d">array([[-0.66666667, 2.66666667],</span>
       <span class="pd_green-d">[ 5.33333333, 4.66666667]])</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> A <span class="pd_gray">*</span> Ainv
<span class="pd_green-d">array([[-1.66666667, 1.33333333],</span>
       <span class="pd_green-d">[ 5.33333333, -1.66666667]])</span></code></pre>
<p class="indent">Although the array sum <span class="pd_green-d"><code><strong>A + Ainv</strong></code></span> has no particular mathematical significance in this context, we see that the elements have been added in accordance with NumPy vectorized operations (<a href="ch11.xhtml#sec11_2_1">Section 11.2.1</a>). Similarly, the array product <span class="pd_green-d"><code><strong>A * Ainv</strong></code></span> has also been calculated term by term. This is a possible source of confusion because in some systems (notably MATLAB) the <span class="pd_green-d"><code><strong>*</strong></code></span> operator performs <em>matrix multiplication</em> in this context, yielding the expected result <em>AA</em><sup>−1</sup> = <em>I</em>. In NumPy, the most convenient way to perform matrix multiplication is with the <span class="pd_green-d"><code><strong>@</strong></code></span> operator:<sup><a id="fn11_8a" href="ch11.xhtml#fn11_8">8</a></sup></p>
<p class="footnote"><a id="fn11_8" href="ch11.xhtml#fn11_8a">8.</a> The <span class="pd_green-d"><code><strong>matmul()</strong></code></span> function works as well; with <span class="pd_green-d"><code><strong>numpy</strong></code></span> imported as <span class="pd_green-d"><code><strong>np</strong></code></span>, this would appear as <span class="pd_green-d"><code><strong>np.matmul(A, Ainv)</strong></code></span> and is equivalent to <span class="pd_green-d"><code><strong>A @ Ainv</strong></code></span>.</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> A <span class="pd_gray">@</span> Ainv
<span class="pd_green-d">array([[1., 0.],</span>
       <span class="pd_green-d">[0., 1.]])</span></code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_332"></span>The result is the 2 × 2 identity matrix as expected. (Note that some elements may be close to but not exactly zero due to numerical roundoff error; see <a href="ch11.xhtml#sec11_2_3">Section 11.2.3</a> for more information.)</p>
<p class="indent">One especially useful method for manipulating matrix objects is <span class="pd_green-d"><code><strong>reshape()</strong></code></span>, which allows us to change (say) a one-dimensional array into a two-dimensional array. The argument to <span class="pd_green-d"><code><strong>reshape()</strong></code></span> is a tuple (<a href="ch03.xhtml#sec3_6">Section 3.6</a>) with the target dimensions:</p>
<p class="codelink"><a href="ch11_images.xhtml#f332-01" id="f332-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a <span class="pd_gray">=</span> np<span class="pd_gray">.</span>arange(<span class="pd_gray">16</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a<span class="pd_gray">.</span>reshape((<span class="pd_gray">2</span>, <span class="pd_gray">8</span>))
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a
<span class="pd_green-d">array([[ 0, 1, 2, 3, 4, 5, 6, 7],</span>
       <span class="pd_green-d">[ 8, 9, 10, 11, 12, 13, 14, 15]])</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> b <span class="pd_gray">=</span> a<span class="pd_gray">.</span>reshape((<span class="pd_gray">4</span>, <span class="pd_gray">4</span>))
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> b
<span class="pd_green-d">array([[ 0, 1, 2, 3],</span>
       <span class="pd_green-d">[ 4, 5, 6, 7],</span>
       <span class="pd_green-d">[ 8, 9, 10, 11],</span>
       <span class="pd_green-d">[12, 13, 14, 15]])</span></code></pre>
<p class="noindent">Using <span class="pd_green-d"><code><strong>reshape()</strong></code></span> is often much more convenient than building up the corresponding arrays by hand. Note that <span class="pd_green-d"><code><strong>reshape()</strong></code></span> doesn’t mutate the array, so we need to make an assignment if we want a name for the reshaped version.</p>
<p class="indent">The <span class="pd_green-d"><code><strong>reshape()</strong></code></span> function supports using <span class="pd_green-d"><code><strong>-1</strong></code></span> as one of the arguments, which has an effect described in the documentation:</p>
<p class="blockquote">One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.</p>
<p class="noindent">For example, we can use the argument <span class="pd_green-d"><code><strong>(-1, 2)</strong></code></span> with an array of 16 elements to get an 8 × 2 matrix, where the <span class="pd_green-d"><code><strong>8</strong></code></span> comes from dividing <span class="pd_green-d"><code><strong>16</strong></code></span> by <span class="pd_green-d"><code><strong>2</strong></code></span>:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a<span class="pd_gray">.</span>reshape((<span class="pd_gray">-1</span>, <span class="pd_gray">2</span>))
<span class="pd_green-d">array([[ 0, 1],</span>
       <span class="pd_green-d">[ 2, 3],</span>
       <span class="pd_green-d">[ 4, 5], </span>
       <span class="pd_green-d">[ 6, 7],</span>
       <span class="pd_green-d">[ 8, 9],</span>
       <span class="pd_green-d">[10, 11],</span>
       <span class="pd_green-d">[12, 13],</span>
       <span class="pd_green-d">[14, 15]])</span></code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_333"></span>In effect, the <span class="pd_green-d"><code><strong>-1</strong></code></span> is a placeholder that says “use the dimensionality needed to make the total number of elements correct.”</p>
<p class="indent">Among other things, this <span class="pd_green-d"><code><strong>-1</strong></code></span> technique can be used to convert a multidimensional array to an array of arrays of <em>one</em> element, which can be accomplished using the argument <span class="pd_green-d"><code><strong>(-1, 1)</strong></code></span> (<a href="ch11.xhtml#ch11list4">Listing 11.4</a>). This format is common as an input to machine-learning algorithms (<a href="ch11.xhtml#sec11_7">Section 11.7</a>).</p>
<p class="listing" id="ch11list4"><strong>Listing 11.4:</strong> Creating an array of one-dimensional arrays.</p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a<span class="pd_gray">.</span>reshape((<span class="pd_gray">-1</span>, <span class="pd_gray">1</span>))
<span class="pd_green-d">array([[ 0],</span>
       <span class="pd_green-d">[ 1],</span>
       <span class="pd_green-d">[ 2],</span>
       <span class="pd_green-d">[ 3],</span>
       <span class="pd_green-d">[ 4],</span>
       <span class="pd_green-d">[ 5],</span>
       <span class="pd_green-d">[ 6],</span>
       <span class="pd_green-d">[ 7],</span>
       <span class="pd_green-d">[ 8],</span>
       <span class="pd_green-d">[ 9],</span>
       <span class="pd_green-d">[10],</span>
       <span class="pd_green-d">[11],</span>
       <span class="pd_green-d">[12],</span>
       <span class="pd_green-d">[13],</span>
       <span class="pd_green-d">[14],</span>
       <span class="pd_green-d">[15]])</span></code></pre>
</div>
</section>
<section>
<h4 class="h4" id="sec11_2_3">11.2.3 Constants, Functions, and Linear Spacing</h4>
<p class="noindent">Like the <span class="pd_green-d"><code><strong>math</strong></code></span> library discussed in <a href="ch04.xhtml#sec4_1">Section 4.1</a>, NumPy comes equipped with mathematical constants, such as Euler’s number <em>e</em>:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>import</strong></span> <span class="pd_nila"><strong>math</strong></span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> math<span class="pd_gray">.</span>e
<span class="pd_green-d">2.718281828459045</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>e
<span class="pd_green-d">2.718281828459045</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> math<span class="pd_gray">.</span>e <span class="pd_gray">==</span> np<span class="pd_gray">.</span>e
<span class="pd_green-d">True</span></code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_334"></span>NumPy also defines <span class="pd_green-d"><code><strong>pi</strong></code></span> but unfortunately doesn’t have <span class="pd_green-d"><code><strong>tau</strong></code></span> as of this writing:</p>
<p class="codelink"><a href="ch11_images.xhtml#f334-01" id="f334-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>pi
<span class="pd_green-d">3.141592653589793</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>tau
<span class="pd_dnila">Traceback (most recent call last):</span>
    <span class="pd_green"><strong>raise</strong></span> <span class="pd_lred"><strong>AttributeError</strong></span>(<span class="pd_red">"module</span> <span class="pd_lpink"><strong>{!r}</strong></span> <span class="pd_red">has no attribute "</span>
<span class="pd_redred">AttributeError</span>: module 'numpy' has no attribute 'tau'</code></pre>
<p class="noindent">We can still use the one in <span class="pd_green-d"><code><strong>math</strong></code></span>, though:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> math<span class="pd_gray">.</span>tau
<span class="pd_green-d">6.283185307179586</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> math<span class="pd_gray">.</span>tau <span class="pd_gray">== 2 *</span> np<span class="pd_gray">.</span>pi
<span class="pd_green-d">True</span></code></pre>
<p class="indent">Also like <span class="pd_green-d"><code><strong>math</strong></code></span>, NumPy has operations like trigonometric functions and logarithms (see below for an explanation of the strange result of <span class="pd_green-d"><code><strong>np.sin(math.tau)</strong></code></span>):</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>cos(math<span class="pd_gray">.</span>tau)
<span class="pd_green-d">1.0</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>sin(math<span class="pd_gray">.</span>tau)
<span class="pd_green-d">-2.4492935982947064e-16</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>log(np<span class="pd_gray">.</span>e)
<span class="pd_green-d">1.0</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>log10(np<span class="pd_gray">.</span>e)
<span class="pd_green-d">0.4342944819032518</span></code></pre>
<p class="noindent">Note that, again as with <span class="pd_green-d"><code><strong>math</strong></code></span>, and like most programming languages, NumPy uses <span class="pd_green-d"><code><strong>log()</strong></code></span> to denote the natural logarithm and <span class="pd_green-d"><code><strong>log10()</strong></code></span> for base-ten logs.</p>
<p class="indent">At this point, you may wonder what the point is of including definitions in NumPy that duplicate those in <span class="pd_green-d"><code><strong>math</strong></code></span>. For constants like <em>e</em> and <em>π</em> it’s mainly for completeness, but with the functions there’s actually a meaningful difference: Unlike <span class="pd_green-d"><code><strong>math</strong></code></span> functions, NumPy’s functions can be threaded over arrays using the same vectorized operations we first saw in <a href="ch11.xhtml#sec11_2_1">Section 11.2.1</a>.</p>
<p class="indent">Consider, for example, one period of cos <em>x</em>, with angles ranging from 0 to <em>τ</em> (<a href="ch11.xhtml#ch11list5">Listing 11.5</a>).<sup><a id="fn11_9a" href="ch11.xhtml#fn11_9">9</a></sup></p>
<p class="footnote"><a id="fn11_9" href="ch11.xhtml#fn11_9a">9.</a> I prefer to use cosine instead of sine as the canonical example because it’s more intuitive from the perspective of simple harmonic motion, which is one of the most important examples of sinusoidal functions. Because the cosine function starts at 1, it corresponds naturally to an oscillator moved some distance from equilibrium and released from rest. In contrast, using sine involves giving a kick or flick to such an oscillator so that it starts with a nonzero velocity at equilibrium, which is a much less common way of initiating such motion.</p>
<span epub:type="pagebreak" id="page_335"></span>
<p class="listing" id="ch11list5"><strong>Listing 11.5:</strong> Angles corresponding to simple fractions of a period of cos <em>x</em>.</p>
<p class="codelink"><a href="ch11_images.xhtml#f335-01" id="f335-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>arange(<span class="pd_gray">5</span>)
<span class="pd_green-d">array([0, 1, 2, 3, 4])</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> angles <span class="pd_gray">=</span> math<span class="pd_gray">.</span>tau <span class="pd_gray">*</span> np<span class="pd_gray">.</span>arange(<span class="pd_gray">5</span>) <span class="pd_gray">/ 4</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> angles
<span class="pd_green-d">array([0.        , 1.57079633, 3.14159265, 4.71238898, 6.28318531])</span></code></pre>
</div>
<p class="noindent">Note that the values of the <span class="pd_green-d"><code><strong>angles</strong></code></span> array in <a href="ch11.xhtml#ch11list5">Listing 11.5</a> are simply the numerical equivalents of 0, <em>τ/</em>4, <em>τ/</em>2, 3<em>τ/</em>4, and <em>τ</em> . Applying <span class="pd_green-d"><code><strong>cos()</strong></code></span> to these angles doesn’t work for the <span class="pd_green-d"><code><strong>math</strong></code></span> version of cosine but does for the NumPy version (<a href="ch11.xhtml#ch11list6">Listing 11.6</a>).</p>
<p class="listing" id="ch11list6"><strong>Listing 11.6:</strong> Applying <span class="pd_green-d"><code><strong>cos()</strong></code></span> to an array of angles.</p>
<p class="codelink"><a href="ch11_images.xhtml#f335-02" id="f335-02a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> math<span class="pd_gray">.</span>cos(angles)
<span class="pd_dnila">Traceback (most recent call last):</span>
  File <span class="pd_green">"&lt;stdin&gt;"</span>, line <span class="pd_gray">1</span>, in &lt;module&gt;
<span class="pd_redred">TypeError</span>: only size-1 arrays can be converted to Python scalars
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a <span class="pd_gray">=</span> np<span class="pd_gray">.</span>cos(angles)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a
<span class="pd_green-d">array([ 1.0000000e+00, 6.1232340e-17, -1.0000000e+00, -1.8369702e-16,</span>
        <span class="pd_green-d">1.0000000e+00])</span></code></pre>
</div>
<p class="indent">Note that, due to floating-point roundoff errors, the zeros of cos <em>x</em> in <a href="ch11.xhtml#ch11list6">Listing 11.6</a> appear as tiny numbers rather than as <span class="pd_green-d"><code><strong>0</strong></code></span> (though such behavior is often system-dependent, so your exact results may differ). We can get rid of these using NumPy’s <span class="pd_green-d"><code><strong>isclose()</strong></code></span> function, which returns <span class="pd_green-d"><code><strong>True</strong></code></span> if a number is “close” to the given number (essentially, within the margin of error of the system’s floating-point arithmetic):</p>
<p class="codelink"><a href="ch11_images.xhtml#f335-03" id="f335-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>isclose(<span class="pd_gray">0.01</span>, <span class="pd_gray">0</span>)
<span class="pd_green-d">False</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>isclose(<span class="pd_gray">10**</span>(<span class="pd_gray">-16</span>), <span class="pd_gray">0</span>)
<span class="pd_green-d">True</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>isclose(a, <span class="pd_gray">0</span>)
<span class="pd_green-d">array([False, True, False, True, False])</span></code></pre>
<p class="noindent">We can actually pass this array of boolean values to the original array itself and set the elements corresponding to <span class="pd_green-d"><code><strong>True</strong></code></span> exactly to <span class="pd_green-d"><code><strong>0</strong></code></span> (<a href="ch11.xhtml#ch11list7">Listing 11.7</a>).</p>
<span epub:type="pagebreak" id="page_336"></span>
<p class="listing" id="ch11list7"><strong>Listing 11.7:</strong> Using <span class="pd_green-d"><code><strong>isclose()</strong></code></span> to zero out values close to <span class="pd_green-d"><code><strong>0</strong></code></span>.</p>
<p class="codelink"><a href="ch11_images.xhtml#f336-01" id="f336-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a[np<span class="pd_gray">.</span>isclose(a, <span class="pd_gray">0</span>)]
<span class="pd_green-d">array([ 6.1232340e-17, -1.8369702e-16])</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a[np<span class="pd_gray">.</span>isclose(a, <span class="pd_gray">0</span>)] <span class="pd_gray">= 0</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a
<span class="pd_green-d">array([ 1.,  0.,  -1.,  0.,  1.])</span></code></pre>
</div>
<p class="indent">In <a href="ch11.xhtml#ch11list5">Listing 11.5</a>, we divided the <span class="pd_green-d"><code><strong>arange(5)</strong></code></span> by 4 when generating the angles, but for technical reasons (related to numerical roundoff error) the preferred way to make such sequences is with <span class="pd_green-d"><code><strong>linspace()</strong></code></span> (“linearly space(d)”). The most common arguments to the <span class="pd_green-d"><code><strong>linspace()</strong></code></span> function are the beginning, end, and total number of points desired. For example, we can use <span class="pd_green-d"><code><strong>linspace()</strong></code></span> to make an array of the four quarters of a period (with 5 total points since we’re including <span class="pd_green-d"><code><strong>0</strong></code></span>):</p>
<p class="codelink"><a href="ch11_images.xhtml#f336-02" id="f336-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> angles <span class="pd_gray">=</span> np<span class="pd_gray">.</span>linspace(<span class="pd_gray">0</span>, math<span class="pd_gray">.</span>tau, <span class="pd_gray">5</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> angles
<span class="pd_green-d">array([0.        , 1.57079633, 3.14159265, 4.71238898, 6.28318531])</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a <span class="pd_gray">=</span> np<span class="pd_gray">.</span>cos(angles)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a[np<span class="pd_gray">.</span>isclose(a, <span class="pd_gray">0</span>)] <span class="pd_gray">= 0</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> a
<span class="pd_green-d">array([ 1., 0., -1., 0., 1.])</span></code></pre>
<p class="indent">The <span class="pd_green-d"><code><strong>linspace()</strong></code></span> function is often used to create an array with much finer spacing using a larger number of points. For instance, we can get 100 points of cos <em>x</em> as follows:</p>
<p class="codelink"><a href="ch11_images.xhtml#f336-03" id="f336-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> angles <span class="pd_gray">=</span> np<span class="pd_gray">.</span>linspace(<span class="pd_gray">0</span>, math<span class="pd_gray">.</span>tau, <span class="pd_gray">100</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> angles
<span class="pd_green-d">array([0.        , 0.06346652, 0.12693304, 0.19039955, 0.25386607,</span>
       <span class="pd_green-d">0.31733259, 0.38079911, 0.44426563, 0.50773215, 0.57119866,</span>
       <span class="pd_green-d">0.63466518, 0.6981317 , 0.76159822, 0.82506474, 0.88853126,</span>
       <span class="pd_green-d">0.95199777, 1.01546429, 1.07893081, 1.14239733, 1.20586385,</span>
       <span class="pd_green-d">1.26933037, 1.33279688, 1.3962634 , 1.45972992, 1.52319644,</span>
       <span class="pd_green-d">1.58666296, 1.65012947, 1.71359599, 1.77706251, 1.84052903,</span>
       <span class="pd_green-d">1.90399555, 1.96746207, 2.03092858, 2.0943951 , 2.15786162,</span>
       <span class="pd_green-d">2.22132814, 2.28479466, 2.34826118, 2.41172769, 2.47519421,</span>
       <span class="pd_green-d">2.53866073, 2.60212725, 2.66559377, 2.72906028, 2.7925268 ,</span>
       <span class="pd_green-d">2.85599332, 2.91945984, 2.98292636, 3.04639288, 3.10985939,</span>
       <span class="pd_green-d">3.17332591, 3.23679243, 3.30025895, 3.36372547, 3.42719199,</span>
       <span class="pd_green-d">3.4906585 , 3.55412502, 3.61759154, 3.68105806, 3.74452458,</span>
       <span class="pd_green-d">3.8079911 , 3.87145761, 3.93492413, 3.99839065, 4.06185717,</span>
       <span class="pd_green-d">4.12532369, 4.1887902 , 4.25225672, 4.31572324, 4.37918976,</span>
       <span class="pd_green-d">4.44265628, 4.5061228 , 4.56958931, 4.63305583, 4.69652235,</span>
       <span class="pd_green-d">4.75998887, 4.82345539, 4.88692191, 4.95038842, 5.01385494,</span>
<span epub:type="pagebreak" id="page_337"></span>       <span class="pd_green-d">5.07732146, 5.14078798, 5.2042545 , 5.26772102, 5.33118753,
       5.39465405, 5.45812057, 5.52158709, 5.58505361, 5.64852012,
       5.71198664, 5.77545316, 5.83891968, 5.9023862 , 5.96585272,
       6.02931923, 6.09278575, 6.15625227, 6.21971879, 6.28318531])</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> np<span class="pd_gray">.</span>cos(angles)
<span class="pd_green-d">array([ 1.        ,  0.99798668,  0.99195481,  0.9819287 ,  0.9679487 ,</span>
        <span class="pd_green-d">0.95007112,  0.92836793,  0.90292654,  0.87384938,  0.84125353,</span>
        <span class="pd_green-d">0.80527026,  0.76604444,  0.72373404,  0.67850941,  0.63055267,</span>
        <span class="pd_green-d">0.58005691,  0.52722547,  0.47227107,  0.41541501,  0.35688622,</span>
        <span class="pd_green-d">0.29692038,  0.23575894,  0.17364818,  0.1108382 ,  0.04758192,</span>
       <span class="pd_green-d">-0.01586596, -0.07924996, -0.14231484, -0.20480667, -0.26647381,</span>
       <span class="pd_green-d">-0.32706796, -0.38634513, -0.44406661, -0.5 ,       -0.55392006,</span>
       <span class="pd_green-d">-0.60560969, -0.65486073, -0.70147489, -0.74526445, -0.78605309,</span>
       <span class="pd_green-d">-0.82367658, -0.85798341, -0.88883545, -0.91610846, -0.93969262,</span>
       <span class="pd_green-d">-0.95949297, -0.97542979, -0.98743889, -0.99547192, -0.99949654,</span>
       <span class="pd_green-d">-0.99949654, -0.99547192, -0.98743889, -0.97542979, -0.95949297,</span>
       <span class="pd_green-d">-0.93969262, -0.91610846, -0.88883545, -0.85798341, -0.82367658,</span>
       <span class="pd_green-d">-0.78605309, -0.74526445, -0.70147489, -0.65486073, -0.60560969,</span>
       <span class="pd_green-d">-0.55392006, -0.5 ,       -0.44406661, -0.38634513, -0.32706796,</span>
       <span class="pd_green-d">-0.26647381, -0.20480667, -0.14231484, -0.07924996, -0.01586596,</span>
        <span class="pd_green-d">0.04758192,  0.1108382 ,  0.17364818,  0.23575894,  0.29692038,</span>
        <span class="pd_green-d">0.35688622,  0.41541501,  0.47227107,  0.52722547,  0.58005691,</span>
        <span class="pd_green-d">0.63055267,  0.67850941,  0.72373404,  0.76604444,  0.80527026,</span>
        <span class="pd_green-d">0.84125353,  0.87384938,  0.90292654,  0.92836793,  0.95007112,</span>
        <span class="pd_green-d">0.9679487 ,  0.9819287 ,  0.99195481,  0.99798668,  1.        ])</span></code></pre>
<p class="noindent">It’s rather hard to visualize this many raw values, but they are the perfect input to a plotting library like Matplotlib, which is the subject of <a href="ch11.xhtml#sec11_3">Section 11.3</a>.</p>
</section>
<section>
<h4 class="h4" id="sec11_2_4">11.2.4 Exercises</h4>
<ol class="number">
<li><p class="number">What happens if the dimensions in <span class="pd_green-d"><code><strong>reshape()</strong></code></span> don’t match the array size (e.g., <span class="pd_green-d"><code><strong>np.arange(16).reshape((4, 17))</strong></code></span>)?</p></li>
<li><p class="number">Confirm that <span class="pd_green-d"><code><strong>A = np.random.rand(5, 5)</strong></code></span> lets you define a 5 × 5 random matrix.</p></li>
<li><p class="number">Find the inverse <span class="pd_green-d"><code><strong>Ainv</strong></code></span> of the 5 × 5 matrix in the previous exercise. (Calculating the inverse of a 2 × 2 matrix as in <a href="ch11.xhtml#sec11_2_2">Section 11.2.2</a> is fairly simple by hand, but the task rapidly gets harder as the matrix size increases, in which case a computational system like NumPy is indispensable.)</p></li>
<li><p class="number">What is the matrix product <span class="pd_green-d"><code><strong>I = A @ Ainv</strong></code></span> of the matrices in the previous two exercises? Use the same <span class="pd_green-d"><code><strong>isclose()</strong></code></span> trick from <a href="ch11.xhtml#ch11list7">Listing 11.7</a> to zero out the elements of <span class="pd_green-d"><code><strong>I</strong></code></span> close to zero and confirm that the resulting matrix is indeed the 5 × 5 identity matrix.</p></li>
</ol>
<span epub:type="pagebreak" id="page_338"></span>
</section>
</section>
<section>
<h3 class="h3" id="sec11_3">11.3 Data Visualization with Matplotlib</h3>
<p class="noindent">Matplotlib is a powerful visualization tool for Python that can do an absurdly large number of awesome things.<sup><a id="fn11_10a" href="ch11.xhtml#fn11_10">10</a></sup> In this section, we’ll start with a simple two-dimensional plot based on the work we did in <a href="ch11.xhtml#sec11_2">Section 11.2</a> and incrementally include additional features, eventually reaching the figure shown in <a href="ch11.xhtml#ch11fig6">Figure 11.6</a>. We’ll then cover a couple of other important cases (scatter plots and histograms), which are important for data analysis with pandas (<a href="ch11.xhtml#sec11_4">Section 11.4</a>). The exact mechanics of displaying Matplotlib plots depends on the particular setup; refer to <a href="ch11.xhtml#ch11box2">Box 11.2</a> to get the display of Matplotlib plots working on your system.</p>
<figure class="image-c" id="ch11fig6">
<img src="graphics/11fig06.jpg" alt="images" width="601" height="464"/>
<figcaption>
<p class="title-f"><strong>Figure 11.6:</strong> A fancy plot showing off features of Matplotlib.</p>
</figcaption>
</figure>
<p class="footnote"><a id="fn11_10" href="ch11.xhtml#fn11_10a">10.</a> It’s worth noting that many Python data scientists also use seaborn, which is a data-visualization library built on Matplotlib. Although learning seaborn is certainly not necessary to be <em>dangerous</em>, it would make a natural follow-on to this section. The official seaborn tutorial would be a good place to start.</p>
<span epub:type="pagebreak" id="page_339"></span>
<div class="box">
<p class="box-title" id="ch11box2"><strong>Box 11.2: Matplotlib mechanics</strong></p>
<p class="sb-noindent">The exact mechanics of getting Matplotlib plots to display varies widely depending on the exact details of your setup. The most explicit way to show plots, which works on most systems from the REPL, is to use the <code>show()</code> method:</p>
<p class="codelink"><a href="ch11_images.xhtml#f339-01" id="f339-01a">Click here to view code image</a></p>
<pre class="pre1-b"><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; x = np.linspace(-2, 2, 100)
&gt;&gt;&gt; fig, ax = plt.subplots(1, 1)
&gt;&gt;&gt; ax.plot(x, x*x)
&gt;&gt;&gt; plt.show()</code></pre>
<p class="sb-noindent">On many systems, this will spawn a window like <a href="ch11.xhtml#ch11fig7">Figure 11.7</a> with the result of the plot.</p>
<figure class="image-c" id="ch11fig7">
<img src="graphics/11fig07.jpg" alt="images" width="600" height="523"/>
<figcaption>
<p class="title-f"><strong>Figure 11.7:</strong> A window spawned by a call to <span class="pd_green-d"><code><strong>show()</strong></code></span>.</p>
</figcaption>
</figure>
<p class="sb-indent">In Jupyter notebooks, the environment can be configured to show Matplotlib plots automatically (“inline”, i.e., right in the notebook) by executing this command in a notebook cell:</p>
<pre class="pre1-b"><code>%matplotlib inline</code></pre>
<p class="sb-noindent">As far as I can tell, on some systems (including mine) this setting is on by default, with plots appearing automatically when the corresponding Jupyter cells are evaluated (<a href="ch11.xhtml#ch11fig8">Figure 11.8</a>).</p>
<figure class="image-c" id="ch11fig8">
<img src="graphics/11fig08.jpg" alt="images" width="725" height="498"/>
<figcaption>
<p class="title-f"><strong>Figure 11.8:</strong> A plot appearing automatically in a Jupyter notebook.</p>
</figcaption>
</figure>
<p class="sb-indent">In an environment such as the cloud IDE, it’s possible to switch to a non-graphical back-end, write out to a file, and then view the file in the browser. See this Stack Overflow thread (<a href="https://bit.ly/cloud-plot">https://bit.ly/cloud-plot</a>) if you’d like to go this route, but the recommended solution is to set up Jupyter on the cloud IDE as described in <a href="ch11.xhtml#ch11box1">Box 11.1</a>. In that case, you can set up inline plot display as described above (if in fact it’s not available automatically).</p>
</div>
<section>
<h4 class="h4" id="sec11_3_1">11.3.1 Plotting</h4>
<p class="noindent">We’ll start by reviewing the final example from <a href="ch11.xhtml#sec11_2">Section 11.2</a>, which defined a linearly spaced array with 100 points from 0 to <em>τ</em> :</p>
<p class="codelink"><a href="ch11_images.xhtml#f339-02" id="f339-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_green-d">(venv) $ python3</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>import</strong></span> <span class="pd_nila"><strong>numpy</strong></span> <span class="pd_green"><strong>as</strong></span> <span class="pd_nila"><strong>np</strong></span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>import</strong></span> <span class="pd_nila"><strong>matplotlib.pyplot</strong></span> <span class="pd_green"><strong>as</strong></span> <span class="pd_nila"><strong>plt</strong></span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>math</strong></span> <span class="pd_green"><strong>import</strong></span> tau
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> x <span class="pd_gray">=</span> np<span class="pd_gray">.</span>linspace(<span class="pd_gray">0</span>, tau, <span class="pd_gray">100</span>)</code></pre>
<p class="indent"><span epub:type="pagebreak" id="page_340"></span>Matplotlib has two key objects, <span class="pd_green-d"><code><strong>Figure</strong></code></span> and <span class="pd_green-d"><code><strong>Axes</strong></code></span>. Roughly speaking, <span class="pd_green-d"><code><strong>Figure</strong></code></span> is a container for the elements that make up the image and <span class="pd_green-d"><code><strong>Axes</strong></code></span> is the data representing the elements. Don’t worry too much about exactly what this means, though; in practice, using Matplotlib often reduces to assigning figure and axes objects (conventionally called <span class="pd_green-d"><code><strong>fig</strong></code></span> and <span class="pd_green-d"><code><strong>ax</strong></code></span>) to the result of calling the <span class="pd_green-d"><code><strong>subplots()</strong></code></span> function:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> fig, ax <span class="pd_gray">=</span> plt<span class="pd_gray">.</span>subplots()</code></pre>
<p class="noindent">This somewhat obscure syntax comes right from the Matplotlib documentation.<sup><a id="fn11_11a" href="ch11.xhtml#fn11_11">11</a></sup></p>
<p class="footnote"><a id="fn11_11" href="ch11.xhtml#fn11_11a">11.</a> In this chapter, we use the so-called “object-oriented” interface to Matplotlib, which is generally preferred by the Matplotlib project itself. There is a second interface, though, which is designed to behave like the plotting features in MATLAB. See the article “Pyplot vs Object Oriented Interface” (<a href="https://matplotlib.org/matplotblog/posts/pyplot-vs-object-oriented-interface/">https://matplotlib.org/matplotblog/posts/pyplot-vs-object-oriented-interface/</a>) for more information.</p>
<p class="indent"><span epub:type="pagebreak" id="page_341"></span>To make a plot of the cosine function, we can then call the <span class="pd_green-d"><code><strong>ax</strong></code></span> object’s <span class="pd_green-d"><code><strong>plot()</strong></code></span> method with <em>x</em> (horizontal) values equal to our 100 linearly spaced points and <em>y</em> (vertical) values given by calling <span class="pd_green-d"><code><strong>np.cos</strong></code></span> on <span class="pd_green-d"><code><strong>x</strong></code></span>:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> ax<span class="pd_gray">.</span>plot(x, np<span class="pd_gray">.</span>cos(x))
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent">As noted in <a href="ch11.xhtml#ch11box2">Box 11.2</a>, the step to view the plot will depend on your exact setup, so we’ll use <span class="pd_green-d"><code><strong>plt.show()</strong></code></span> as a shorthand for “whatever the corresponding command is on your system.” (Note in particular that the <span class="pd_green-d"><code><strong>fig</strong></code></span> object won’t generally be needed unless you’re saving the figure to disk; <span class="pd_green-d"><code><strong>ax</strong></code></span> is where most of the action is.) The result in this case is the nice basic cosine plot shown in <a href="ch11.xhtml#ch11fig9">Figure 11.9</a>.</p>
<figure class="image-c" id="ch11fig9">
<img src="graphics/11fig09.jpg" alt="images" width="601" height="429"/>
<figcaption>
<p class="title-f"><strong>Figure 11.9:</strong> A nice basic plot of the cosine function.</p>
</figcaption>
</figure>
<p class="indent">For most of the remaining examples, I’ll be leaving off the <span class="pd_green-d"><code><strong>&gt;&gt;&gt;</strong></code></span> prompt so that you can more easily copy and paste if you want. This is mainly because building up plots can be a bit cumbersome since you have to rerun all the commands every time. One <span epub:type="pagebreak" id="page_342"></span>big advantage of Jupyter notebooks is that you can avoid this by building up the plot incrementally in a single cell and then repeatedly execute the code using Shift-Return.</p>
<p class="indent">As a next step, let’s include ticks for the <em>x</em>- and <em>y</em>-axes (using <span class="pd_green-d"><code><strong>set_xticks()</strong></code></span> and <span class="pd_green-d"><code><strong>set_yticks()</strong></code></span>) and add an overall grid (using <span class="pd_green-d"><code><strong>plt.grid()</strong></code></span>):</p>
<p class="codelink"><a href="ch11_images.xhtml#f342-01" id="f342-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_green-d">fig, ax = plt.subplots()</span>

<span class="b-y"><span class="pd_green">ax.set_xticks([0, tau/4, tau/2, 3*tau/4, tau])</span></span>
<span class="b-y"><span class="pd_green">ax.set_yticks([-1, -1/2, 0, 1/2, 1])</span></span>
<span class="b-y"><span class="pd_green">plt.grid(True)</span></span>

<span class="pd_green-d">ax.plot(x, np.cos(x))</span>
<span class="pd_green-d">plt.show()</span></code></pre>
<p class="noindent">The resulting plot makes it easier to see the structure of cosine, with four congruent pieces corresponding to each of the four quarters of the full period (<a href="ch11.xhtml#ch11fig10">Figure 11.10</a>).</p>
<figure class="image-c" id="ch11fig10">
<img src="graphics/11fig10.jpg" alt="images" width="601" height="436"/>
<figcaption>
<p class="title-f"><strong>Figure 11.10:</strong> Adding ticks and a grid.</p>
</figcaption>
</figure>
<p class="indent">The tick labels in <a href="ch11.xhtml#ch11fig10">Figure 11.10</a> are their default decimal values, but it would be more convenient to express them as fractions of the full period (i.e., <em>τ</em> ) on the <em>x-</em>axis and as fractions of ±1 on the <em>y</em>-axis. One great thing about Matplotlib is that it supports the widely used L<span class="smallcaps">A</span>T<sub>E</sub>X syntax for mathematical typesetting, which typically <span epub:type="pagebreak" id="page_343"></span>involves surrounding mathematical notation in dollar signs and indicating commands with backslashes.<sup><a id="fn11_12a" href="ch11.xhtml#fn11_12">12</a></sup> For example, this paragraph contains the following L<span class="smallcaps">A</span>T<sub>E</sub>X code:<sup><a id="fn11_13a" href="ch11.xhtml#fn11_13">13</a></sup></p>
<p class="footnote"><a id="fn11_12" href="ch11.xhtml#fn11_12a">12.</a> The pronunciation of L<span class="smallcaps">A</span>T<sub>E</sub>X differs; my preferred pronunciation is <em>lay</em>-tech, with “tech” as in “technology”. (I was gratified to discover that the text-to-speech program on macOS agrees.)</p>
<p class="footnote"><a id="fn11_13" href="ch11.xhtml#fn11_13a">13.</a> Using dollar signs (<span class="pd_green-d"><code><strong>$...$</strong></code></span> for inline math, <span class="pd_green-d"><code><strong>$$...$$</strong></code></span> for centered math) is properly associated with T<sub>E</sub>X, the system underlying L<span class="smallcaps">A</span>T<sub>E</sub>X. Technically, the preferred L<span class="smallcaps">A</span>T<sub>E</sub>X syntax is <span class="pd_green-d"><code><strong>\(...\)</strong></code></span> for inline math and <span class="pd_green-d"><code><strong>\[...\]</strong></code></span> for centered math. So far as I can tell, Jupyter notebooks support only the plain T<sub>E</sub>X syntax.</p>
<p class="codelink"><a href="ch11_images.xhtml#f343-01" id="f343-01a">Click here to view code image</a></p>
<pre class="pre1"><code>The tick labels in Figure~<span class="pd_green"><strong>\ref</strong></span><span class="pd_green">{</span>fig:cosine<span class="pd_green">_</span>ticks<span class="pd_green">}</span> are their default decimal
values, but it would be more convenient to express them as fractions of the
full period (i.e., <span class="pd_red">$</span><span class="pd_blue">\tau</span><span class="pd_red">$</span>) on the <span class="pd_red">$</span><span class="pd_green">x</span><span class="pd_red">$</span>-axis and as fractions of <span class="pd_red">$</span><span class="pd_blue">\pm</span> <span class="pd_gray">1</span><span class="pd_red">$</span> on the
<span class="pd_red">$</span><span class="pd_green">y</span><span class="pd_red">$</span>-axis.</code></pre>
<p class="indent">Because L<span class="smallcaps">A</span>T<sub>E</sub>X commands generally contain pesky backslashes, which often have strange behavior when placed inside strings, we’ll use raw strings (<a href="ch02.xhtml#sec2_2_2">Section 2.2.2</a>) so that we won’t have to escape them out. The resulting tick labels, which use the <span class="pd_green-d"><code><strong>set_xticklabels()</strong></code></span> and <span class="pd_green-d"><code><strong>set_yticklabels()</strong></code></span> methods, appear as follows:</p>
<span epub:type="pagebreak" id="page_344"></span>
<p class="codelink"><a href="ch11_images.xhtml#f344-01" id="f344-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_green-d">fig, ax = plt.subplots()</span>

<span class="pd_green-d">ax.set_xticks([0, tau/4, tau/2, 3*tau/4, tau])</span>
<span class="pd_green-d">ax.set_yticks([-1, -1/2, 0, 1/2, 1])</span>
<span class="pd_green-d">plt.grid(True)</span>

<span class="b-y"><span class="pd_green">ax.set_xticklabels([r"$0$", r"$</span><span class="pd_green">\</span><span class="pd_green">tau/4$", r"$</span><span class="pd_green">\</span><span class="pd_green">tau/2$", r"$3</span><span class="pd_green">\</span><span class="pd_green">tau/4$", r"$</span><span class="pd_green">\</span><span class="pd_green">tau$"])</span></span>
<span class="b-y"><span class="pd_green">ax.set_yticklabels([r"$-1$", r"$-1/2$", r"$0$", r"$1/2$", r"$1$"])</span></span>

<span class="pd_green-d">ax.plot(x, np.cos(x))</span>
<span class="pd_green-d">plt.show()</span></code></pre>
<p class="noindent">The result appears in <a href="ch11.xhtml#ch11fig11">Figure 11.11</a>.</p>
<figure class="image-c" id="ch11fig11">
<img src="graphics/11fig11.jpg" alt="images" width="601" height="435"/>
<figcaption>
<p class="title-f"><strong>Figure 11.11:</strong> Adding nice L<span class="smallcaps">A</span>T<sub>E</sub>X axis labels to the cosine plot.</p>
</figcaption>
</figure>
<p class="indent">Next, let’s add sine as well, along with axis labels and a plot title:</p>
<p class="codelink"><a href="ch11_images.xhtml#f344-02" id="f344-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_green-d">fig, ax = plt.subplots()</span>

<span class="pd_green-d">ax.set_xticks([0, tau/4, tau/2, 3*tau/4, tau])</span>
<span class="pd_green-d">ax.set_yticks([-1, -1/2, 0, 1/2, 1])</span>
<span class="pd_green-d">plt.grid(True)</span>

<span epub:type="pagebreak" id="page_345"></span><span class="pd_green-d">ax.set_xticklabels([r"$0$", r"$\tau/4$", r"$\tau/2$", r"$3\tau/4$", r"$\tau$"])</span>
<span class="pd_green-d">ax.set_yticklabels([r"$-1$", r"$-1/2$", r"$0$", r"$1/2$", r"$1$"])</span>

<span class="b-y"><span class="pd_green">ax.set_xlabel(r"$</span><span class="pd_green">\</span><span class="pd_green">theta$", fontsize=16)</span></span>
<span class="b-y"><span class="pd_green">ax.set_ylabel(r"$f(</span><span class="pd_green">\</span><span class="pd_green">theta)$", fontsize=16)</span></span>
<span class="b-y"><span class="pd_green">ax.set_title("One period of cosine and sine", fontsize=16)</span></span>

<span class="pd_green-d">ax.plot(x, np.cos(x))</span>
<span class="b-y"><span class="pd_green">ax.plot(x, np.sin(x))</span></span>
<span class="pd_green-d">plt.show()</span></code></pre>
<p class="noindent">Here we’ve used the Greek letter <em>θ</em> (theta) in the axis labels, which is a traditional letter for angles. The result appears in <a href="ch11.xhtml#ch11fig12">Figure 11.12</a>.</p>
<figure class="image-c" id="ch11fig12">
<img src="graphics/11fig12.jpg" alt="images" width="601" height="464"/>
<figcaption>
<p class="title-f"><strong>Figure 11.12:</strong> Adding sine and some additional labels.</p>
</figcaption>
</figure>
<p class="indent">Note from <a href="ch11.xhtml#ch11fig12">Figure 11.12</a> that Matplotlib automatically uses a new color for additional plots on the same <span class="pd_green-d"><code><strong>Axis</strong></code></span> object to help us tell them apart. We can further distinguish cosine from sine by adding <em>annotations</em>, which can be accomplished with the <span class="pd_green-d"><code><strong>annotate()</strong></code></span> method. See if you can figure out from context what the arguments <span class="pd_green-d"><code><strong>xy</strong></code></span>, <span class="pd_green-d"><code><strong>xytext</strong></code></span>, and <span class="pd_green-d"><code><strong>arrowprops</strong></code></span> do:</p>
<span epub:type="pagebreak" id="page_346"></span>
<p class="codelink"><a href="ch11_images.xhtml#f346-01" id="f346-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_green-d">fig, ax = plt.subplots()</span>

<span class="pd_green-d">ax.set_xticks([0, tau/4, tau/2, 3*tau/4, tau])</span>
<span class="pd_green-d">ax.set_yticks([-1, -1/2, 0, 1/2, 1])</span>
<span class="pd_green-d">plt.grid(True)</span>

<span class="pd_green-d">ax.set_xticklabels([r"$0$", r"$\tau/4$", r"$\tau/2$", r"$3\tau/4$", r"$\tau$"])</span>
<span class="pd_green-d">ax.set_yticklabels([r"$-1$", r"$-1/2$", r"$0$", r"$1/2$", r"$1$"])</span>

<span class="pd_green-d">ax.set_title("One period of cosine and sine", fontsize=16)</span>
<span class="pd_green-d">ax.set_xlabel(r"$\theta$", fontsize=16)</span>
<span class="pd_green-d">ax.set_ylabel(r"$f(\theta)$", fontsize=16)</span>

<span class="b-y"><span class="pd_green">ax.annotate(r"$</span><span class="pd_green">\</span><span class="pd_green">cos</span><span class="pd_green">\</span><span class="pd_green">theta$", xy=(1.75, -0.3), xytext=(0.5, -0.75),</span></span>
            <span class="b-y"><span class="pd_green">arrowprops="facecolor": "black", "width": 1, fontsize=16)</span></span>
<span class="b-y"><span class="pd_green">ax.annotate(r"$</span><span class="pd_green">\</span><span class="pd_green">sin</span><span class="pd_green">\</span><span class="pd_green">theta$", xy=(2.75, 0.5), xytext=(3.5, 0.75),</span></span>
            <span class="b-y"><span class="pd_green">arrowprops="facecolor": "black", "width": 1, fontsize=16)</span></span>


<span class="pd_green-d">ax.plot(x, np.cos(x))</span>
<span class="pd_green-d">ax.plot(x, np.sin(x))</span>
<span class="pd_green-d">plt.show()</span></code></pre>
<p class="indent">We see from <a href="ch11.xhtml#ch11fig13">Figure 11.13</a> that <span class="pd_green-d"><code><strong>xy</strong></code></span> indicates the point to be annotated, <span class="pd_green-d"><code><strong>xytext</strong></code></span> indicates the location of the annotation text, and <span class="pd_green-d"><code><strong>arrowprops</strong></code></span> determines the properties of the annotation arrow.</p>
<figure class="image-c" id="ch11fig13">
<img src="graphics/11fig13.jpg" alt="images" width="601" height="464"/>
<figcaption>
<p class="title-f"><strong>Figure 11.13:</strong> Adding annotations.</p>
</figcaption>
</figure>
<span epub:type="pagebreak" id="page_347"></span>
<p class="indent">Finally, let’s add custom colors and line styles, plus a higher resolution (in dots per inch, or <span class="pd_green-d"><code><strong>dpi</strong></code></span>). For convenience, the resulting code, shown in <a href="ch11.xhtml#ch11list8">Listing 11.8</a>, includes all of the commands needed to create the full figure (<a href="ch11.xhtml#ch11fig14">Figure 11.14</a>) from scratch.</p>
<figure class="image-c" id="ch11fig14">
<img src="graphics/11fig14.jpg" alt="images" width="601" height="464"/>
<figcaption>
<p class="title-f"><strong>Figure 11.14:</strong> The final fancy plot of cosine and sine.</p>
</figcaption>
</figure>
<p class="listing" id="ch11list8"><strong>Listing 11.8:</strong> The code for a fancy sinusoidal plot.</p>
<p class="codelink"><a href="ch11_images.xhtml#f347-01" id="f347-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_green-d">from math import tau</span>

<span class="pd_green-d">import numpy as np</span>
<span class="pd_green-d">import matplotlib.pyplot as plt</span>


<span class="pd_green-d">x = np.linspace(0, tau, 100)</span>

<span class="pd_green-d">fig, ax = plt.subplots()</span>

<span class="pd_green-d">ax.set_xticks([0, tau/4, tau/2, 3*tau/4, tau])</span>
<span class="pd_green-d">ax.set_yticks([-1, -1/2, 0, 1/2, 1])</span>
<span class="pd_green-d">plt.grid(True)</span>

<span class="pd_green-d">ax.set_xticklabels([r"$0$", r"$\tau/4$", r"$\tau/2$", r"$3\tau/4$", r"$\tau$"])</span>
<span class="pd_green-d">ax.set_yticklabels([r"$-1$", r"$-1/2$", r"$0$", r"$1/2$", r"$1$"])</span>

<span class="pd_green-d">ax.set_title("One period of cosine and sine", fontsize=16)</span>
<span class="pd_green-d">ax.set_xlabel(r"$\theta$", fontsize=16)</span>
<span class="pd_green-d">ax.set_ylabel(r"$f(\theta)$", fontsize=16)</span>

<span class="pd_green-d">ax.annotate(r"$\cos\theta$", xy=(1.75, -0.3), xytext=(0.5, -0.75),</span>
            <span class="pd_green-d">arrowprops={"facecolor": "black", "width": 1}, fontsize=16)</span>
<span class="pd_green-d">ax.annotate(r"$\sin\theta$", xy=(2.75, 0.5), xytext=(3.5, 0.75),</span>
            <span class="pd_green-d">arrowprops={"facecolor": "black", "width": 1}, fontsize=16)</span>

<span class="b-y"><span class="pd_green">fig.set_dpi(150)</span></span>

<span class="b-y"><span class="pd_green">ax.plot(x, np.cos(x), color="red", linestyle="dashed")</span></span>
<span class="b-y"><span class="pd_green">ax.plot(x, np.sin(x), color="blue", linestyle="dotted")</span></span>
<span class="pd_green-d">plt.show()</span></code></pre>
</div>
</section>
<section>
<h4 class="h4" id="sec11_3_2">11.3.2 Scatter Plots</h4>
<p class="noindent">The plot in <a href="ch11.xhtml#sec11_3_1">Section 11.3.1</a> introduced some of the key ideas of Matplotlib, and from this point there are a million possible ways to go. In this section and the next, we’ll focus on two kinds of visualizations especially important in data science: <em>scatter plots</em> <span epub:type="pagebreak" id="page_348"></span>and <em>histograms</em>. Don’t worry if everything doesn’t sink in right away; we’ll have ample opportunity to see further examples of both scatter plots and histograms in <a href="ch11.xhtml#sec11_5">Section 11.5</a>, <a href="ch11.xhtml#sec11_6">Section 11.6</a>, and <a href="ch11.xhtml#sec11_7">Section 11.7</a>.</p>
<p class="indent">A scatter plot just plots a bunch of discrete function values against the corresponding points, which is a great way to get an overall sense of what relationships the function values might satisfy. Let’s take a look at a concrete example to see what this means.</p>
<p class="indent">We’ll begin by generating some random points chosen from the <em>standard normal</em> distribution,<sup><a id="fn11_14a" href="ch11.xhtml#fn11_14">14</a></sup> which is a normal distribution (or “bell curve”) with an average value (mean) of 0 and a spread (standard deviation) of 1.<sup><a id="fn11_15a" href="ch11.xhtml#fn11_15">15</a></sup> We can obtain these values using <span epub:type="pagebreak" id="page_349"></span>NumPy’s <span class="pd_green-d"><code><strong>random</strong></code></span> library, which includes a default random number generator called <span class="pd_green-d"><code><strong>default_rng()</strong></code></span> (<a href="ch11.xhtml#ch11list9">Listing 11.9</a>).</p>
<p class="footnote"><a id="fn11_14" href="ch11.xhtml#fn11_14a">14.</a> There’s nothing “abnormal” about other distributions; use of the word “normal” is in large part an idiosyncrasy of history.</p>
<p class="footnote"><a id="fn11_15" href="ch11.xhtml#fn11_15a">15.</a> The functional form of the standard normal distribution is given by the probability density <img src="graphics/pg348-01.jpg" alt="images" width="146" height="36"/>, where <img src="graphics/pg348-02.jpg" alt="images" width="137" height="21"/> is a normalization factor to ensure that the total probability <img src="graphics/pg349-01.jpg" alt="images" width="94" height="22"/> is equal to 1. The density function for a general normal distribution with mean <em>μ</em> and standard deviation <em>σ</em> is <img src="graphics/pg349-02.jpg" alt="images" width="226" height="41"/>; setting <em>μ</em> = 0 and <em>σ</em> = 1 then yields the standard normal.</p>
<p class="listing" id="ch11list9"><strong>Listing 11.9:</strong> Generating random values using the standard normal distribution.</p>
<p class="codelink"><a href="ch11_images.xhtml#f349-01" id="f349-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>numpy.random</strong></span> <span class="pd_green"><strong>import</strong></span> default_rng
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> rng <span class="pd_gray">=</span> default_rng()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> n_pts <span class="pd_gray">= 50</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> x <span class="pd_gray">=</span> rng<span class="pd_gray">.</span>standard_normal(n_pts)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> x
<span class="pd_green-d">array([ 0.41256003,  0.67594205,  1.264653  ,  1.16351491, -0.41594407,</span>
       <span class="pd_green-d">-0.60157015,  0.84889823, -0.59984223,  0.24374326,  0.06055498,</span>
       <span class="pd_green-d">-0.48512829,  1.02253594, -1.10982933, -0.40609179,  0.55076245,</span>
        <span class="pd_green-d">0.13046238,  0.86712869,  0.06139358, -2.26538163,  1.45785923,</span>
       <span class="pd_green-d">-0.56220574, -1.38775239, -2.39643977, -0.77498392,  1.16794796,</span>
       <span class="pd_green-d">-0.6588802 ,  1.66343434,  1.57475219, -0.03374501, -0.62757059,</span>
       <span class="pd_green-d">-0.99378175,  0.69259747, -1.04555996,  0.62653116, -0.9042063 ,</span>
       <span class="pd_green-d">-0.32565268, -0.99762804, -0.4270288 ,  0.69940045, -0.46574267,</span>
        <span class="pd_green-d">1.82225132,  0.23925201, -1.0443741 , -0.54779683,  1.17466477,</span>
       <span class="pd_green-d">-2.54906663, -0.31495622,  0.25224765, -1.20869217, -1.02737145])</span></code></pre>
</div>
<p class="noindent">(You may see code like <span class="pd_green-d"><code><strong>random.standard_normal(50)</strong></code></span> in tutorial examples online, but this variant is now deprecated, and the technique shown in <a href="ch11.xhtml#ch11list9">Listing 11.9</a> is the current preferred method for generating random values with NumPy.)</p>
<p class="indent">With those <em>x</em> values in hand, let’s create a set of <em>y</em> values by adding a constant multiple (the slope) of 5 times <em>x</em> plus another random factor:</p>
<p class="codelink"><a href="ch11_images.xhtml#f349-02" id="f349-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> y <span class="pd_gray">= 5*</span>x <span class="pd_gray">+</span> rng<span class="pd_gray">.</span>standard_normal(n_pts)</code></pre>
<p class="noindent">This broadly follows the pattern of the equation for a line, <em>y</em> = <em>mx</em> + <em>b</em>, only with random values for <em>x</em> and <em>b</em>. Because the functional form of <em>y</em> is essentially linear, a plot of <em>y</em> vs. <em>x</em> should look roughly like a line, which we can confirm with a scatter plot as follows:</p>
<span epub:type="pagebreak" id="page_350"></span>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> fig, ax <span class="pd_gray">=</span> plt<span class="pd_gray">.</span>subplots()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> ax<span class="pd_gray">.</span>scatter(x, y)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent">As seen in <a href="ch11.xhtml#ch11fig15">Figure 11.15</a>, our guess was correct. (Because we didn’t fix a particular seed value for the random number generator, your exact results will differ.)</p>
<figure class="image-c" id="ch11fig15">
<img src="graphics/11fig15.jpg" alt="images" width="601" height="440"/>
<figcaption>
<p class="title-f"><strong>Figure 11.15:</strong> A Matplotlib scatter plot.</p>
</figcaption>
</figure>
</section>
<section>
<h4 class="h4" id="sec11_3_3">11.3.3 Histograms</h4>
<p class="noindent">Finally, let’s apply some of the same ideas from <a href="ch11.xhtml#sec11_3_2">Section 11.3.2</a> to visualize the distribution of 1000 random values drawn from the standard normal distribution:</p>
<p class="codelink"><a href="ch11_images.xhtml#f350-01" id="f350-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> values <span class="pd_gray">=</span> rng<span class="pd_gray">.</span>standard_normal(<span class="pd_gray">1000</span>)</code></pre>
<p class="indent">A common way to get a sense of what these values look like is to make a fixed number of “bins” and plot how many values fit into each bin. The resulting plot is known as a <em>histogram</em>, and can be generated automatically using Matplotlib’s <span class="pd_green-d"><code><strong>hist()</strong></code></span> method:</p>
<span epub:type="pagebreak" id="page_351"></span>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> fig, ax <span class="pd_gray">=</span> plt<span class="pd_gray">.</span>subplots()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> ax<span class="pd_gray">.</span>hist(values)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent">The result is a good approximation of a “bell curve”, as seen in <a href="ch11.xhtml#ch11fig16">Figure 11.16</a>.</p>
<figure class="image-c" id="ch11fig16">
<img src="graphics/11fig16.jpg" alt="images" width="601" height="441"/>
<figcaption>
<p class="title-f"><strong>Figure 11.16:</strong> A histogram of normally distributed random values.</p>
</figcaption>
</figure>
<p class="indent">The default number of bins is <span class="pd_green-d"><code><strong>10</strong></code></span>, but we can investigate the result of different bin sizes by passing a <span class="pd_green-d"><code><strong>bins</strong></code></span> argument to <span class="pd_green-d"><code><strong>hist()</strong></code></span>, say <span class="pd_green-d"><code><strong>bins=20</strong></code></span>:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> fig, ax <span class="pd_gray">=</span> plt<span class="pd_gray">.</span>subplots()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> ax<span class="pd_gray">.</span>hist(values, bin<span class="pd_gray">s=20</span>) <span class="pd_blue">
<strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent">In this case, the result is a finer-grained version of the distribution (<a href="ch11.xhtml#ch11fig17">Figure 11.17</a>).</p>
<figure class="image-c" id="ch11fig17">
<img src="graphics/11fig17.jpg" alt="images" width="601" height="442"/>
<figcaption>
<p class="title-f"><strong>Figure 11.17:</strong> A rebinned version of <a href="ch11.xhtml#ch11fig16">Figure 11.16</a>.</p>
</figcaption>
</figure>
<p class="indent">Because Matplotlib is a general system for plotting and data visualization, there’s practically no end to the things you can do with it. Although we’ve now covered the basics of what we’ll need for the rest of this tutorial, I encourage you to explore further, and the Matplotlib documentation is a good place to start.</p>
<span epub:type="pagebreak" id="page_352"></span>
</section>
<section>
<h4 class="h4" id="sec11_3_4">11.3.4 Exercises</h4>
<ol class="number">
<li><p class="number">Add a title and axis labels to the plot shown in <a href="ch11.xhtml#ch11fig15">Figure 11.15</a>.</p></li>
<li><p class="number">Add titles to the histograms in <a href="ch11.xhtml#sec11_3_3">Section 11.3.3</a>.</p></li>
<li><p class="number">One common plotting task is including multiple subplots in the same figure. Show that the code in <a href="ch11.xhtml#ch11list10">Listing 11.10</a> creates vertically stacked subplots, as shown in <a href="ch11.xhtml#ch11fig18">Figure 11.18</a>. (Here the <span class="pd_green-d"><code><strong>suptitle()</strong></code></span> method produces a “supertitle” that sits above both plots. See the Matplotlib documentation on subplots for other ways to create multiple subplots.)</p>
<figure class="image-c" id="ch11fig18">
<img src="graphics/11fig18.jpg" alt="images" width="601" height="489"/>
<figcaption>
<p class="title-f"><strong>Figure 11.18:</strong> Vertically stacked plots.</p>
</figcaption>
</figure></li>
<li><p class="number">Add a plot of the function cos<em>(x</em> − <em>τ/</em>8<em>)</em> to the plot in <a href="ch11.xhtml#ch11fig14">Figure 11.14</a> with color <span class="pd_green-d"><code><strong>"orange"</strong></code></span> and linestyle <span class="pd_green-d"><code><strong>"dashdot"</strong></code></span>. <em>Extra credit</em>: Add an annotation as well. (The extra-credit step is <em>much</em> easier in an interactive Jupyter notebook, especially when finding the right coordinates for the annotation label and arrow.)</p></li>
</ol>
<span epub:type="pagebreak" id="page_353"></span>
<p class="listing" id="ch11list10"><strong>Listing 11.10:</strong> Stacking subplots.</p>
<p class="codelink"><a href="ch11_images.xhtml#f353-01" id="f353-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> x <span class="pd_gray">=</span> np<span class="pd_gray">.</span>linspace(<span class="pd_gray">0</span>, tau, <span class="pd_gray">100</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> fig, (ax1, ax2) <span class="pd_gray">=</span> plt<span class="pd_gray">.</span>subplots(<span class="pd_gray">2</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> fig<span class="pd_gray">.</span>suptitle(<span class="pd_red">r"Vertically stacked plots of $\cos\theta$ and $\sin\theta$."</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> ax1<span class="pd_gray">.</span>plot(x, np<span class="pd_gray">.</span>cos(x))
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> ax2<span class="pd_gray">.</span>plot(x, np<span class="pd_gray">.</span>sin(x))</code></pre>
</div>
</section>
</section>
<section>
<h3 class="h3" id="sec11_4">11.4 Introduction to Data Analysis with pandas</h3>
<p class="noindent">One of the most heavily used tools in Python data science is <em>pandas</em>, a powerful library for analyzing data. In essence, pandas (from “<strong>pan</strong>el <strong>da</strong>ta”) lets us perform many of the same tasks as a spreadsheet or Structured Query Language (SQL), only with the <span epub:type="pagebreak" id="page_354"></span>power and flexibility of a full-strength, general-purpose programming language under the hood (<a href="ch11.xhtml#ch11fig19">Figure 11.19</a><sup><a id="fn11_16a" href="ch11.xhtml#fn11_16">16</a></sup>).</p>
<figure class="image-c" id="ch11fig19">
<img src="graphics/11fig19.jpg" alt="images" width="725" height="483"/>
<figcaption>
<p class="title-f"><strong>Figure 11.19:</strong> Pandas are famous for their love of bamboo and their remarkable aptitude for data science.</p>
</figcaption>
</figure>
<p class="footnote"><a id="fn11_16" href="ch11.xhtml#fn11_16a">16.</a> Image courtesy of San Hoyano/Shutterstock.</p>
<p class="indent">The pandas interface can take some getting used to, and there’s no substitute for seeing lots of examples. Thus, this chapter covers three cases of increasing sophistication, starting with simplified handcrafted examples (<a href="ch11.xhtml#sec11_4_1">Section 11.4.1</a>) and then showing more complex analysis techniques using two real-world datasets: Nobel Prizes (<a href="ch11.xhtml#sec11_5">Section 11.5</a>) and survival rates from <em>Titanic</em> (<a href="ch11.xhtml#sec11_6">Section 11.6</a>). (The second of these datasets will also serve as our main source of examples on machine learning in <a href="ch11.xhtml#sec11_7">Section 11.7</a>.)</p>
<p class="indent">In addition, there’s <em>really</em> no substitute for asking and answering questions for yourself. In my experience, following tutorials such as this one can give you a great start, and often yields easy-looking results like <a href="ch11.xhtml#ch11fig20">Figure 11.20</a>. But the minute you deviate even a millimeter from carefully chosen examples and try to answer something for yourself, you end up with things that look like <a href="ch11.xhtml#ch11fig21">Figure 11.21</a>.</p>
<span epub:type="pagebreak" id="page_355"></span>
<figure class="image-c" id="ch11fig20">
<img src="graphics/11fig20.jpg" alt="images" width="725" height="508"/>
<figcaption>
<p class="title-f"><strong>Figure 11.20:</strong> Making pandas look easy.</p>
</figcaption>
</figure>
<figure class="image-c" id="ch11fig21">
<img src="graphics/11fig21.jpg" alt="images" width="725" height="525"/>
<figcaption>
<p class="title-f"><strong>Figure 11.21:</strong> The often hard reality.</p>
</figcaption>
</figure>
<p class="indent">My best suggestion is to follow along at first to get your pandas bearings and then launch into the investigations of your own questions. But if you feel inspired at any point to venture out on your own, don’t let me stop you—just know what to expect if you do.</p>
<section>
<h4 class="h4" id="sec11_4_1">11.4.1 Handcrafted Examples</h4>
<p class="noindent">The first steps to getting started are nearly always to import NumPy as <span class="pd_green-d"><code><strong>np</strong></code></span> and pandas as <span class="pd_green-d"><code><strong>pd</strong></code></span>, along with <span class="pd_green-d"><code><strong>matplotlib.pyplot</strong></code></span> as <span class="pd_green-d"><code><strong>plt</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f355-01" id="f355-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green-d"><strong>import</strong></span> <span class="pd_nila"><strong>numpy</strong></span> <span class="pd_green-d"><strong>as</strong></span> <span class="pd_nila"><strong>np</strong></span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green-d"><strong>import</strong></span> <span class="pd_nila"><strong>pandas</strong></span> <span class="pd_green-d"><strong>as</strong></span> <span class="pd_nila"><strong>pd</strong></span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green-d"><strong>import</strong></span> <span class="pd_nila"><strong>matplotlib.pyplot</strong></span> <span class="pd_green-d"><strong>as</strong></span> <span class="pd_nila"><strong>plt</strong></span></code></pre>
<p class="noindent">The core data structures of pandas are <span class="pd_green-d"><code><strong>Series</strong></code></span> and <span class="pd_green-d"><code><strong>DataFrame</strong></code></span>. The latter is more important, but it’s built up from the former, so that’s where we’ll start.</p>
<span epub:type="pagebreak" id="page_356"></span>
<section>
<h5 class="h5" id="sec11_1_1_1">Series</h5>
<p class="noindent">A Series is essentially a fancy array with elements of arbitrary types (much like a list), each of which is called an <em>axis</em>. For example, the following command defines a Series of numbers and strings, plus a special (and commonly encountered) value known as <em>NaN</em> (“Not a Number”):</p>
<p class="codelink"><a href="ch11_images.xhtml#f356-01" id="f356-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> pd<span class="pd_gray">.</span>Series([<span class="pd_gray">1</span>, <span class="pd_gray">2</span>, <span class="pd_gray">3</span>, <span class="pd_red">"foo"</span>, np<span class="pd_gray">.</span>nan, <span class="pd_red">"bar"</span>])
<span class="pd_green-d">0      1</span>
<span class="pd_green-d">1      2</span>
<span class="pd_green-d">2      3</span>
<span class="pd_green-d">3    foo</span>
<span class="pd_green-d">4    NaN</span>
<span class="pd_green-d">5    bar</span>
<span class="pd_green-d">dtype: object</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> pd<span class="pd_gray">.</span>Series([<span class="pd_gray">1</span>, <span class="pd_gray">2</span>, <span class="pd_gray">3</span>, <span class="pd_red">"foo"</span>, np<span class="pd_gray">.</span>nan, <span class="pd_red">"bar"</span>])<span class="pd_gray">.</span>dropna()
<span class="pd_green-d">0      1</span>
<span epub:type="pagebreak" id="page_357"></span><span class="pd_green-d">1      2</span>
<span class="pd_green-d">2      3</span>
<span class="pd_green-d">3    foo</span>
<span class="pd_green-d">5    bar</span>
<span class="pd_green-d">dtype: object</span></code></pre>
<p class="noindent">The second command here shows how to clean the data a bit using the <span class="pd_green-d"><code><strong>dropna()</strong></code></span> method, which drops any “Not Available” values, such as <span class="pd_green-d"><code><strong>None</strong></code></span>, <span class="pd_green-d"><code><strong>NaN</strong></code></span>, or <span class="pd_green-d"><code><strong>NaT</strong></code></span> (“Not a Time”).</p>
<p class="indent">By default, Series axis labels are numbered just like array indices (in this case, <span class="pd_green-d"><code><strong>0</strong></code></span>–<span class="pd_green-d"><code><strong>5</strong></code></span>). The set of axes is known as the <em>index</em> of the Series:</p>
<p class="codelink"><a href="ch11_images.xhtml#f357-01" id="f357-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> pd<span class="pd_gray">.</span>Series([<span class="pd_gray">1</span>, <span class="pd_gray">2</span>, <span class="pd_gray">3</span>, <span class="pd_red">"foo"</span>, np<span class="pd_gray">.</span>nan, <span class="pd_red">"bar"</span>])<span class="pd_gray">.</span>index
<span class="pd_green-d">RangeIndex(start=0, stop=6, step=1)</span></code></pre>
<p class="indent">It’s also possible to define our own axis labels, which must have the same number of elements as the Series:</p>
<p class="codelink"><a href="ch11_images.xhtml#f357-02" id="f357-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>numpy.random</strong></span> <span class="pd_green"><strong>import</strong></span> default_rng
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> rng <span class="pd_gray">=</span> default_rng()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> s <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>Series(rng<span class="pd_gray">.</span>standard_normal(<span class="pd_gray">5</span>), index<span class="pd_gray">=</span>[<span class="pd_red">"a"</span>, <span class="pd_red">"b"</span>, <span class="pd_red">"c"</span>, <span class="pd_red">"d"</span>, <span class="pd_red">"e"</span>])
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> s
<span class="pd_green-d">a     0.770407</span>
<span class="pd_green-d">b    -0.698040</span>
<span class="pd_green-d">c     1.977234</span>
<span class="pd_green-d">d    -1.559065</span>
<span class="pd_green-d">e    -0.713496</span>
<span class="pd_green-d">dtype: float64</span></code></pre>
<p class="noindent">Series act both like NumPy ndarrays and like regular Python dictionaries:</p>
<p class="codelink"><a href="ch11_images.xhtml#f357-03" id="f357-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> s[<span class="pd_gray">0</span>]                <span class="pd_blue1"><em># Acting like an ndarray</em></span>
<span class="pd_green-d">0.7704065892197263</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> s[<span class="pd_gray">1</span>:<span class="pd_gray">3</span>]              <span class="pd_blue1"><em># Supports slicing</em></span>
<span class="pd_green-d">b   -0.698040</span>
<span class="pd_green-d">c    1.977234</span>
<span class="pd_green-d">dtype: float64</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> s<span class="pd_red">["c"</span>]              <span class="pd_blue1"><em># Access by axis label</em></span>
<span class="pd_green-d">1.977233512910388</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> s<span class="pd_gray">.</span>keys()            <span class="pd_blue1"><em># Keys are just the Series index.</em></span>
<span class="pd_green-d">Index(['a', 'b', 'c', 'd', 'e'], dtype='object')</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> s<span class="pd_gray">.</span>index
<span class="pd_green-d">Index(['a', 'b', 'c', 'd', 'e'], dtype='object')</span></code></pre>
<p class="indent"><span epub:type="pagebreak" id="page_358"></span>Series come equipped with a wealth of methods, including plotting methods that use Matplotlib (<a href="ch11.xhtml#sec11_3">Section 11.3</a>) under the hood. For example, here’s a histogram for a Series with 1000 values generated with the standard normal distribution:</p>
<p class="codelink"><a href="ch11_images.xhtml#f358-01" id="f358-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> s <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>Series(rng<span class="pd_gray">.</span>standard_normal(<span class="pd_gray">1000</span>))
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> s<span class="pd_gray">.</span>hist()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>show()</code></pre>
<p class="indent">Apart from minor formatting differences, the result (<a href="ch11.xhtml#ch11fig22">Figure 11.22</a>) is essentially the same as the histogram we created directly in <a href="ch11.xhtml#sec11_3_3">Section 11.3.3</a> (<a href="ch11.xhtml#ch11fig16">Figure 11.16</a>).</p>
<figure class="image-c" id="ch11fig22">
<img src="graphics/11fig22.jpg" alt="images" width="601" height="441"/>
<figcaption>
<p class="title-f"><strong>Figure 11.22:</strong> A Series histogram.</p>
</figcaption>
</figure>
</section>
<section>
<h5 class="h5" id="sec11_1_1_2">DataFrame</h5>
<p class="noindent">The other main pandas object type, known as a <span class="pd_green-d"><code><strong>DataFrame</strong></code></span> object, is the heart of Python data analysis. A DataFrame can be thought of as a two-dimensional grid of cells containing arbitrary data types—roughly equivalent to an Excel worksheet. In this section, we’ll create a few simple DataFrames by hand just to get a sense of how they work, but it’s worth bearing in mind that most real-world DataFrame objects are <span epub:type="pagebreak" id="page_359"></span>created by importing data from files (or even from live URLs), a technique we’ll cover starting in <a href="ch11.xhtml#sec11_5">Section 11.5</a>.</p>
<p class="indent">There are a large number of ways to initialize or build DataFrames appropriate to a correspondingly large number of circumstances. For example, one option is to initialize it with a Python dictionary, as shown in <a href="ch11.xhtml#ch11list11">Listing 11.11</a>.</p>
<p class="listing" id="ch11list11"><strong>Listing 11.11:</strong> Initializing a DataFrame with a dictionary.</p>
<p class="codelink"><a href="ch11_images.xhtml#f359-01" id="f359-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>math</strong></span> <span class="pd_green"><strong>import</strong></span> tau
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>numpy.random</strong></span> <span class="pd_green"><strong>import</strong></span> default_rng
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> rng <span class="pd_gray">=</span> default_rng()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> df <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>DataFrame(
<span class="pd_blue"><strong>...</strong></span>     {
<span class="pd_blue"><strong>...</strong></span>         <span class="pd_red">"Number"</span>: <span class="pd_gray">1.0</span>,
<span class="pd_blue"><strong>...</strong></span>         <span class="pd_red">"String"</span>: <span class="pd_red">"foo"</span>,
<span class="pd_blue"><strong>...</strong></span>         <span class="pd_red">"Angles"</span>: np<span class="pd_gray">.</span>linspace(<span class="pd_gray">0</span>, tau, <span class="pd_gray">5</span>),
<span class="pd_blue"><strong>...</strong></span>         <span class="pd_red">"Random"</span>: pd<span class="pd_gray">.</span>Series(rng<span class="pd_gray">.</span>standard_normal(<span class="pd_gray">5</span>)),
<span class="pd_blue"><strong>...</strong></span>         <span class="pd_red">"Timestamp"</span>: pd<span class="pd_gray">.</span>Timestamp(<span class="pd_red">"20221020"</span>),
<span class="pd_blue"><strong>...</strong></span>         <span class="pd_red">"Size"</span>: pd<span class="pd_gray">.</span>Categorical(<span class="pd_red">["tiny"</span>, <span class="pd_red">"small"</span>, <span class="pd_red">"mid"</span>, <span class="pd_red">"big"</span>, <span class="pd_red">"huge"</span>])
<span class="pd_blue"><strong>...</strong></span>     })
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> df
   <span class="pd_green-d">Number String     Angles     Random  Timestamp   Size</span>
<span class="pd_green-d">0     1.0    foo   0.000000  -1.954002 2022-10-20   tiny</span>
<span class="pd_green-d">1     1.0    foo   1.570796  0.967171  2022-10-20  small</span>
<span class="pd_green-d">2     1.0    foo   3.141593  -1.149739 2022-10-20    mid</span>
<span class="pd_green-d">3     1.0    foo   4.712389  -0.084962 2022-10-20    big</span>
<span class="pd_green-d">4     1.0    foo   6.283185  0.310634  2022-10-20   huge</span></code></pre>
</div>
<p class="noindent">Here we’ve applied the <span class="pd_green-d"><code><strong>linspace()</strong></code></span> method from <a href="ch11.xhtml#sec11_2_3">Section 11.2.3</a> and two new pandas methods, <span class="pd_green-d"><code><strong>TimeStamp</strong></code></span> (just what it sounds like) and <span class="pd_green-d"><code><strong>Categorical</strong></code></span> (which contains values of a <em>categorical variable</em>). The result is a set of labeled rows and columns with a heterogeneous set of data.</p>
<p class="indent">We can access a DataFrame column using the column name as a key:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> df<span class="pd_red">["Size"</span>]
<span class="pd_green-d">0      tiny</span>
<span class="pd_green-d">1     small</span>
<span class="pd_green-d">2       mid</span>
<span class="pd_green-d">3       big</span>
<span class="pd_green-d">4      huge</span></code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_360"></span>We can also calculate statistics, such as the mean value of the <span class="pd_green-d"><code><strong>Random</strong></code></span> column:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> df[<span class="pd_red">"Random"</span>]<span class="pd_gray">.</span>mean()
<span class="pd_green-d">-0.3821796291792846</span></code></pre>
<p class="indent">One useful pandas function for getting a general overview of numeric data is <span class="pd_green-d"><code><strong>describe()</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f360-01" id="f360-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> df<span class="pd_gray">.</span>describe()
<span class="pd_green-d">      Number     Angles     Random</span>
<span class="pd_green-d">count    5.0   5.000000   5.000000</span>
<span class="pd_green-d">mean     1.0   3.141593  -0.382180</span>
<span class="pd_green-d">std      0.0   2.483647   1.167138</span>
<span class="pd_green-d">min      1.0   0.000000  -1.954002</span>
<span class="pd_green-d">25%      1.0   1.570796  -1.149739</span>
<span class="pd_green-d">50%      1.0   3.141593  -0.084962</span>
<span class="pd_green-d">75%      1.0   4.712389   0.310634</span>
<span class="pd_green-d">max      1.0   6.283185   0.96717</span></code></pre>
<p class="noindent">This automatically displays the total count, mean, standard deviation, minimum, and maximum values, and the middle three quartiles (25%, 50%, and 75%) of each numeric column. These values won’t always be meaningful—the standard deviation of the linearly spaced angles, for example, doesn’t really tell us anything useful—but <span class="pd_green-d"><code><strong>describe()</strong></code></span> is often helpful as a first step in an analysis. We’ll see examples of two other useful summary methods, <span class="pd_green-d"><code><strong>head()</strong></code></span> and <span class="pd_green-d"><code><strong>info()</strong></code></span>, starting in <a href="ch11.xhtml#sec11_5">Section 11.5</a>.</p>
<p class="indent">Another useful method is <span class="pd_green-d"><code><strong>map()</strong></code></span>, which we can use to map categorical values to numbers. Suppose, for example, that <span class="pd_green-d"><code><strong>"Size"</strong></code></span> corresponds to drink sizes in ounces, which we can represent as a <span class="pd_green-d"><code><strong>sizes</strong></code></span> dictionary. Using <span class="pd_green-d"><code><strong>map()</strong></code></span> on the <span class="pd_green-d"><code><strong>"Size"</strong></code></span> column then gives the desired result (<a href="ch11.xhtml#ch11list12">Listing 11.12</a>).</p>
<p class="listing" id="ch11list12"><strong>Listing 11.12:</strong> Using <span class="pd_green-d"><code><strong>map()</strong></code></span> to modify values.</p>
<p class="codelink"><a href="ch11_images.xhtml#f360-02" id="f360-02a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> sizes <span class="pd_gray">=</span> {<span class="pd_red">"tiny"</span>: <span class="pd_gray">4</span>, <span class="pd_red">"small"</span>: <span class="pd_gray">8</span>, <span class="pd_red">"mid"</span>: <span class="pd_gray">12</span>, <span class="pd_red">"big"</span>: <span class="pd_gray">16</span>, <span class="pd_red">"huge"</span>: <span class="pd_gray">24</span>}
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> df[<span class="pd_red">"Size"</span>]<span class="pd_gray">.</span>map(sizes)
<span class="pd_green-d">0     4</span>
<span class="pd_green-d">1     8</span>
<span class="pd_green-d">2    12</span>
<span class="pd_green-d">3    16</span>
<span class="pd_green-d">4    24</span></code></pre>
</div>
<p class="noindent"><span epub:type="pagebreak" id="page_361"></span>This technique is especially valuable when applying machine-learning algorithms (<a href="ch11.xhtml#sec11_7">Section 11.7</a>), which can’t typically handle categorical data but do just fine with integers or floats.</p>
</section>
</section>
<section>
<h4 class="h4" id="sec11_4_2">11.4.2 Exercise</h4>
<ol class="number">
<li><p class="number">The <span class="pd_green-d"><code><strong>info()</strong></code></span> method provides an overview of a DataFrame that is complementary to <span class="pd_green-d"><code><strong>describe()</strong></code></span>. What is the result of running <span class="pd_green-d"><code><strong>df.info()</strong></code></span> on the DataFrame defined in <a href="ch11.xhtml#ch11list11">Listing 11.11</a>?</p></li>
</ol>
</section>
</section>
<section>
<h3 class="h3" id="sec11_5">11.5 pandas Example: Nobel Laureates</h3>
<p class="noindent">In <a href="ch11.xhtml#sec11_4">Section 11.4</a>, we got a glimpse of how to use pandas and what good it does us, but doing anything interesting typically requires bigger datasets, which are cumbersome to create by hand. Instead, the most common practice is to load data from external files and then take the analysis from there. Accordingly, in this section and the next (<a href="ch11.xhtml#sec11_6">Section 11.6</a>), we’ll read the initial data from what is probably the most common input format, CSV files (for “comma-separated values”).</p>
<p class="indent">Our first step is to download a dataset on winners of the Nobel Prize, who are typically known as <em>laureates</em> (a reference to the ancient practice of using wreaths from a laurel tree to honor great accomplishments).<sup><a id="fn11_17a" href="ch11.xhtml#fn11_17">17</a></sup> We can do this by using the <span class="pd_green-d"><code><strong>curl</strong></code></span> command-line command in the same directory being used for the data analysis:<sup><a id="fn11_18a" href="ch11.xhtml#fn11_18">18</a></sup></p>
<p class="footnote"><a id="fn11_17" href="ch11.xhtml#fn11_17a">17.</a> This section draws on the excellent pandas section from Python for Scientific Computing.</p>
<p class="footnote"><a id="fn11_18" href="ch11.xhtml#fn11_18a">18.</a> This data was originally downloaded directly from the official Nobel Prize website at <a href="http://api.nobelprize.org/v1/laureate.csv">http://api.nobelprize.org/v1/laureate.csv</a>. It has been uploaded to the Learn Enough CDN for maximum compatibility in case the version at nobelprize.org changes or disappears.</p>
<p class="codelink"><a href="ch11_images.xhtml#f361-01" id="f361-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>(venv) $</strong></span> curl -OL https://cdn.learnenough.com/laureates.csv</code></pre>
<p class="noindent">We can then read the data using pandas’ <span class="pd_green-d"><code><strong>read_csv()</strong></code></span> function:</p>
<p class="codelink"><a href="ch11_images.xhtml#f361-02" id="f361-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>read_csv(<span class="pd_red">"laureates.csv"</span>)</code></pre>
<p class="noindent">The statistics for the numeric columns aren’t very meaningful, so <span class="pd_green-d"><code><strong>describe()</strong></code></span> doesn’t tell us much:</p>
<span epub:type="pagebreak" id="page_362"></span>
<p class="codelink"><a href="ch11_images.xhtml#f362-01" id="f362-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>describe()
                    <span class="pd_green-d">id         year        share</span>
<span class="pd_green-d">count       975.000000   975.000000   975.000000</span>
<span class="pd_green-d">mean        496.221538  1972.471795     2.014359</span>
<span class="pd_green-d">std         290.594353    34.058064     0.943909</span>
<span class="pd_green-d">min           1.000000  1901.000000     1.000000</span>
<span class="pd_green-d">25%         244.500000  1948.500000     1.000000</span>
<span class="pd_green-d">50%         488.000000  1978.000000     2.000000</span>
<span class="pd_green-d">75%         746.500000  2001.000000     3.000000</span>
<span class="pd_green-d">max        1009.000000  2021.000000     4.000000</span></code></pre>
<p class="noindent">We can get something a little more useful with <span class="pd_green-d"><code><strong>head()</strong></code></span> (<a href="ch11.xhtml#ch11list13">Listing 11.13</a>).</p>
<p class="listing" id="ch11list13"><strong>Listing 11.13:</strong> Looking at the <span class="pd_green-d"><code><strong>head()</strong></code></span> of the Nobel Prize data.</p>
<p class="codelink"><a href="ch11_images.xhtml#f362-02" id="f362-02a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>head()
   <span class="pd_green-d">id       firstname  ...       city            country</span>
<span class="pd_green-d">0   1  Wilhelm Conrad  ...     Munich            Germany</span>
<span class="pd_green-d">1   2      Hendrik A.  ...     Leiden    the Netherlands</span>
<span class="pd_green-d">2   3          Pieter  ...  Amsterdam    the Netherlands</span>
<span class="pd_green-d">3   4           Henri  ...      Paris             France</span>
<span class="pd_green-d">4   5          Pierre  ...      Paris             France</span>
<span class="pd_green-d">[5 rows x 20 columns]</span></code></pre>
</div>
<p class="noindent">Here we’ve used the <span class="pd_green-d"><code><strong>head()</strong></code></span> method to take a peek at the first few entries; in a Jupyter notebook, you can scroll to see all of the columns, but in the terminal we see only a few. We can get more useful info using <span class="pd_green-d"><code><strong>info()</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f362-03" id="f362-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>info()
<span class="pd_green-d">&lt;class 'pandas.core.frame.DataFrame'&gt;</span>
<span class="pd_green-d">RangeIndex: 975 entries, 0 to 974</span>
<span class="pd_green-d">Data columns (total 20 columns):</span>
<span class="pd_green-d">#    Column             Non-Null Count   Dtype</span>
<span class="pd_green-d">---  ------             --------------   -----</span>
<span class="pd_green-d">0    id                 975 non-null     int64</span>
<span class="pd_green-d">1    firstname          975 non-null     object</span>
<span class="pd_green-d">2    surname            945 non-null     object</span>
<span class="pd_green-d">3    born               974 non-null     object</span>
<span class="pd_green-d">4    died               975 non-null     object</span>
<span class="pd_green-d">5    bornCountry        946 non-null     object</span>
<span class="pd_green-d">6    bornCountryCode    946 non-null     object</span>
<span class="pd_green-d">7    bornCity           943 non-null     object</span>
<span class="pd_green-d">8    diedCountry        640 non-null     object</span>
<span class="pd_green-d">9    diedCountryCode    640 non-null     object</span>
<span epub:type="pagebreak" id="page_363"></span><span class="pd_green-d">10   diedCity           634 non-null     object</span>
<span class="pd_green-d">11   gender             975 non-null     object</span>
<span class="pd_green-d">12   year               975 non-null     int64</span>
<span class="pd_green-d">13   category           975 non-null     object</span>
<span class="pd_green-d">14   overallMotivation  23 non-null      object</span>
<span class="pd_green-d">15   share              975 non-null     int64</span>
<span class="pd_green-d">16   motivation         975 non-null     object</span>
<span class="pd_green-d">17   name               717 non-null     object</span>
<span class="pd_green-d">18   city               712 non-null     object</span>
<span class="pd_green-d">19   country            713 non-null     object</span>
<span class="pd_green-d">dtypes: int64(3), object(17)</span>
<span class="pd_green-d">memory usage: 152.5+ KB</span></code></pre>
<p class="noindent">Here we see a complete list of the column names, together with the number of non-null values for each one.</p>
<section>
<h4 class="h4" id="sec11_5_1a"></h4>
<section>
<h5 class="h5" id="sec11_1_1_3">Locating Data</h5>
<p class="noindent">One of the most useful tasks in pandas is locating data that satisfies desired criteria. For example, we can locate a Nobel laureate with a particular surname. As a Caltech graduate, I am contractually obligated to use one of Caltech’s most beloved figures, physicist Richard Feynman (pronounced “FINE-mən”). In addition to his groundbreaking work in theoretical physics (especially quantum electrodynamics and its associated Feynman diagrams), Feynman is known for <em>The Feynman Lectures on Physics</em>, which covers the elementary physics curriculum (mechanics, thermal physics, electrodynamics, etc.) in an unusually entertaining and insightful way.</p>
<p class="indent">Let’s use square brackets and a boolean criterion on the <span class="pd_green-d"><code><strong>"surname"</strong></code></span> column to find Feynman’s record in the laureates data:<sup><a id="fn11_19a" href="ch11.xhtml#fn11_19">19</a></sup></p>
<p class="footnote"><a id="fn11_19" href="ch11.xhtml#fn11_19a">19.</a> From here on out, unimportant output such as <span class="pd_green-d"><code><strong>[1 rows x 20 columns]</strong></code></span> and <span class="pd_green-d"><code><strong>Name: year, dtype: int64</strong></code></span> will generally be omitted for brevity.</p>
<p class="codelink"><a href="ch11_images.xhtml#f363-01" id="f363-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel[nobel[<span class="pd_red">"surname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Feynman"</span>]
    <span class="pd_green-d">id   firstname  ...        city country</span>
<span class="pd_green-d">86  86   Richard P. ... Pasadena CA    USA</span></code></pre>
<p class="noindent">This array-style notation returns the full record, which allows us to determine the year Feynman won his Nobel Prize. In a Jupyter notebook, you can probably just scroll to the side and read it off (<a href="ch11.xhtml#ch11fig23">Figure 11.23</a>), but in the REPL we can look directly at the <span class="pd_green-d"><code><strong>year</strong></code></span> attribute:</p>
<span epub:type="pagebreak" id="page_364"></span>
<figure class="image-c" id="ch11fig23">
<img src="graphics/11fig23.jpg" alt="images" width="725" height="140"/>
<figcaption>
<p class="title-f"><strong>Figure 11.23:</strong> Examining a pandas record in a Jupyter notebook.</p>
</figcaption>
</figure>
<p class="codelink"><a href="ch11_images.xhtml#f364-01" id="f364-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel[nobel[<span class="pd_red">"surname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Feynman"</span>]<span class="pd_gray">.</span>year
<span class="pd_green-d">86    1965</span></code></pre>
<p class="noindent">This method also allows us to, e.g., assign it to a variable, which is potentially more useful than inspecting it by eye.</p>
<p class="indent">By the way, the syntax</p>
<p class="codelink"><a href="ch11_images.xhtml#f364-02" id="f364-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel[nobel[<span class="pd_red">"surname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Feynman"</span>]</code></pre>
<p class="noindent">can be a little confusing since it might not be clear why we have to refer to <span class="pd_green-d"><code><strong>nobel</strong></code></span> twice. The answer is that the inner part of the syntax returns a Series (<a href="ch11.xhtml#sec11_4_1">Section 11.4.1</a>) consisting of boolean values for every laureate, with <span class="pd_green-d"><code><strong>True</strong></code></span> if the surname is equal to <span class="pd_green-d"><code><strong>"Feynman"</strong></code></span> and <span class="pd_green-d"><code><strong>False</strong></code></span> otherwise:</p>
<p class="codelink"><a href="ch11_images.xhtml#f364-03" id="f364-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel[<span class="pd_red">"surname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Feynman"</span>
<span class="pd_green-d">0      False</span>
<span class="pd_green-d">1      False</span>
<span class="pd_green-d">2      False</span>
<span class="pd_green-d">3      False</span>
<span class="pd_green-d">4      False</span>
<span class="pd_green-d">       ...</span>
<span class="pd_green-d">970    False</span>
<span class="pd_green-d">971    False</span>
<span class="pd_green-d">972    False</span>
<span class="pd_green-d">973    False</span>
<span class="pd_green-d">974    False</span></code></pre>
<p class="noindent">By using the correct index (i.e., <span class="pd_green-d"><code><strong>86</strong></code></span>), we can confirm that the value in that case is <span class="pd_green-d"><code><strong>True</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f364-04" id="f364-04a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> (nobel[<span class="pd_red">"surname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Feynman"</span>)[<span class="pd_gray">86</span>]
<span class="pd_green-d">True</span></code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_365"></span>In this way, we arrange for</p>
<p class="codelink"><a href="ch11_images.xhtml#f365-01" id="f365-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel[nobel[<span class="pd_red">"surname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Feynman"</span>]</code></pre>
<p class="noindent">to select only the values of <span class="pd_green-d"><code><strong>nobel</strong></code></span> where <span class="pd_green-d"><code><strong>nobel["surname"] == "Feynman"</strong></code></span> is <span class="pd_green-d"><code><strong>True</strong></code></span>. This is similar to the <span class="pd_green-d"><code><strong>isclose()</strong></code></span> trick shown in <a href="ch11.xhtml#ch11list7">Listing 11.7</a>, where we used an ndarray of booleans to select the elements in a matrix close to 0 (and set them to exactly 0).</p>
<p class="indent">Another method for getting the year is by specifying the column along with the boolean criterion, which we might try like this (only the most relevant line of output is shown):</p>
<p class="codelink"><a href="ch11_images.xhtml#f365-02" id="f365-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel[nobel[<span class="pd_red">"surname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Feynman"</span>, <span class="pd_red">"year"</span>]
<span class="pd_green-d">pandas.errors.InvalidIndexError</span></code></pre>
<p class="noindent">This doesn’t work, but we can accomplish what we want using the <span class="pd_green-d"><code><strong>loc</strong></code></span> (“location”) attribute:<sup><a id="fn11_20a" href="ch11.xhtml#fn11_20">20</a></sup></p>
<p class="footnote"><a id="fn11_20" href="ch11.xhtml#fn11_20a">20.</a> More specifically, <span class="pd_green-d"><code><strong>loc</strong></code></span> is a <em>property</em>, which is a special kind of attribute created using a property decorator.</p>
<p class="codelink"><a href="ch11_images.xhtml#f365-03" id="f365-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"surname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Feynman"</span>, <span class="pd_red">"year"</span>]
<span class="pd_green-d">86    1965</span></code></pre>
<p class="noindent">This returns just the overall id (in this case, <span class="pd_green-d"><code><strong>86</strong></code></span>) and the column of interest. The <span class="pd_green-d"><code><strong>loc</strong></code></span> attribute can be used in place of brackets in many places and is generally a more flexible way to pull out data items of interest.</p>
<p class="indent">After I finished my Ph.D., I was recruited to work on a <em>Feynman Lectures</em> project (<a href="https://www.michaelhartl.com/feynman-lectures/">https://www.michaelhartl.com/feynman-lectures/</a>) by Kip Thorne, who was one of my mentors at Caltech (<a href="ch11.xhtml#ch11fig24">Figure 11.24</a><sup><a id="fn11_21a" href="ch11.xhtml#fn11_21">21</a></sup>). Kip went on to win a Nobel Prize himself, so let’s figure out which year.</p>
<figure class="image-c" id="ch11fig24">
<img src="graphics/11fig24.jpg" alt="images" width="724" height="434"/>
<figcaption>
<p class="title-f"><strong>Figure 11.24:</strong> The author with Nobel laureate Kip Thorne and Stephen Hawking.</p>
</figcaption>
</figure>
<p class="footnote"><a id="fn11_21" href="ch11.xhtml#fn11_21a">21.</a> Image copyright © 2012 Michael Hartl.</p>
<p class="indent">We could search by surname as we did with Feynman, but Kip insists on being called “Kip”, so let’s search by first name instead:</p>
<p class="codelink"><a href="ch11_images.xhtml#f365-04" id="f365-04a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"firstname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Kip"</span>]
<span class="pd_green-d">Empty DataFrame</span></code></pre>
<p class="noindent">Hmm, the result is empty. Looking back at the <span class="pd_green-d"><code><strong>head()</strong></code></span> in <a href="ch11.xhtml#ch11list13">Listing 11.13</a>, we can guess why; for example, the entry for Hendrik Lorentz includes a middle initial, so perhaps <span epub:type="pagebreak" id="page_366"></span>the same is the case for Kip’s entry in the DataFrame. Kip’s middle initial is “S.” (for Stephen), so let’s include that in our comparison:</p>
<p class="codelink"><a href="ch11_images.xhtml#f366-01" id="f366-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"firstname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Kip S."</span>]
      <span class="pd_green-d">id firstname surname  ...                      name city country</span>
<span class="pd_green-d">916  943    Kip S.  Thorne  ...  LIGO/VIRGO Collaboration  NaN     NaN</span></code></pre>
<p class="noindent">Bingo. Now we can look for the year as with Feynman’s entry:</p>
<p class="codelink"><a href="ch11_images.xhtml#f366-02" id="f366-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"firstname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Kip S."</span>]<span class="pd_gray">.</span>year
<span class="pd_green-d">2017</span></code></pre>
<p class="indent">But what if we didn’t happen to know Kip’s middle initial (and didn’t think to check Wikipedia for it)? It would be nice to be able to search all the first names for the string <span class="pd_green-d"><code><strong>"Kip"</strong></code></span>. We can do this using <span class="pd_green-d"><code><strong>Series.str</strong></code></span>, which allows us to use string functions on a Series, together with <span class="pd_green-d"><code><strong>contains()</strong></code></span> to search for a substring (<a href="ch11.xhtml#ch11list14">Listing 11.14</a>).</p>
<span epub:type="pagebreak" id="page_367"></span>
<p class="listing" id="ch11list14"><strong>Listing 11.14:</strong> Finding a record by substring.</p>
<p class="codelink"><a href="ch11_images.xhtml#f367-01" id="f367-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"firstname"</span>]<span class="pd_gray">.</span>str<span class="pd_gray">.</span>contains(<span class="pd_red">"Kip"</span>)]
      <span class="pd_green-d">id firstname surname  ...                      name city country
916  943    Kip S.  Thorne  ...  LIGO/VIRGO Collaboration  NaN     NaN</span></code></pre>
</div>
<p class="noindent">Perhaps unsurprisingly, since it’s a fairly uncommon name, there’s only one “Kip” in the dataset. What about any other Feynmans? We can try again with <span class="pd_green-d"><code><strong>"surname"</strong></code></span> in place of <span class="pd_green-d"><code><strong>"firstname"</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f367-02" id="f367-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"surname"</span>]<span class="pd_gray">.</span>str<span class="pd_gray">.</span>contains(<span class="pd_red">"Feynman"</span>)]
<span class="pd_green-d">ValueError: Cannot mask with non-boolean array containing NA / NaN values</span></code></pre>
<p class="noindent">Oops, we got an error. This is because of a large number of NaNs from organizations that have won the Nobel Prize for Peace:</p>
<p class="codelink"><a href="ch11_images.xhtml#f367-03" id="f367-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"surname"</span>]<span class="pd_gray">.</span>isnull()]
      <span class="pd_green-d">id                                          firstname  ... city country
465  467                     Institute of International Law  ...  NaN     NaN
474  477               Permanent International Peace Bureau  ...  NaN     NaN
479  482           International Committee of the Red Cross  ...  NaN     NaN
480  482           International Committee of the Red Cross  ...  NaN     NaN
.
.
.</span></code></pre>
<p class="indent">We can filter out NaNs and other Not Available values by passing the option <span class="pd_green-d"><code><strong>na=False</strong></code></span> to <span class="pd_green-d"><code><strong>contains()</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f367-04" id="f367-04a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"surname"</span>]<span class="pd_gray">.</span>str<span class="pd_gray">.</span>contains(<span class="pd_red">"Feynman"</span>, na<span class="pd_gray">=</span><span class="pd_green"><strong>False</strong></span>)]
    <span class="pd_green-d">id   firstname  ...         city country
86  86  Richard P.  ...  Pasadena CA     USA</span></code></pre>
<p class="noindent">It looks like there’s only one result, which we can confirm with <span class="pd_green-d"><code><strong>len()</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f367-05" id="f367-05a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green">len</span>(nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"surname"</span>]<span class="pd_gray">.</span>str<span class="pd_gray">.</span>contains(<span class="pd_red">"Feynman"</span>, na<span class="pd_gray">=</span><span class="pd_green"><strong>False</strong></span>)])
<span class="pd_green-d">1</span></code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_368"></span>Although there’s only one Nobel laureate named “Feynman”, there are famously several named “Curie”, as seen in <a href="ch11.xhtml#ch11list15">Listing 11.15</a>.</p>
<p class="listing" id="ch11list15"><strong>Listing 11.15:</strong> Finding Curies in the <span class="pd_green-d"><code><strong>laureates.csv</strong></code></span> dataset.</p>
<p class="codelink"><a href="ch11_images.xhtml#f368-01" id="f368-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> curies <span class="pd_gray">=</span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"surname"</span>]<span class="pd_gray">.</span>str<span class="pd_gray">.</span>contains(<span class="pd_red">"Curie"</span>, na<span class="pd_gray">=</span><span class="pd_green"><strong>False</strong></span>)]
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> curies
      <span class="pd_green-d">id firstname  ...   city country
4      5    Pierre  ...  Paris  France
5      6     Marie  ...    NaN     NaN
6      6     Marie  ...  Paris  France
191  194     Irène  ...  Paris  France</span></code></pre>
</div>
<p class="noindent">Here we’ve assigned the result to the variable <span class="pd_green-d"><code><strong>curies</strong></code></span> for convenience. For example, we can get the first name and surname for each Curie laureate as follows:</p>
<p class="codelink"><a href="ch11_images.xhtml#f368-02" id="f368-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> curies[[<span class="pd_red">"firstname"</span>, <span class="pd_red">"surname"</span>]]
<span class="pd_green-d">4      Pierre         Curie
5       Marie         Curie
6       Marie         Curie
191     Irène  Joliot-Curie</span></code></pre>
<p class="indent">We see that Marie Curie (also known as Marie Skłodowska-Curie)<sup><a id="fn11_22a" href="ch11.xhtml#fn11_22">22</a></sup> won two Nobel Prizes (<a href="ch11.xhtml#ch11fig25">Figure 11.25</a><sup><a id="fn11_23a" href="ch11.xhtml#fn11_23">23</a></sup>). The other Curie laureates are Pierre Curie, Marie’s husband, and Irène Joliot-Curie, one of their daughters. (In fact, there’s even one more Nobel laureate in the absurdly accomplished Curie family; see <a href="ch11.xhtml#sec11_5_1">Section 11.5.1</a> for more.)</p>
<p class="footnote"><a id="fn11_22" href="ch11.xhtml#fn11_22a">22.</a> Although typically known only as “Marie Curie” in English-language sources, Marie herself preferred to use the Polish part of her name as well, and many European sources (including Polish ones) follow this convention.</p>
<figure class="image-c" id="ch11fig25">
<img src="graphics/11fig25.jpg" alt="images" width="519" height="457"/>
<figcaption>
<p class="title-f"><strong>Figure 11.25:</strong> Marie Skłodowska-Curie with her husband and co-laureate Pierre Curie.</p>
</figcaption>
</figure>
<p class="footnote"><a id="fn11_23" href="ch11.xhtml#fn11_23a">23.</a> Image courtesy of Morphart Creation/Shutterstock.</p>
<p class="indent">Marie Skłodowska-Curie is the only person to win a Nobel Prize for two different sciences. Let’s use pandas to see if there are any other multiple Nobel prizewinners. One way to investigate this question is to use <span class="pd_green-d"><code><strong>groupby()</strong></code></span> to group the winners by name and then use the <span class="pd_green-d"><code><strong>size()</strong></code></span> method to see how many there are:</p>
<span epub:type="pagebreak" id="page_369"></span>
<p class="codelink"><a href="ch11_images.xhtml#f369-01" id="f369-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>groupby([<span class="pd_red">"firstname"</span>, <span class="pd_red">"surname"</span>])<span class="pd_gray">.</span>size()
<span class="pd_green-d">firstname   surname</span>
<span class="pd_green-d">A. Michael  Spence         1</span>
<span class="pd_green-d">Aage N.     Bohr           1</span>
<span class="pd_green-d">Aaron       Ciechanover    1</span>
            <span class="pd_green-d">Klug           1</span>
<span class="pd_green-d">Abdulrazak  Gurnah         1</span>
                          <span class="pd_green-d">..</span>
<span class="pd_green-d">Youyou      Tu             1</span>
<span class="pd_green-d">Yuan T.     Lee            1</span>
<span class="pd_green-d">Yves        Chauvin        1</span>
<span class="pd_green-d">Zhores      Alferov        1</span>
<span class="pd_green-d">Élie        Ducommun       1</span></code></pre>
<p class="noindent">All the displayed values here are <span class="pd_green-d"><code><strong>1</strong></code></span>, but we can sort them using <span class="pd_green-d"><code><strong>sort_values()</strong></code></span> to find any multiple laureates:</p>
<span epub:type="pagebreak" id="page_370"></span>
<p class="codelink"><a href="ch11_images.xhtml#f370-01" id="f370-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>groupby([<span class="pd_red">"firstname"</span>, <span class="pd_red">"surname"</span>])<span class="pd_gray">.</span>size()<span class="pd_gray">.</span>sort_values()
<span class="pd_green-d">firstname     surname
A. Michael    Spence      1
Nicolay G.    Basov       1
Niels         Bohr        1
Niels K.      Jerne       1
Niels Ryberg  Finsen      1
                         ..
Élie          Ducommun    1
Linus         Pauling     2
John          Bardeen     2
Frederick     Sanger      2
Marie         Curie       2</span></code></pre>
<p class="noindent">This yields four multiple winners.</p>
<p class="indent">Although the <span class="pd_green-d"><code><strong>sort_values()</strong></code></span> trick is nice, it would have failed if there had been too many multiple laureates. A more general way to select winners of more than one Prize is to use a boolean criterion directly. We can do this with the same grouping by size combined with the criterion <span class="pd_green-d"><code><strong>size &gt; 1</strong></code></span> (<a href="ch11.xhtml#ch11list16">Listing 11.16</a>). Note that we’ve added <span class="pd_green-d"><code><strong>"id"</strong></code></span> to <span class="pd_green-d"><code><strong>groupby()</strong></code></span> to take into account the (unlikely but possible) case of different people with the same name both winning Nobel Prizes.</p>
<p class="listing" id="ch11list16"><strong>Listing 11.16:</strong> Finding winners of multiple Nobel Prizes.</p>
<p class="codelink"><a href="ch11_images.xhtml#f370-02" id="f370-02a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> laureates <span class="pd_gray">=</span> nobel<span class="pd_gray">.</span>groupby([<span class="pd_red">"id"</span>, <span class="pd_red">"firstname"</span>, <span class="pd_red">"surname"</span>])
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> sizes <span class="pd_gray">=</span> laureates<span class="pd_gray">.</span>size()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> sizes[sizes <span class="pd_gray">&gt; 1</span>]
<span class="pd_green-d">id   firstname  surname
6    Marie      Curie      2
66   John       Bardeen    2
217  Linus      Pauling    2
222  Frederick  Sanger     2</span></code></pre>
</div>
<p class="noindent">We see from <a href="ch11.xhtml#ch11list16">Listing 11.16</a> that, at the time this dataset was assembled, only four people had ever won more than one Nobel Prize: Frederick Sanger (Chemistry), John Bardeen (Physics), Linus Pauling (Chemistry and Peace), and of course Marie Curie (Physics and Chemistry). (2022 saw the emergence of a fifth multiple-laureate when K. Barry Sharpless won his second Nobel Prize for Chemistry.)</p>
<span epub:type="pagebreak" id="page_371"></span>
</section>
<section>
<h5 class="h5" id="sec11_1_1_4">Selecting Dates</h5>
<p class="noindent">One of pandas’ greatest strengths is its ability to deal with times and <em>time series</em>, so let’s start by taking a look at selecting dates. One way we can do this is by searching for laureates by exact birthday as a string:</p>
<p class="codelink"><a href="ch11_images.xhtml#f371-01" id="f371-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"born"</span>] <span class="pd_gray">==</span> <span class="pd_red">"1879-03-14"</span>]
    <span class="pd_green-d">id firstname  ...    city  country
25  26    Albert  ...  Berlin  Germany</span></code></pre>
<p class="noindent">You might suspect that a Nobel Prize–winning “Albert” born in 1879 might be Albert Einstein, and you’d be right, as we can see by checking the <span class="pd_green-d"><code><strong>"surname"</strong></code></span> field:<sup><a id="fn11_24a" href="ch11.xhtml#fn11_24">24</a></sup></p>
<p class="footnote"><a id="fn11_24" href="ch11.xhtml#fn11_24a">24.</a> If you’re using Jupyter, you can probably just read off the full name from the result of evaluating the cell.</p>
<p class="codelink"><a href="ch11_images.xhtml#f371-02" id="f371-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"born"</span>] <span class="pd_gray">==</span> <span class="pd_red">"1879-03-14"</span>][<span class="pd_red">"surname"</span>]
<span class="pd_green-d">Einstein</span></code></pre>
<p class="indent">Looking closely, we see that Einstein was born on March 14, which is sometimes known as Pi Day because of the resemblance between 03-14 (or 3/14 in the American calendar system) and the first three digits of <em>π</em> ≈ 3<em>.</em>14. Fans of Pi Day are quick to point out how great this is.</p>
<p class="indent">As the founder of Tau Day (<a href="https://tauday.com/">https://tauday.com/</a>), I was naturally interested in finding some great Nobel laureate who was born on 06-28 (6/28) to match the first three digits of <em>τ</em> ≈ 6<em>.</em>28. We’ve seemingly already solved this problem of searching by substring (as in, e.g., <a href="ch11.xhtml#ch11list14">Listing 11.14</a>), so let’s try it out with the <span class="pd_green-d"><code><strong>"born"</strong></code></span> field:</p>
<p class="codelink"><a href="ch11_images.xhtml#f371-03" id="f371-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"born"</span>]<span class="pd_gray">.</span>str<span class="pd_gray">.</span>contains(<span class="pd_red">"06-08"</span>, na<span class="pd_gray">=</span><span class="pd_green"><strong>False</strong></span>)]
      <span class="pd_green-d">id    firstname  ...          city  country
79    79        Maria  ...  San Diego CA      USA
125  126        Klaus  ...     Stuttgart  Germany
281  283  F. Sherwood  ...     Irvine CA      USA
304  306       Alexis  ...   New York NY      USA
598  607        Luigi  ...           NaN      NaN
790  809     Muhammad  ...           NaN      NaN
889  916   William C.  ...    Madison NJ      USA</span>

<span class="pd_green-d">[7 rows x 20 columns]</span></code></pre>
<p class="indent">That’s 7 rows. Let’s narrow it down by restricting the results to Nobel laureates in Physics using the <span class="pd_green-d"><code><strong>&amp;</strong></code></span> operator to perform a logical <em>and</em> (note that this syntax differs from Python itself (<a href="ch02.xhtml#sec2_4_1">Section 2.4.1</a>)):</p>
<span epub:type="pagebreak" id="page_372"></span>
<p class="codelink"><a href="ch11_images.xhtml#f372-01" id="f372-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[(nobel[<span class="pd_red">"born"</span>]<span class="pd_gray">.</span>astype(<span class="pd_red">'string'</span>)<span class="pd_gray">.</span>str<span class="pd_gray">.</span>contains(<span class="pd_red">"06-28"</span>)) <span class="pd_gray">&amp;</span>
<span class="pd_blue"><strong>...</strong></span>           (nobel[<span class="pd_red">"category"</span>] <span class="pd_gray">==</span> <span class="pd_red">"physics"</span>)]
      <span class="pd_green-d">id firstname  ...          city  country
79    79     Maria  ...  San Diego CA      USA
125  126     Klaus  ...     Stuttgart  Germany</span>

<span class="pd_green-d">[2 rows x 20 columns]</span></code></pre>
<p class="noindent">That’s more like it. Let’s take a look at the first record using <span class="pd_green-d"><code><strong>iloc</strong></code></span> (“index location”) to find it by its index number, which is <span class="pd_green-d"><code><strong>79</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f372-02" id="f372-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>iloc[<span class="pd_gray">79</span>]
<span class="pd_green-d">id                                                               79
firstname                                                     Maria
surname                                              Goeppert Mayer
born                                                     1906-06-28
died                                                     1972-02-20
.
.
.</span></code></pre>
<p class="noindent">That’s Maria Goeppert Mayer (<a href="ch11.xhtml#ch11fig26">Figure 11.26</a><sup><a id="fn11_25a" href="ch11.xhtml#fn11_25">25</a></sup>), who won a Nobel Prize in Physics for her contributions to the nuclear shell model, and who is the official physicist of Tau Day. (Take that, Al!)</p>
<figure class="image-c" id="ch11fig26">
<img src="graphics/11fig26.jpg" alt="images" width="300" height="424"/>
<figcaption>
<p class="title-f"><strong>Figure 11.26:</strong> Maria Goeppert Mayer, Nobel laureate and official physicist of Tau Day.</p>
</figcaption>
</figure>
<p class="footnote"><a id="fn11_25" href="ch11.xhtml#fn11_25a">25.</a> Image courtesy of Archive PL/Alamy Alamy Stock Photo.</p>
<p class="indent">Speaking of birthdates, the lifespans of Nobel laureates have been the subject of some scientific research over the years.<sup><a id="fn11_26a" href="ch11.xhtml#fn11_26">26</a></sup> Although we are not in a position to draw any conclusions about the effect (if any) of winning a Nobel Prize on longevity, we can make a histogram of the laureates’ ages to get a sense of the distribution.</p>
<p class="footnote"><a id="fn11_26" href="ch11.xhtml#fn11_26a">26.</a> See, for example, Matthew D. Rablen and Andrew J. Oswald, “Mortality and immortality: The Nobel Prize as an experiment into the effect of status upon longevity”. <em>Journal of Health Economics</em>, Volume 27, Issue 6. December 2008, pp. 1462–1471.</p>
<p class="indent">Let’s begin by finding the record for Hans Bethe (“BAY-tuh”), one of the longest-lived Nobel laureates:<sup><a id="fn11_27a" href="ch11.xhtml#fn11_27">27</a></sup></p>
<p class="footnote"><a id="fn11_27" href="ch11.xhtml#fn11_27a">27.</a> Bethe was already a famous physicist in the 1930s due to his groundbreaking series of papers on nuclear theory. He later served as the head of the theoretical division at Los Alamos during the making of the atomic bomb. And yet he lived so long that I had a chance to meet him in the early 2000s when he came to give an astrophysics talk at Caltech.</p>
<span epub:type="pagebreak" id="page_373"></span>
<p class="codelink"><a href="ch11_images.xhtml#f373-01" id="f373-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> bethe <span class="pd_gray">=</span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"surname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Bethe"</span>]
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> bethe[<span class="pd_red">"born"</span>]
<span class="pd_green-d">88    1906-07-02</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> bethe[<span class="pd_red">"died"</span>]
<span class="pd_green-d">88    2005-03-06</span></code></pre>
<p class="noindent">By subtracting in our heads, we can see that Bethe lived to be 98, but doing this one by one for all the laureates would be most impractical.</p>
<p class="indent">Let’s see if we can calculate Bethe’s age by pure subtraction:</p>
<p class="codelink"><a href="ch11_images.xhtml#f373-02" id="f373-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> bethe[<span class="pd_red">"died"</span>] <span class="pd_gray">-</span> bethe[<span class="pd_red">"born"</span>]
<span class="pd_green-d">TypeError: unsupported operand type(s) for -: 'str' and 'str'</span></code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_374"></span>OK, the dates are being stored as strings, so it’s not surprising that simple subtraction didn’t work. Let’s try converting to <span class="pd_green-d"><code><strong>datetime</strong></code></span> objects:</p>
<p class="codelink"><a href="ch11_images.xhtml#f374-01" id="f374-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> diff <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>to_datetime(bethe[<span class="pd_red">"died"</span>]) <span class="pd_gray">-</span> pd<span class="pd_gray">.</span>to_datetime(bethe[<span class="pd_red">"born"</span>])
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> diff
<span class="pd_green-d">88   36042 days
dtype: timedelta64[ns]</span></code></pre>
<p class="noindent">That’s much more promising, but it’s a Series of <span class="pd_green-d"><code><strong>timedelta64</strong></code></span> objects, though, rather than floats. We can fix this by using <span class="pd_green-d"><code><strong>dt</strong></code></span> to gain access to the datetime directly and <span class="pd_green-d"><code><strong>days</strong></code></span> to find the number of days:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> diff<span class="pd_gray">.</span>dt<span class="pd_gray">.</span>days
<span class="pd_green-d">88    36042
dtype: int64</span></code></pre>
<p class="indent">At this point, we could then divide by 365 (or 365.25) to get the approximate number of years, which is probably good enough for a histogram, but it isn’t quite right because of leap years, the number of which will vary based on the exact date range. Luckily, NumPy comes with a method called <span class="pd_green-d"><code><strong>timedelta64</strong></code></span> that handles this automatically:</p>
<p class="codelink"><a href="ch11_images.xhtml#f374-02" id="f374-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> diff<span class="pd_gray">/</span>np<span class="pd_gray">.</span>timedelta64(<span class="pd_gray">1</span>, <span class="pd_red">"Y"</span>)
<span class="pd_green-d">88    98.679644
dtype: float64</span></code></pre>
<p class="noindent">Here <span class="pd_green-d"><code><strong>1, "Y"</strong></code></span> refers to a time delta (change) of “1 Year”.</p>
<p class="indent">Now let’s apply the same idea to the full list of Nobel laureates:</p>
<p class="codelink"><a href="ch11_images.xhtml#f374-03" id="f374-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel[<span class="pd_red">"born"</span>] <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>to_datetime(nobel[<span class="pd_red">"born"</span>])
<span class="pd_green-d">dateutil.parser._parser.ParserError: month must be in 1..12: 1873-00-00</span></code></pre>
<p class="noindent">Here there’s an error because at least one of the <span class="pd_green-d"><code><strong>"born"</strong></code></span> dates has <span class="pd_green-d"><code><strong>00-00</strong></code></span> for the month and year. Why?</p>
<span epub:type="pagebreak" id="page_375"></span>
<p class="codelink"><a href="ch11_images.xhtml#f375-01" id="f375-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"born"</span>] <span class="pd_gray">==</span> <span class="pd_red">"1873-00-00"</span>]
      <span class="pd_green-d">id                       firstname surname  ... name city country
465  467  Institute of International Law     NaN  ...  NaN  NaN     NaN</span>

<span class="pd_green-d">[1 rows x 20 columns]</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>iloc[<span class="pd_gray">465</span>]<span class="pd_gray">.</span>born
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>iloc[<span class="pd_gray">465</span>]<span class="pd_gray">.</span>category
<span class="pd_green-d">465    peace
Name: category, dtype: object</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>iloc[<span class="pd_gray">465</span>]<span class="pd_gray">.</span>year
<span class="pd_green-d">465    1904
Name: year, dtype: int64</span></code></pre>
<p class="noindent">Ah, so an organization called the Institute of International Law won the Nobel Prize for Peace in 1904. As you might guess from the <span class="pd_green-d"><code><strong>"born"</strong></code></span> date, it was founded in 1873, but because it’s not a person the Nobel data declines to specify an exact “birth” date.</p>
<p class="indent">This complicates matters somewhat because we can’t just drop Not Available values like <span class="pd_green-d"><code><strong>NaN</strong></code></span> and <span class="pd_green-d"><code><strong>NaT</strong></code></span>. Luckily, pandas has an option to force, or <span class="pd_green-d"><code><strong>coerce</strong></code></span>, such values when making the conversion. We can convert in place (thereby overwriting the old data) like this:</p>
<p class="codelink"><a href="ch11_images.xhtml#f375-02" id="f375-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel[<span class="pd_red">"born"</span>] <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>to_datetime(nobel[<span class="pd_red">"born"</span>], errors<span class="pd_gray">=</span><span class="pd_red">"coerce"</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel[<span class="pd_red">"died"</span>] <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>to_datetime(nobel[<span class="pd_red">"died"</span>], errors<span class="pd_gray">=</span><span class="pd_red">"coerce"</span>)</code></pre>
<p class="noindent">Now we can double-check the value for the Institute of International Law:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>iloc[<span class="pd_gray">465</span>]<span class="pd_gray">.</span>born
<span class="pd_green-d">NaT</span></code></pre>
<p class="noindent">So the coercion converted the invalid date to Not a Time, which is perfect for our purposes because such values are ignored automatically when plotting histograms.</p>
<p class="indent">At this point, we’re ready to calculate the laureates’ lifespans by subtracting datetimes and dividing by NumPy’s magic time delta:</p>
<p class="codelink"><a href="ch11_images.xhtml#f375-03" id="f375-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel[<span class="pd_red">"lifespan"</span>] <span class="pd_gray">=</span> (nobel[<span class="pd_red">"died"</span>] <span class="pd_gray">-</span> nobel[<span class="pd_red">"born"</span>])<span class="pd_gray">/</span>np<span class="pd_gray">.</span>timedelta64(<span class="pd_gray">1</span>, <span class="pd_red">"Y"</span>)</code></pre>
<p class="noindent">Note that this dynamically creates a new <span class="pd_green-d"><code><strong>"lifespan"</strong></code></span> column in our <span class="pd_green-d"><code><strong>nobel</strong></code></span> DataFrame. We can do a reality check by making sure we’ve replicated the calculation we did for Bethe:</p>
<span epub:type="pagebreak" id="page_376"></span>
<p class="codelink"><a href="ch11_images.xhtml#f376-01" id="f376-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> bethe <span class="pd_gray">=</span> nobel<span class="pd_gray">.</span>loc[nobel[<span class="pd_red">"surname"</span>] <span class="pd_gray">==</span> <span class="pd_red">"Bethe"</span>]
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> bethe[<span class="pd_red">"lifespan"</span>]
<span class="pd_green-d">88    98.679644</span></code></pre>
<p class="noindent">So Hans Bethe’s lifespan checks out from our previous calculation.</p>
<p class="indent">We’re now finally ready to make the histogram. With all the work we’ve done, it’s as simple as calling <span class="pd_green-d"><code><strong>hist()</strong></code></span> with the <span class="pd_green-d"><code><strong>"lifespan"</strong></code></span> column (<a href="ch11.xhtml#ch11list17">Listing 11.17</a>).</p>
<p class="listing" id="ch11list17"><strong>Listing 11.17:</strong> The code to make a lifespan histogram.</p>
<p class="codelink"><a href="ch11_images.xhtml#f376-02" id="f376-02a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> nobel<span class="pd_gray">.</span>hist(column<span class="pd_gray">=</span><span class="pd_red">"lifespan"</span>)
<span class="pd_green-d">array([[&lt;AxesSubplot:title={'center':'lifespan'}&gt;]], dtype=object)</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>show()</code></pre>
</div>
<p class="noindent">The result appears in <a href="ch11.xhtml#ch11fig27">Figure 11.27</a>. As expected from the research on the subject, the lifespans of the Nobel laureates are skewed toward the upper end of the usual range.</p>
<figure class="image-c" id="ch11fig27">
<img src="graphics/11fig27.jpg" alt="images" width="601" height="465"/>
<figcaption>
<p class="title-f"><strong>Figure 11.27:</strong> A histogram of Nobel laureates’ lifespans.</p>
</figcaption>
</figure>
<span epub:type="pagebreak" id="page_377"></span>
</section>
</section>
<section>
<h4 class="h4" id="sec11_5_1">11.5.1 Exercises</h4>
<ol class="number">
<li><p class="number">Confirm that Frédéric Joliot-Curie, who shared the 1935 Nobel Prize for Chemistry with his wife Irène, appears in the <span class="pd_green-d"><code><strong>laureates.csv</strong></code></span> dataset. Why did we miss him when we searched for Curies in <a href="ch11.xhtml#ch11list15">Listing 11.15</a>? <em>Hint</em>: Search for an entry with <span class="pd_green-d"><code><strong>"firstname"</strong></code></span> equal to <span class="pd_green-d"><code><strong>"Frédéric"</strong></code></span> (making sure to include the proper accents).</p></li>
<li><p class="number">Verify that the Nobel Prize categories cited after <a href="ch11.xhtml#ch11list16">Listing 11.16</a> are correct (e.g., that Frederick Sanger’s Nobel Prizes really were for Chemistry, etc.).</p></li>
<li><p class="number">In <a href="ch11.xhtml#ch11list17">Listing 11.17</a>, what happens if you just call <span class="pd_green-d"><code><strong>nobel.hist()</strong></code></span>, with no column specified?</p></li>
</ol>
</section>
</section>
<section>
<h3 class="h3" id="sec11_6">11.6 pandas Example: <em>Titanic</em></h3>
<p class="noindent">Our second major pandas example uses survival data from the tragic sinking of RMS <em>Titanic</em> in 1912 (<a href="ch11.xhtml#ch11fig28">Figure 11.28</a><sup><a id="fn11_28a" href="ch11.xhtml#fn11_28">28</a></sup>). This is a standard dataset used by the pandas documentation itself,<sup><a id="fn11_29a" href="ch11.xhtml#fn11_29">29</a></sup> and as such has been extensively analyzed, making the “Google for it” algorithm unusually effective.</p>
<figure class="image-c" id="ch11fig28">
<img src="graphics/11fig28.jpg" alt="images" width="725" height="517"/>
<figcaption>
<p class="title-f"><strong>Figure 11.28:</strong> The ill-fated RMS <em>Titanic.</em></p>
</figcaption>
</figure>
<p class="footnote"><a id="fn11_28" href="ch11.xhtml#fn11_28a">28.</a> Image courtesy of Shawshots/Alamy.</p>
<p class="footnote"><a id="fn11_29" href="ch11.xhtml#fn11_29a">29.</a> It’s available at <a href="https://github.com/pandas-dev/pandas/blob/main/doc/data/titanic.csv">https://github.com/pandas-dev/pandas/blob/main/doc/data/titanic.csv</a>, but as with the Nobel laureate data we’ll use the version at the Learn Enough CDN for maximum compatibility in case the pandas version changes or disappears.</p>
<p class="indent">As usual, our first step is to download the data, which we can do directly from the Web, as shown in <a href="ch11.xhtml#ch11list18">Listing 11.18</a>. (We saw in <a href="ch09.xhtml#sec9_2">Section 9.2</a> that <span class="pd_green-d"><code><strong>request.get()</strong></code></span> automatically follows redirects, but as far as I can tell <span class="pd_green-d"><code><strong>read_csv()</strong></code></span> does not. I have been unable to figure out how to get it to do so (if it’s even possible), so <a href="ch11.xhtml#ch11list18">Listing 11.18</a> uses the raw Amazon S3 URL instead.)</p>
<p class="indent">Let’s take a look at the <span class="pd_green-d"><code><strong>head()</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f377-01" id="f377-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic<span class="pd_gray">.</span>head()
<span class="pd_green-d">   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked
0            1         0       3  ...   7.2500   NaN         S
1            2         1       1  ...  71.2833   C85         C
2            3         1       3  ...   7.9250   NaN         S
3            4         1       1  ...  53.1000  C123         S
4            5         0       3  ...   8.0500   NaN</span></code></pre>
<span epub:type="pagebreak" id="page_378"></span>
<p class="listing" id="ch11list18"><strong>Listing 11.18:</strong> Reading data right from a (raw S3) URL.</p>
<p class="codelink"><a href="ch11_images.xhtml#f378-01" id="f378-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> URL <span class="pd_gray">=</span> <span class="pd_red">"https://learnenough.s3.amazonaws.com/titanic.csv"</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>read_csv(URL)</code></pre>
</div>
<p class="noindent">We see that the data is indexed by <span class="pd_green-d"><code><strong>PassengerId</strong></code></span>, but that’s not very meaningful, and we can give it a more personal touch by rereading the data and indexing on <span class="pd_green-d"><code><strong>Name</strong></code></span> instead. The way to do this is by specifying an index column using <span class="pd_green-d"><code><strong>index_col</strong></code></span> (<a href="ch11.xhtml#ch11list19">Listing 11.19</a>).</p>
<span epub:type="pagebreak" id="page_379"></span>
<p class="listing" id="ch11list19"><strong>Listing 11.19:</strong> Setting a custom index column.</p>
<p class="codelink"><a href="ch11_images.xhtml#f379-01" id="f379-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>read_csv(URL, index_col<span class="pd_gray">=</span><span class="pd_red">"Name"</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic<span class="pd_gray">.</span>head()
                                                    <span class="pd_green-d">PassengerId  ...  Embarked
Name                                                             ...
Braund, Mr. Owen Harris                                       1  ...         S
Cumings, Mrs. John Bradley (Florence Briggs Tha...            2  ...         C
Heikkinen, Miss. Laina                                        3  ...         S
Futrelle, Mrs. Jacques Heath (Lily May Peel)                  4  ...         S
Allen, Mr. William Henry                                      5  ...         S</span></code></pre>
</div>
<p class="indent">We can look at the value of the <span class="pd_green-d"><code><strong>"Survived"</strong></code></span> column for each passenger whether they survived or not:</p>
<p class="codelink"><a href="ch11_images.xhtml#f379-02" id="f379-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic<span class="pd_gray">.</span>iloc[<span class="pd_gray">0</span>][<span class="pd_red">"Survived"</span>]
<span class="pd_green-d">0</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic<span class="pd_gray">.</span>iloc[<span class="pd_gray">1</span>][<span class="pd_red">"Survived"</span>]
<span class="pd_green-d">1</span></code></pre>
<p class="noindent">Here <span class="pd_green-d"><code><strong>1</strong></code></span> is for “Survived” and <span class="pd_green-d"><code><strong>0</strong></code></span> is for “Didn’t Survive”, which follows the standard practice for a category where each entry takes only one of two values (variously called a “binary predictor”, an “indicator variable”, or a “dummy variable”).</p>
<p class="indent">Because of the choice of encoding, the mean value of the <span class="pd_green-d"><code><strong>"Survived"</strong></code></span> attribute is the total survival rate:</p>
<p class="equ"><img src="graphics/pg379-01.jpg" alt="images" width="661" height="68"/></p>
<p class="noindent">As a result, we can get the overall survival rate by calling <span class="pd_green-d"><code><strong>mean()</strong></code></span> on the <span class="pd_green-d"><code><strong>"Survived"</strong></code></span> column:</p>
<p class="codelink"><a href="ch11_images.xhtml#f379-03" id="f379-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic[<span class="pd_red">"Survived"</span>]<span class="pd_gray">.</span>mean()
<span class="pd_green-d">0.3838383838383838</span></code></pre>
<p class="noindent">So the <em>Titanic</em> disaster survival rate was approximately 38%.</p>
<p class="indent">Let’s take a look at how survival rate was affected by some of the variables applicable to the passengers. We’ll start by getting some <span class="pd_green-d"><code><strong>info()</strong></code></span>:</p>
<span epub:type="pagebreak" id="page_380"></span>
<p class="codelink"><a href="ch11_images.xhtml#f380-01" id="f380-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic<span class="pd_gray">.</span>info()
<span class="pd_green-d">&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 891 entries, Braund, Mr. Owen Harris to Dooley, Mr. Patrick
Data columns (total 11 columns):
 #   Column       Non-Null Count  Dtype
---  ------       --------------  -----
 0   PassengerId  891 non-null    int64
 1   Survived     891 non-null    int64
 2   Pclass       891 non-null    int64
 3   Sex          891 non-null    object
 4   Age          714 non-null    float64
 5   SibSp        891 non-null    int64
 6   Parch        891 non-null    int64
 7   Ticket       891 non-null    object
 8   Fare         891 non-null    float64
 9   Cabin        204 non-null    object
 10  Embarked     889 non-null    object
dtypes: float64(2), int64(5), object(4)</span></code></pre>
<p class="noindent">The most interesting columns from the perspective of survival rate are probably passenger class (<span class="pd_green-d"><code><strong>"Pclass"</strong></code></span>), sex (<span class="pd_green-d"><code><strong>"Sex"</strong></code></span>), and age (<span class="pd_green-d"><code><strong>"Age"</strong></code></span>).</p>
<p class="indent">We can use pandas to discover that passenger class consists of three categories:</p>
<p class="codelink"><a href="ch11_images.xhtml#f380-02" id="f380-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic[<span class="pd_red">"Pclass"</span>]<span class="pd_gray">.</span>unique()
<span class="pd_green-d">array([3, 1, 2])</span></code></pre>
<p class="noindent">These represent first-, second-, and third-class tickets, which correspond to accommodation quality from the highest to the lowest.</p>
<p class="indent">If we <span class="pd_green-d"><code><strong>groupby()</strong></code></span> class we can see how survival rates vary:</p>
<p class="codelink"><a href="ch11_images.xhtml#f380-03" id="f380-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic<span class="pd_gray">.</span>groupby(<span class="pd_red">"Pclass"</span>)[<span class="pd_red">"Survived"</span>]<span class="pd_gray">.</span>mean()
<span class="pd_green-d">Pclass
1    0.629630
2    0.472826
3    0.242363</span></code></pre>
<p class="noindent">So we see that the survival rate varies strongly by class, with first-class passengers surviving at a rate of 62.9% and third-class passengers surviving at only 24.2%.</p>
<p class="indent">We can visualize this result by plotting a bar chart of survival rate. Each pandas Series object has a <span class="pd_green-d"><code><strong>plot</strong></code></span> attribute that lets us call <span class="pd_green-d"><code><strong>bar()</strong></code></span> to make a bar chart, which includes the bar labels automatically. Heights are given by the height of each categorical variable, in this case the survival rates we just calculated:</p>
<span epub:type="pagebreak" id="page_381"></span>
<p class="codelink"><a href="ch11_images.xhtml#f381-01" id="f381-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> survival_rates <span class="pd_gray">=</span> titanic<span class="pd_gray">.</span>groupby(<span class="pd_red">"Pclass"</span>)[<span class="pd_red">"Survived"</span>]<span class="pd_gray">.</span>mean()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> survival_rates<span class="pd_gray">.</span>plot<span class="pd_gray">.</span>bar()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent">The result appears in <a href="ch11.xhtml#ch11fig29">Figure 11.29</a>.</p>
<figure class="image-c" id="ch11fig29">
<img src="graphics/11fig29.jpg" alt="images" width="601" height="461"/>
<figcaption>
<p class="title-f"><strong>Figure 11.29:</strong> <em>Titanic</em> survival rates by passenger class.</p>
</figcaption>
</figure>
<p class="indent">We can apply similar techniques to the categorical variable <span class="pd_green-d"><code><strong>"Sex"</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f381-02" id="f381-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic[<span class="pd_red">"Sex"</span>]<span class="pd_gray">.</span>unique()
<span class="pd_green-d">array(['male', 'female'], dtype=object)</span></code></pre>
<p class="noindent">The code to make the bar chart is essentially the same but with grouping by <span class="pd_green-d"><code><strong>"Sex"</strong></code></span> instead of <span class="pd_green-d"><code><strong>"Pclass"</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f381-03" id="f381-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> survival_rates <span class="pd_gray">=</span> titanic<span class="pd_gray">.</span>groupby(<span class="pd_red">"Sex"</span>)[<span class="pd_red">"Survived"</span>]<span class="pd_gray">.</span>mean()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> survival_rates<span class="pd_gray">.</span>plot<span class="pd_gray">.</span>bar()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>subplots_adjust(bottom<span class="pd_gray">=0.20</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_382"></span>The <span class="pd_green-d"><code><strong>subplots_adjust()</strong></code></span> line here may be necessary to create enough room for the labels on the <em>x</em>-axis to display properly on some systems (it was on mine). The result should appear as in <a href="ch11.xhtml#ch11fig30">Figure 11.30</a>. We see that the survival rate for female passengers was significantly higher than that for male passengers.</p>
<figure class="image-c" id="ch11fig30">
<img src="graphics/11fig30.jpg" alt="images" width="601" height="457"/>
<figcaption>
<p class="title-f"><strong>Figure 11.30:</strong> <em>Titanic</em> survival rates by sex.</p>
</figcaption>
</figure>
<p class="indent">We come now to the third major variable of likely interest, age. The class and sex variables are categorical, which made creating a bar chart easy, but the <span class="pd_green-d"><code><strong>"Age"</strong></code></span> variable is numeric, so we have to bin the data, similar to making a histogram (<a href="ch11.xhtml#sec11_3_3">Section 11.3.3</a>).</p>
<p class="indent">The ages of <em>Titanic</em> passengers ranged from infants to 80:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic[<span class="pd_red">"Age"</span>]<span class="pd_gray">.</span>min()
<span class="pd_green-d">0.42</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic[<span class="pd_red">"Age"</span>]<span class="pd_gray">.</span>max()
<span class="pd_green-d">80.0</span></code></pre>
<p class="noindent">At this point, we have to decide how many bins to use. Using <span class="pd_green-d"><code><strong>7</strong></code></span> gives an age of approximately 11 for the top of the first bin:</p>
<span epub:type="pagebreak" id="page_383"></span>
<p class="codelink"><a href="ch11_images.xhtml#f383-01" id="f383-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> (titanic[<span class="pd_red">"Age"</span>]<span class="pd_gray">.</span>max() <span class="pd_gray">-</span> titanic[<span class="pd_red">"Age"</span>]<span class="pd_gray">.</span>min())<span class="pd_gray">/7</span>
<span class="pd_green-d">11.368571428571428</span></code></pre>
<p class="noindent">This is a reasonable cutoff for a “child”.</p>
<p class="indent">The next step is to bin the data, which we can do with a pandas method called <span class="pd_green-d"><code><strong>cut()</strong></code></span>. First, we need to select only passengers with valid ages, which we can accomplish with the <span class="pd_green-d"><code><strong>notna()</strong></code></span> method to ensure that age is <em>not</em> Not Available (<a href="ch11.xhtml#ch11list20">Listing 11.20</a>).</p>
<p class="listing" id="ch11list20"><strong>Listing 11.20:</strong> Selecting only values that are <em>not</em> Not Available.</p>
<p class="codelink"><a href="ch11_images.xhtml#f383-02" id="f383-02a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="b-y"><span class="pd_bluea">&gt;&gt;&gt;</span> titanic[<span class="pd_red">"Age"</span>].notna()</span>
<span class="pd_green-d">
Name
Braund, Mr. Owen Harris                               True
Cumings, Mrs. John Bradley (Florence Briggs Thayer)   True
Heikkinen, Miss. Laina                                True
Futrelle, Mrs. Jacques Heath (Lily May Peel)          True
Allen, Mr. William Henry                              True
                                                     ...
Montvila, Rev. Juozas                                 True
Graham, Miss. Margaret Edith                          True
Johnston, Miss. Catherine Helen "Carrie"             False
Behr, Mr. Karl Howell                                 True
Dooley, Mr. Patrick                                   True
Name: Age, Length: 891, dtype: bool</span>
<span class="b-y"><span class="pd_bluea">&gt;&gt;&gt;</span> valid_ages = titanic[titanic[<span class="pd_red">"Age"</span>].notna()]</span></code></pre>
</div>
<p class="noindent">The values of <span class="pd_green-d"><code><strong>titanic["Age"].notna()</strong></code></span> include booleans that are <span class="pd_green-d"><code><strong>True</strong></code></span> if the age is valid, which we can then use as an index to the <span class="pd_green-d"><code><strong>titanic</strong></code></span> object to select only the passengers with valid ages (the final line in <a href="ch11.xhtml#ch11list20">Listing 11.20</a>).</p>
<p class="indent">Next, we need to group the data by age and sort it to bring rows with similar ages next to each other before binning:</p>
<p class="codelink"><a href="ch11_images.xhtml#f383-03" id="f383-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> sorted_by_age <span class="pd_gray">=</span> valid_ages<span class="pd_gray">.</span>sort_values(by<span class="pd_gray">=</span><span class="pd_red">"Age"</span>)</code></pre>
<p class="noindent">This is necessary because otherwise we would be binning ages based on passenger name, which doesn’t make any sense since it would mix passengers of completely unrelated ages in the same bin.</p>
<p class="indent">At this point, we’re ready to use <span class="pd_green-d"><code><strong>cut()</strong></code></span> to put the data into the desired number of bins:</p>
<span epub:type="pagebreak" id="page_384"></span>
<p class="codelink"><a href="ch11_images.xhtml#f384-01" id="f384-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> sorted_by_age[<span class="pd_red">"Age range"</span>] <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>cut(sorted_by_age[<span class="pd_red">"Age"</span>], 7)</code></pre>
<p class="noindent">Finally, we calculate the survival rate per bin by grouping by bins and finding the <span class="pd_green-d"><code><strong>mean()</strong></code></span> of the <span class="pd_green-d"><code><strong>"Survived"</strong></code></span> column (remember, this works because of the 1=Survived, 0=Didn’t Survive encoding typically used for binary predictors):</p>
<p class="codelink"><a href="ch11_images.xhtml#f384-02" id="f384-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> survival_rates <span class="pd_gray">=</span> sorted_by_age<span class="pd_gray">.</span>groupby(<span class="pd_red">"Age range"</span>)[<span class="pd_red">"Survived"</span>]<span class="pd_gray">.</span>mean()</code></pre>
<p class="indent">At this point, we can use the same bar chart technique used for <span class="pd_green-d"><code><strong>"Pclass"</strong></code></span> and <span class="pd_green-d"><code><strong>"Sex"</strong></code></span> (with a bottom adjustment to get the labels to fit):</p>
<p class="codelink"><a href="ch11_images.xhtml#f384-03" id="f384-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> survival_rates<span class="pd_gray">.</span>plot<span class="pd_gray">.</span>bar()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>subplots_adjust(bottom<span class="pd_gray">=0.33</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent">The result appears as shown in <a href="ch11.xhtml#ch11fig31">Figure 11.31</a>.</p>
<figure class="image-c" id="ch11fig31">
<img src="graphics/11fig31.jpg" alt="images" width="601" height="471"/>
<figcaption>
<p class="title-f"><strong>Figure 11.31:</strong> <em>Titanic</em> survival rates by age.</p>
</figcaption>
</figure>
<p class="indent"><span epub:type="pagebreak" id="page_385"></span>We see from <a href="ch11.xhtml#ch11fig31">Figure 11.31</a> that the survival rate was highest for the youngest passengers, was approximately constant for most adults, and then fell off sharply in the highest age range. But the male passengers were also older:</p>
<p class="codelink"><a href="ch11_images.xhtml#f385-01" id="f385-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic[titanic[<span class="pd_red">"Sex"</span>] <span class="pd_gray">==</span> <span class="pd_red">"male"</span>][<span class="pd_red">"Age"</span>]<span class="pd_gray">.</span>mean()
<span class="pd_green-d">30.72664459161148</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic[titanic[<span class="pd_red">"Sex"</span>] <span class="pd_gray">==</span> <span class="pd_red">"female"</span>][<span class="pd_red">"Age"</span>]<span class="pd_gray">.</span>mean()
<span class="pd_green-d">27.915708812260537</span></code></pre>
<p class="noindent">We know from <a href="ch11.xhtml#ch11fig30">Figure 11.30</a> that male passengers also had lower survival rates, so this could account for some of the age disparity. We’ll see in <a href="ch11.xhtml#sec11_7">Section 11.7</a> one way to examine the relative contribution of each variable separately.</p>
<section>
<h4 class="h4" id="sec11_6_1">11.6.1 Exercises</h4>
<ol class="number">
<li><p class="number">Confirm using the code in <a href="ch11.xhtml#ch11list21">Listing 11.21</a> that the <em>Titanic</em> survival rate for female passengers in third class was 50%. How does this compare to the survival rate for <em>male</em> passengers in <em>first</em> class?</p></li>
<li><p class="number">Make two versions of the bar chart for <em>Titanic</em> survival rates by age shown in <a href="ch11.xhtml#ch11fig31">Figure 11.31</a>, one each for male passengers and female passengers. <em>Hint</em>: Define sex-specific variables as shown in <a href="ch11.xhtml#ch11list22">Listing 11.22</a> and redo the analysis after <a href="ch11.xhtml#ch11list20">Listing 11.20</a> separately for the <span class="pd_green-d"><code><strong>male_</strong></code></span> and <span class="pd_green-d"><code><strong>female_</strong></code></span> variables.</p></li>
<li><p class="number">Widener Library at Harvard University was built by Eleanor Elkins Widener, who survived the <em>Titanic</em> sinking, to honor her son Harry (<a href="ch11.xhtml#ch11fig32">Figure 11.32</a><sup><a id="fn11_30a" href="ch11.xhtml#fn11_30">30</a></sup>), who did not. Using a substring search similar to the one in <a href="ch11.xhtml#ch11list14">Listing 11.14</a>, show that Harry is in our <em>Titanic</em> dataset, but Eleanor is not. How old was Harry when he died? <em>Hint</em>: You can search for names containing the substring <span class="pd_green-d"><code><strong>"Widener"</strong></code></span>, but because we set <span class="pd_green-d"><code><strong>"Name"</strong></code></span> as the index column in <a href="ch11.xhtml#ch11list19">Listing 11.19</a>, you should use <span class="pd_green-d"><code><strong>titanic.index</strong></code></span> instead of <span class="pd_green-d"><code><strong>titanic["Name"]</strong></code></span> in the search.</p></li>
</ol>
<figure class="image-c" id="ch11fig32">
<img src="graphics/11fig32.jpg" alt="images" width="725" height="543"/>
<figcaption>
<p class="title-f"><strong>Figure 11.32:</strong> A portrait of Harry Elkins Widener inside Widener Library at Harvard University.</p>
</figcaption>
</figure>
<p class="footnote"><a id="fn11_30" href="ch11.xhtml#fn11_30a">30.</a> Image copyright © 2022 Michael Hartl.</p>
<p class="listing" id="ch11list21"><strong>Listing 11.21:</strong> Finding a survival rate using multiple boolean criteria.</p>
<p class="codelink"><a href="ch11_images.xhtml#f385-02" id="f385-02a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code>titanic[(titanic[<span class="pd_red">"Sex"</span>] <span class="pd_gray">==</span> <span class="pd_red">"female"</span>) <span class="pd_gray">&amp;</span>
        (titanic[<span class="pd_red">"Pclass"</span>] <span class="pd_gray">== 3</span>)][<span class="pd_red">"Survived"</span>]<span class="pd_gray">.</span>mean()</code></pre>
</div>
<span epub:type="pagebreak" id="page_386"></span>
<p class="listing" id="ch11list22"><strong>Listing 11.22:</strong> Preparing to visualize <em>Titanic</em> survival rates by age separately by sex.</p>
<p class="codelink"><a href="ch11_images.xhtml#f386-01" id="f386-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code>male_passengers <span class="pd_gray">=</span> titanic[titanic[<span class="pd_red">"Sex"</span>] <span class="pd_gray">==</span> <span class="pd_red">"male"</span>]
female_passengers <span class="pd_gray">=</span> titanic[titanic[<span class="pd_red">"Sex"</span>] <span class="pd_gray">==</span> <span class="pd_red">"female"</span>]
valid_male_ages <span class="pd_gray">=</span> male_passengers[titanic[<span class="pd_red">"Age"</span>]<span class="pd_gray">.</span>notna()]
valid_female_ages <span class="pd_gray">=</span> female_passengers[titanic[<span class="pd_red">"Age"</span>]<span class="pd_gray">.</span>notna()]</code></pre>
</div>
</section>
</section>
<section>
<h3 class="h3" id="sec11_7">11.7 Machine Learning with scikit-learn</h3>
<p class="noindent">This section contains a brief introduction to <em>machine learning</em>, which is a field of computing involving programs that “learn” in response to data inputs. Although opinions differ over whether machine learning is part of “data science” per se, at the very least <span epub:type="pagebreak" id="page_387"></span>it’s a closely related field, and is therefore suitable for inclusion in an introduction such as this one.</p>
<p class="indent">Machine learning is a giant subject, and in this section we can only scratch the surface. As with the other sections in this chapter, the main value is in developing a basic familiarity with a relevant Python package, which in this case is known as scikit-learn.</p>
<p class="indent">Building on the <em>Titanic</em> analysis in <a href="ch11.xhtml#sec11_6">Section 11.6</a>, we’ll first look at an example of <em>linear regression</em> (<a href="ch11.xhtml#sec11_7_1">Section 11.7.1</a>), and we’ll then consider more sophisticated machine-learning models (<a href="ch11.xhtml#sec11_7_2">Section 11.7.2</a>). We’ll end with an example of a <em>clustering algorithm</em> as just one example of the many additional subjects at which scikit-learn excels.</p>
<section>
<h4 class="h4" id="sec11_7_1">11.7.1 Linear Regression</h4>
<p class="noindent">In this section, we’ll use scikit-learn to perform a <em>linear regression</em>, which finds the best fit to a set of data (for a suitable definition of “best”).<sup><a id="fn11_31a" href="ch11.xhtml#fn11_31">31</a></sup> Referring to linear regression as “machine learning” is sometimes considered a sort of inside joke because the technique is relatively simple and has been in use for many years. Nevertheless, it’s a great place to start.</p>
<p class="footnote"><a id="fn11_31" href="ch11.xhtml#fn11_31a">31.</a> This section was inspired in part by the article “Linear Regression in Python” by Mirko Stojiljković.</p>
<p class="indent">As with <a href="ch11.xhtml#sec11_6">Section 11.6</a>, we’ll use the survival data from <em>Titanic</em>. We’ll start by importing the necessary libraries and creating a <span class="pd_green-d"><code><strong>titanic</strong></code></span> DataFrame:</p>
<p class="codelink"><a href="ch11_images.xhtml#f387-01" id="f387-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>import</strong></span> <span class="pd_nila"><strong>numpy</strong></span> <span class="pd_green"><strong>as</strong></span> <span class="pd_nila"><strong>np</strong></span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>import</strong></span> <span class="pd_nila"><strong>pandas</strong></span> <span class="pd_green"><strong>as</strong></span> <span class="pd_nila"><strong>pd</strong></span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>import</strong></span> <span class="pd_nila"><strong>matplotlib.pyplot</strong></span> <span class="pd_green"><strong>as</strong></span> <span class="pd_nila"><strong>plt</strong></span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> URL <span class="pd_gray">=</span> <span class="pd_red">"https://learnenough.s3.amazonaws.com/titanic.csv"</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> titanic <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>read_csv(URL)</code></pre>
<p class="indent">Our goal is to consider the effect of age on survival rate. We’ll start by making a scatter plot (<a href="ch11.xhtml#sec11_3_2">Section 11.3.2</a>) of survival rate vs. age and then find the best linear fit to the data using scikit-learn.</p>
<p class="indent">We’ll first select just the <span class="pd_green-d"><code><strong>"Age"</strong></code></span> and <span class="pd_green-d"><code><strong>"Survived"</strong></code></span> columns, since those are the columns of interest. Then, as a basic matter of data cleaning, we’ll consider only passengers with known age, so we’ll use <span class="pd_green-d"><code><strong>dropna()</strong></code></span> (<a href="ch11.xhtml#sec11_4_1">Section 11.4.1</a>) to drop the NaN values:</p>
<span epub:type="pagebreak" id="page_388"></span>
<p class="codelink"><a href="ch11_images.xhtml#f388-01" id="f388-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> passenger_age <span class="pd_gray">=</span> titanic[[<span class="pd_red">"Age"</span> , <span class="pd_red">"Survived"</span>]]<span class="pd_gray">.</span>dropna()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> passenger_age<span class="pd_gray">.</span>head()
    <span class="pd_green-d">Age  Survived
0  22.0         0
1  38.0         1
2  26.0         1
3  35.0         1
4  35.0         0</span></code></pre>
<p class="indent">For the <em>x</em>-axis of our plot, we’ll use the ages of the survivors, which we can obtain by calculating the unique values of <span class="pd_green-d"><code><strong>passenger_age["Age"]</strong></code></span> and then sorting them to put them in ascending order:</p>
<p class="codelink"><a href="ch11_images.xhtml#f388-02" id="f388-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> passenger_ages <span class="pd_gray">=</span> passenger_age[<span class="pd_red">"Age"</span>]<span class="pd_gray">.</span>unique()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> passenger_ages<span class="pd_gray">.</span>sort()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> passenger_ages
<span class="pd_green-d">array([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,
        5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  , 13.  ,
       14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 20.5 ,
       21.  , 22.  , 23.  , 23.5 , 24.  , 24.5 , 25.  , 26.  , 27.  ,
       28.  , 28.5 , 29.  , 30.  , 30.5 , 31.  , 32.  , 32.5 , 33.  ,
       34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,
       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  ,
       48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  , 55.5 ,
       56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 63.  , 64.  ,
       65.  , 66.  , 70.  , 70.5 , 71.  , 74.  , 80.  ])</span></code></pre>
<p class="indent">At this point, we’re ready to calculate the survival rate for each age:</p>
<p class="codelink"><a href="ch11_images.xhtml#f388-03" id="f388-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> survival_rate <span class="pd_gray">=</span> passenger_age<span class="pd_gray">.</span>groupby(<span class="pd_red">"Age"</span>)[<span class="pd_red">"Survived"</span>]<span class="pd_gray">.</span>mean()</code></pre>
<p class="noindent">Let’s look at a slice from the middle as a reality check:</p>
<p class="codelink"><a href="ch11_images.xhtml#f388-04" id="f388-04a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> survival_rate<span class="pd_gray">.</span>loc[<span class="pd_gray">30</span>:<span class="pd_gray">40</span>]
<span class="pd_green-d">Age
30.0    0.400000
30.5    0.000000
31.0    0.470588
32.0    0.500000
32.5    0.500000
33.0    0.400000
34.0    0.400000
34.5    0.000000
35.0    0.611111
<span epub:type="pagebreak" id="page_389"></span>36.0    0.500000
36.5    0.000000
37.0    0.166667
38.0    0.454545
39.0    0.357143
40.0    0.461538
Name: Survived, dtype: float64</span></code></pre>
<p class="noindent">So it looks like, say, 37-year-olds survived at a rate of 1<em>/</em>6 ≈ 16<em>.</em>7%.</p>
<p class="indent">As noted in <a href="ch11.xhtml#sec11_3_2">Section 11.3.2</a>, a scatter plot is a great way to get a broad overview of the data:</p>
<p class="codelink"><a href="ch11_images.xhtml#f389-01" id="f389-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> fig, ax <span class="pd_gray">=</span> plt<span class="pd_gray">.</span>subplots()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> ax<span class="pd_gray">.</span>scatter(passenger_ages, survival_rate)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent">The result is shown in <a href="ch11.xhtml#ch11fig33">Figure 11.33</a>.</p>
<figure class="image-c" id="ch11fig33">
<img src="graphics/11fig33.jpg" alt="images" width="601" height="445"/>
<figcaption>
<p class="title-f"><strong>Figure 11.33:</strong> A scatter plot of <em>Titanic</em> survival rates by age.</p>
</figcaption>
</figure>
<p class="indent"><span epub:type="pagebreak" id="page_390"></span>It appears in <a href="ch11.xhtml#ch11fig33">Figure 11.33</a> that there is a general downward trend, in agreement with the bar chart in <a href="ch11.xhtml#ch11fig31">Figure 11.31</a>. We can quantify this trend using the <span class="pd_green-d"><code><strong>LinearRegression</strong></code></span> model from scikit-learn (<a href="ch11.xhtml#ch11list23">Listing 11.23</a>).<sup><a id="fn11_32a" href="ch11.xhtml#fn11_32">32</a></sup></p>
<p class="listing" id="ch11list23"><strong>Listing 11.23:</strong> Importing a linear regression model.</p>
<p class="codelink"><a href="ch11_images.xhtml#f390-01" id="f390-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>sklearn.linear_model</strong></span> <span class="pd_green"><strong>import</strong></span> LinearRegression</code></pre>
</div>
<p class="footnote"><a id="fn11_32" href="ch11.xhtml#fn11_32a">32.</a> SciPy also has a linear regression function (<span class="pd_green-d"><code><strong>scipy.stats.linregress</strong></code></span>), but in this section we use the one in scikit-learn in order to unify the treatment with the more advanced models in <a href="ch11.xhtml#sec11_7_2">Section 11.7.2</a>.</p>
<p class="indent">We’ll now define variables <span class="pd_green-d"><code><strong>X</strong></code></span> and <span class="pd_green-d"><code><strong>Y</strong></code></span> based on the ages and survival rates as inputs to the scikit-learn regression model.<sup><a id="fn11_33a" href="ch11.xhtml#fn11_33">33</a></sup> The input format expected by scikit-learn models is an array of one-dimensional arrays for <span class="pd_green-d"><code><strong>X</strong></code></span> and a regular NumPy ndarray for <span class="pd_green-d"><code><strong>Y</strong></code></span>. The former is exactly the format created using the <span class="pd_green-d"><code><strong>reshape((-1, 1))</strong></code></span> method in <a href="ch11.xhtml#sec11_2_2">Section 11.2.2</a> (<a href="ch11.xhtml#ch11list4">Listing 11.4</a>):</p>
<p class="footnote"><a id="fn11_33" href="ch11.xhtml#fn11_33a">33.</a> Conventions for the capitalization of regression variables are rather complicated; see here (<a href="https://stats.stackexchange.com/questions/389395/why-uppercase-for-x-and-lowercase-for-y">https://stats.stackexchange.com/questions/389395/why-uppercase-for-x-and-lowercase-for-y</a>) for more.</p>
<p class="codelink"><a href="ch11_images.xhtml#f390-02" id="f390-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> X <span class="pd_gray">=</span> np<span class="pd_gray">.</span>array(passenger_ages)<span class="pd_gray">.</span>reshape((<span class="pd_gray">-1</span>, <span class="pd_gray">1</span>))
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> X[:<span class="pd_gray">10</span>]    <span class="pd_blue1"><em># Look at the first 10 ages as a reality check.</em></span>
<span class="pd_green-d">array([[0.42],
       [0.67],
       [0.75],
       [0.83],
       [0.92],
       [1.  ],
       [2.  ],
       [3.  ],
       [4.  ],
       [5.  ]])</span></code></pre>
<p class="noindent">Defining <span class="pd_green-d"><code><strong>Y</strong></code></span>, meanwhile, is much more straightforward:</p>
<p class="codelink"><a href="ch11_images.xhtml#f390-03" id="f390-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> Y <span class="pd_gray">=</span> np<span class="pd_gray">.</span>array(survival_rate)</code></pre>
<p class="indent">At this point, we’re ready to use a linear regression to find the best fit of the model to the data:</p>
<span epub:type="pagebreak" id="page_391"></span>
<p class="codelink"><a href="ch11_images.xhtml#f391-01" id="f391-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> model <span class="pd_gray">=</span> LinearRegression()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> model<span class="pd_gray">.</span>fit(X, Y)
<span class="pd_green-d">LinearRegression()</span></code></pre>
<p class="noindent">The results of this calculation include the coefficient of determination, also called <em>R</em><sup>2</sup> (for technical reasons), which is the square of the Pearson correlation coefficient and can take any value between −1 and 1, with 1 being perfect correlation and −1 being perfect anti-correlation. <em>R</em><sup>2</sup> is available as the <span class="pd_green-d"><code><strong>score()</strong></code></span> of the model:</p>
<p class="codelink"><a href="ch11_images.xhtml#f391-02" id="f391-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> model<span class="pd_gray">.</span>score(X, Y)    <span class="pd_blue1"><em># coefficient of determination R^2</em></span>
<span class="pd_green-d">0.13539675574075116</span></code></pre>
<p class="noindent">An <em>R</em><sup>2</sup> value of 0<em>.</em>135 is small but not negligible, though it’s important to bear in mind the difficulty of interpreting <em>R</em><sup>2</sup>.</p>
<p class="indent">We can get a visual indication of the fit by plotting the regression line itself. The slope and <em>y</em>-intercept of the line are available via the <span class="pd_green-d"><code><strong>coef_</strong></code></span> and <span class="pd_green-d"><code><strong>intercept_</strong></code></span> attributes of the model:</p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> m <span class="pd_gray">=</span> model<span class="pd_gray">.</span>coef_
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> b <span class="pd_gray">=</span> model<span class="pd_gray">.</span>intercept_</code></pre>
<p class="noindent">Here the trailing underscores on the names are a scikit-learn convention for attributes that are available only after the model has been applied using <span class="pd_green-d"><code><strong>model.score()</strong></code></span>.</p>
<p class="indent">We’ve named the slope and intercept using the standard names <span class="pd_green-d"><code><strong>m</strong></code></span> and <span class="pd_green-d"><code><strong>b</strong></code></span> for describing a line in the <em>xy</em>-plane:</p>
<p class="equ"><math xmlns:m="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtable><mtr><mtd><mi>y</mi><mo>=</mo><mi>m</mi><mi>x</mi><mo>+</mo><mi>b</mi></mtd><mtd><mi>equation of a line.</mi></mtd></mtr></mtable></mrow></mrow></math></p>
<p class="noindent">We can combine a plot of this line with the scatter plot from <a href="ch11.xhtml#ch11fig33">Figure 11.33</a> to visualize the fit (included here without the REPL prompt for ease of copying):</p>
<p class="codelink"><a href="ch11_images.xhtml#f391-03" id="f391-03a">Click here to view code image</a></p>
<pre class="pre1"><code>fig, ax <span class="pd_gray">=</span> plt<span class="pd_gray">.</span>subplots()
ax<span class="pd_gray">.</span>scatter(passenger_ages, survival_rate)
ax<span class="pd_gray">.</span>plot(passenger_ages, m <span class="pd_gray">*</span> passenger_ages <span class="pd_gray">+</span> b, color<span class="pd_gray">=</span><span class="pd_red">"orange"</span>)
ax<span class="pd_gray">.</span>set_xlabel(<span class="pd_red">"Age"</span>)
ax<span class="pd_gray">.</span>set_ylabel(<span class="pd_red">"Survival Rate"</span>)
ax<span class="pd_gray">.</span>set_title(<span class="pd_red">"Titanic survival rates by age"</span>)
plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_392"></span>The result is shown in <a href="ch11.xhtml#ch11fig34">Figure 11.34</a>. As indicated by the modest <em>R</em><sup>2</sup> value, the fit in <a href="ch11.xhtml#ch11fig34">Figure 11.34</a> is OK but not great. Clearly, the correlation with age is far from perfect, and we found in <a href="ch11.xhtml#sec11_6">Section 11.6</a> that both sex and passenger class had a significant effect on survival rates. We’ll investigate these relationships further with more sophisticated learning models in <a href="ch11.xhtml#sec11_7_2">Section 11.7.2</a>.</p>
<figure class="image-c" id="ch11fig34">
<img src="graphics/11fig34.jpg" alt="images" width="601" height="475"/>
<figcaption>
<p class="title-f"><strong>Figure 11.34:</strong> Adding a regression line (and labels) to <a href="ch11.xhtml#ch11fig33">Figure 11.33</a>.</p>
</figcaption>
</figure>
</section>
<section>
<h4 class="h4" id="sec11_7_2">11.7.2 Machine-Learning Models</h4>
<p class="noindent">In <a href="ch11.xhtml#sec11_6">Section 11.6</a>, we used pandas to discover an association between <em>Titanic</em> survival rates and the key variables of passenger class (<span class="pd_green-d"><code><strong>"Pclass"</strong></code></span>), sex (<span class="pd_green-d"><code><strong>"Sex"</strong></code></span>), and age (<span class="pd_green-d"><code><strong>"Age"</strong></code></span>). In <a href="ch11.xhtml#sec11_7_1">Section 11.7.1</a>, we calculated a linear regression for survival rates as a function of age, but the predictive capability of the linear regression model was fairly modest. In <span epub:type="pagebreak" id="page_393"></span>this section, we’ll look at significantly more sophisticated learning models that yield correspondingly better predictions.<sup><a id="fn11_34a" href="ch11.xhtml#fn11_34">34</a></sup></p>
<p class="footnote"><a id="fn11_34" href="ch11.xhtml#fn11_34a">34.</a> The analysis here is based in part on the article “Predicting the Survival of Titanic Passengers”, which uses data from the popular Machine Learning from Disaster contest run by machine-learning site Kaggle (a subsidiary of Google). The Kaggle dataset includes both training and test data; the purpose of the contest is to use the training data to train models and then submit predictions based on the test data. Unfortunately, this step isn’t clear in “Predicting the Survival of Titanic Passengers”, which uses scikit-learn’s <span class="pd_green-d"><code><strong>predict()</strong></code></span> method to calculate predictions but then doesn’t do anything with them. For participants in the contest, those predictions would be used to create the submissions required by Kaggle.</p>
<p class="indent">As in previous sections, we’ll import the necessary packages and create the necessary DataFrame (shown without the REPL prompt for ease of copying):</p>
<p class="codelink"><a href="ch11_images.xhtml#f393-01" id="f393-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_green"><strong>import</strong></span> <span class="pd_nila"><strong>numpy</strong></span> <span class="pd_green"><strong>as</strong></span> <span class="pd_nila"><strong>np</strong></span>
<span class="pd_green"><strong>import</strong></span> <span class="pd_nila"><strong>pandas</strong></span> <span class="pd_green"><strong>as</strong></span> <span class="pd_nila"><strong>pd</strong></span>
<span class="pd_green"><strong>import</strong></span> <span class="pd_nila"><strong>matplotlib.pyplot</strong></span> <span class="pd_green"><strong>as</strong></span> <span class="pd_nila"><strong>plt</strong></span>


URL <span class="pd_gray">=</span> <span class="pd_red">"https://learnenough.s3.amazonaws.com/titanic.csv"</span>
titanic <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>read_csv(URL)</code></pre>
<p class="indent">There is a large variety of different models supported by scikit-learn that we might try. A detailed discussion of such models is beyond the scope of this tutorial, but here is a selection of the models we’ll be considering in this section with links for more information:</p>
<ul class="sq">
<li><p class="bull">Logistic Regression (<a href="https://stats.stackexchange.com/questions/389395/whyuppercase-for-x-and-lowercase-for-y">https://stats.stackexchange.com/questions/389395/whyuppercase-for-x-and-lowercase-for-y</a>)</p></li>
<li><p class="bull">Naive Bayes (<a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">https://en.wikipedia.org/wiki/Naive_Bayes_classifier</a>)</p></li>
<li><p class="bull">Perceptron (<a href="https://en.wikipedia.org/wiki/Perceptron">https://en.wikipedia.org/wiki/Perceptron</a>)</p></li>
<li><p class="bull">Decision Tree (<a href="https://en.wikipedia.org/wiki/Decision_tree">https://en.wikipedia.org/wiki/Decision_tree</a>)</p></li>
<li><p class="bull">Random Forest (<a href="https://en.wikipedia.org/wiki/Random_forest">https://en.wikipedia.org/wiki/Random_forest</a>)</p></li>
</ul>
<p class="noindent">These models were chosen as representative samples of different kinds of candidate algorithms. The only exception is Random Forest, which in the case of our dataset will turn out to be equivalent to Decision Tree, but was retained because “Random Forest” sounds really cool. (In all seriousness, seeing when and by how much Random Forest differs from Decision Tree is discussed in an exercise (<a href="ch11.xhtml#sec11_7_4">Section 11.7.4</a>).)</p>
$$$$$$$$$$$$$$$$$$
<p class="indent"><span epub:type="pagebreak" id="page_394"></span>To use the various models on our training DataFrame, we first need to import them from scikit-learn, which is available via the <span class="pd_green-d"><code><strong>sklearn</strong></code></span> package (<a href="ch11.xhtml#ch11list24">Listing 11.24</a>).</p>
<p class="listing" id="ch11list24"><strong>Listing 11.24:</strong> Importing learning models.</p>
<p class="codelink"><a href="ch11_images.xhtml#f394-01" id="f394-01a">Click here to view code image</a></p>
<div class="box1">
<pre class="pre"><code><span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>sklearn.linear_model</strong></span> <span class="pd_green"><strong>import</strong></span> LogisticRegression
<span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>sklearn.naive_bayes</strong></span> <span class="pd_green"><strong>import</strong></span> GaussianNB
<span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>sklearn.linear_model</strong></span> <span class="pd_green"><strong>import</strong></span> Perceptron
<span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>sklearn.tree</strong></span> <span class="pd_green"><strong>import</strong></span> DecisionTreeClassifier
<span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>sklearn.ensemble</strong></span> <span class="pd_green"><strong>import</strong></span> RandomForestClassifier</code></pre>
</div>
<p class="noindent">Note the similarity between these import statements and the one used for linear regressions in <a href="ch11.xhtml#sec11_7_1">Section 11.7.1</a> (<a href="ch11.xhtml#ch11list23">Listing 11.23</a>).</p>
<p class="indent">At this point, we need to bring our data into the format required for input into scikit-learn’s learning models. Because we’ve decided to focus on the effects of class, sex, and age on survival rates, our first step is to drop the columns that we won’t be considering. For convenience, we’ll spell out the corresponding column names in a list and then iterate over it, using the pandas <span class="pd_green-d"><code><strong>drop()</strong></code></span> method to drop the corresponding column (which by convention is <span class="pd_green-d"><code><strong>axis=1</strong></code></span>; the default value of <span class="pd_green-d"><code><strong>axis=0</strong></code></span> would try to drop a row instead):</p>
<p class="codelink"><a href="ch11_images.xhtml#f394-02" id="f394-02a">Click here to view code image</a></p>
<pre class="pre1"><code>dropped_columns <span class="pd_gray">=</span> [<span class="pd_red">"PassengerId"</span>, <span class="pd_red">"Name"</span>, <span class="pd_red">"Cabin"</span>, <span class="pd_red">"Embarked"</span>,
                   <span class="pd_red">"SibSp"</span>, <span class="pd_red">"Parch"</span>, <span class="pd_red">"Ticket"</span>, <span class="pd_red">"Fare"</span>]

<span class="pd_green"><strong>for</strong></span> column <span class="pd_pink"><strong>in</strong></span> dropped_columns:
    titanic <span class="pd_gray">=</span> titanic<span class="pd_gray">.</span>drop(column, axis<span class="pd_gray">=1</span>)</code></pre>
<p class="indent">Unlike things like histogram plots, which typically ignore Not Available values like NaNs and NaTs, the learning models will croak if given invalid values. To avoid this unfortunate circumstance, we’ll use the same trick seen in <a href="ch11.xhtml#ch11list20">Listing 11.20</a> and redefine our DataFrames to include only values that are <em>not</em> Not Available using the <span class="pd_green-d"><code><strong>notna()</strong></code></span> method (seen before in <a href="ch11.xhtml#ch11list20">Listing 11.20</a>):</p>
<p class="codelink"><a href="ch11_images.xhtml#f394-03" id="f394-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_green"><strong>for</strong></span> column <span class="pd_pink"><strong>in</strong></span> [<span class="pd_red">"Age"</span>, <span class="pd_red">"Sex"</span>, <span class="pd_red">"Pclass"</span>]:
    titanic <span class="pd_gray">=</span> titanic[titanic[column<span class="pd_gray">]</span>.notna()]</code></pre>
<p class="indent">Another cause of model errors is raw categorical values like <span class="pd_green-d"><code><strong>"male"</strong></code></span> and <span class="pd_green-d"><code><strong>"female"</strong></code></span>, which the models don’t know how to handle. In order to fix this, we’ll <span epub:type="pagebreak" id="page_395"></span>associate each of these categories to a number using the pandas <span class="pd_green-d"><code><strong>map()</strong></code></span> method we saw in <a href="ch11.xhtml#ch11list12">Listing 11.12</a>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f395-01" id="f395-01a">Click here to view code image</a></p>
<pre class="pre1"><code>sexes <span class="pd_gray">=</span> {<span class="pd_red">"male"</span>: <span class="pd_gray">0</span>, <span class="pd_red">"female"</span>: <span class="pd_gray">1</span>}
titanic[<span class="pd_red">"Sex"</span>] <span class="pd_gray">=</span> titanic[<span class="pd_red">"Sex"</span>]<span class="pd_gray">.</span>map(sexes)</code></pre>
<p class="indent">If <span class="pd_green-d"><code><strong>"Pclass"</strong></code></span> were represented using strings like <span class="pd_green-d"><code><strong>"first"</strong></code></span>, <span class="pd_green-d"><code><strong>"second"</strong></code></span>, and <span class="pd_green-d"><code><strong>"third"</strong></code></span>, we would have to do something similar for that variable, but luckily it’s already represented using the integers <span class="pd_green-d"><code><strong>1</strong></code></span>, <span class="pd_green-d"><code><strong>2</strong></code></span>, and <span class="pd_green-d"><code><strong>3</strong></code></span>. That means we’re ready to move on to the next step, which is to prepare our data. The independent variables are class, sex, and age, while the dependent variable is survival rate. Following the usual convention, we’ll call these <span class="pd_green-d"><code><strong>X</strong></code></span> and <span class="pd_green-d"><code><strong>Y</strong></code></span>, respectively:</p>
<p class="codelink"><a href="ch11_images.xhtml#f395-02" id="f395-02a">Click here to view code image</a></p>
<pre class="pre1"><code>X <span class="pd_gray">=</span> titanic<span class="pd_gray">.</span>drop(<span class="pd_red">"Survived"</span>, axis<span class="pd_gray">=1</span>)
Y <span class="pd_gray">=</span> titanic[<span class="pd_red">"Survived"</span>]</code></pre>
<p class="noindent">Note that we’ve dropped the dependent <span class="pd_green-d"><code><strong>"Survived"</strong></code></span> column from the <span class="pd_green-d"><code><strong>X</strong></code></span> training variable because that’s exactly what we’re trying to predict.</p>
<p class="indent">Before applying the learning model algorithms, let’s take a look at everything just to make sure the data looks sensible:</p>
<p class="codelink"><a href="ch11_images.xhtml#f395-03" id="f395-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_green">print</span>(X<span class="pd_gray">.</span>head(), <span class="pd_red">"</span><span class="pd_orange"><strong>\n</strong></span><span class="pd_red">----</span><span class="pd_orange"><strong>\n</strong></span><span class="pd_red">"</span>)
<span class="pd_green">print</span>(Y<span class="pd_gray">.</span>head(), <span class="pd_red">"</span><span class="pd_orange"><strong>\n</strong></span><span class="pd_red">----</span><span class="pd_orange"><strong>\n</strong></span><span class="pd_red">"</span>)
   Pclass  Sex   Age
<span class="pd_gray">0       3    0  22.0
1       1    1  38.0
2       3    1  26.0
3       1    1  35.0
4       3    0  35.0
----

0    0
1    1
2    1
3    1
4    0</span>
Name: Survived, dtype: int64
<span class="pd_gray">----</span></code></pre>
<p class="noindent">Looks good.</p>
<p class="indent"><span epub:type="pagebreak" id="page_396"></span>The original competition that inspired this example involved supplying training data for creating models, which was then applied to test data not available to participants in the competition. Because this section isn’t part of that competition, we’ll split the given data into separate training and test datasets ourselves. Using such separate datasets helps guard against overfitting, which involves using so many free parameters that the model doesn’t have predictive value beyond the original datasets—as the great John von Neumann once reportedly quipped, “With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.” (We’ll cover a second guard against overfitting called <em>cross-validation</em> as well.)</p>
<p class="indent">The main scikit-learn method for doing a train/test split is called, appropriately enough, <span class="pd_green-d"><code><strong>train_test_split()</strong></code></span>, which returns four values consisting of a training and test variable for each of <span class="pd_green-d"><code><strong>X</strong></code></span> and <span class="pd_green-d"><code><strong>Y</strong></code></span>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f396-01" id="f396-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>sklearn.model_selection</strong></span> <span class="pd_green"><strong>import</strong></span> train_test_split
(X_train, X_test, Y_train, Y_test) <span class="pd_gray">=</span> train_test_split(X, Y, random_state<span class="pd_gray">=1</span>)</code></pre>
<p class="noindent">Because <span class="pd_green-d"><code><strong>train_test_split()</strong></code></span> shuffles the data before doing the split, we’ve set the <span class="pd_green-d"><code><strong>random_state</strong></code></span> option so that your results will be consistent with those shown in the text.</p>
<p class="indent">At this point, we’re ready to try the various models out on the training data and see how accurate their fits are when applied to the test data. Our strategy is to define an instance of each of the models imported in <a href="ch11.xhtml#ch11list24">Listing 11.24</a>, calculate the <span class="pd_green-d"><code><strong>fit()</strong></code></span> on the training data, and then look at the <span class="pd_green-d"><code><strong>score()</strong></code></span> of the model on the test data. We’ll then compare the scores to compare the accuracy of the models.</p>
<p class="indent">First up is Logistic Regression:</p>
<p class="codelink"><a href="ch11_images.xhtml#f396-02" id="f396-02a">Click here to view code image</a></p>
<pre class="pre1"><code>logreg <span class="pd_gray">=</span> LogisticRegression()
logreg<span class="pd_gray">.</span>fit(X_train, Y_train)
accuracy_logreg <span class="pd_gray">=</span> logreg<span class="pd_gray">.</span>score(X_test, Y_test)</code></pre>
<p class="noindent">Next is (Gaussian) Naive Bayes:</p>
<p class="codelink"><a href="ch11_images.xhtml#f396-03" id="f396-03a">Click here to view code image</a></p>
<pre class="pre1"><code>naive_bayes <span class="pd_gray">=</span> GaussianNB()
naive_bayes<span class="pd_gray">.</span>fit(X_train, Y_train)
accuracy_naive_bayes <span class="pd_gray">=</span> naive_bayes<span class="pd_gray">.</span>score(X_test, Y_test)</code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_397"></span>Then Perceptron:</p>
<p class="codelink"><a href="ch11_images.xhtml#f397-01" id="f397-01a">Click here to view code image</a></p>
<pre class="pre1"><code>perceptron <span class="pd_gray">=</span> Perceptron()
perceptron<span class="pd_gray">.</span>fit(X_train, Y_train)
accuracy_perceptron <span class="pd_gray">=</span> perceptron<span class="pd_gray">.</span>score(X_test, Y_test)</code></pre>
<p class="noindent">Then Decision Tree:</p>
<p class="codelink"><a href="ch11_images.xhtml#f397-02" id="f397-02a">Click here to view code image</a></p>
<pre class="pre1"><code>decision_tree <span class="pd_gray">=</span> DecisionTreeClassifier()
decision_tree<span class="pd_gray">.</span>fit(X_train, Y_train)
accuracy_decision_tree <span class="pd_gray">=</span> decision_tree<span class="pd_gray">.</span>score(X_test, Y_test)</code></pre>
<p class="noindent">And finally Random Forest (using a <span class="pd_green-d"><code><strong>random_state</strong></code></span> option as with <span class="pd_green-d"><code><strong>train_test_-split()</strong></code></span> to obtain consistent results):</p>
<p class="codelink"><a href="ch11_images.xhtml#f397-03" id="f397-03a">Click here to view code image</a></p>
<pre class="pre1"><code>random_forest <span class="pd_gray">=</span> RandomForestClassifier(random_state<span class="pd_gray">=1</span>)
random_forest<span class="pd_gray">.</span>fit(X_train, Y_train)
accuracy_random_forest <span class="pd_gray">=</span> random_forest<span class="pd_gray">.</span>score(X_test, Y_test)</code></pre>
<p class="indent">Let’s make a DataFrame to hold and display the results (again omitting the prompt for easier copying):</p>
<p class="codelink"><a href="ch11_images.xhtml#f397-04" id="f397-04a">Click here to view code image</a></p>
<pre class="pre1"><code>results <span class="pd_gray">=</span> pd<span class="pd_gray">.</span>DataFrame({
    <span class="pd_red">"Model"</span>: [<span class="pd_red">"Logistic Regression"</span>, <span class="pd_red">"Naive Bayes"</span>, <span class="pd_red">"Perceptron"</span>,
              <span class="pd_red">"Decision Tree"</span>, <span class="pd_red">"Random Forest"</span>],
    <span class="pd_red">"Score"</span>: [accuracy_logreg, accuracy_naive_bayes, accuracy_perceptron,
              accuracy_decision_tree, accuracy_random_forest]})
result_df <span class="pd_gray">=</span> results<span class="pd_gray">.</span>sort_values(by<span class="pd_gray">=</span><span class="pd_red">"Score"</span>, ascending<span class="pd_gray">=</span><span class="pd_green"><strong>False</strong></span>)
result_df <span class="pd_gray">=</span> result_df<span class="pd_gray">.</span>set_index(<span class="pd_red">"Score"</span>)
result_df</code></pre>
<p class="noindent">The result appears in <a href="ch11.xhtml#ch11list25">Listing 11.25</a>.</p>
<p class="listing" id="ch11list25"><strong>Listing 11.25:</strong> The model accuracy results.</p>
<div class="box1">
<pre class="pre"><code>                        Model
Score
<span class="pd_gray">0.854749</span>  Decision Tree
<span class="pd_gray">0.854749</span>  Random Forest
<span class="pd_gray">0.787709</span>  Logistic Regression
<span class="pd_gray">0.770950</span>  Naive Bayes
<span class="pd_gray">0.743017</span>  Perceptron</code></pre>
</div>
<p class="noindent"><span epub:type="pagebreak" id="page_398"></span>We see that Decision Tree and Random Forest are tied for the most accurate score, followed by Logistic Regression and Naive Bayes neck-and-neck, with Perceptron bringing up the rear. The models are close enough, though, that different values for <span class="pd_green-d"><code><strong>random_state</strong></code></span> could easily affect their order (<a href="ch11.xhtml#sec11_7_4">Section 11.7.4</a>).</p>
<p class="indent">Once we’ve performed the <span class="pd_green-d"><code><strong>fit()</strong></code></span>, we can look at how important each factor was in determining the results of the model. For example, for the Random Forest model, the importances are as follows:</p>
<p class="codelink"><a href="ch11_images.xhtml#f398-01" id="f398-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> random_forest<span class="pd_gray">.</span>feature_importances_
<span class="pd_green-d">array([0.16946036, 0.35821155, 0.47232809])</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> X_train<span class="pd_gray">.</span>columns
<span class="pd_green-d">Index(['Pclass', 'Sex', 'Age'], dtype='object')</span></code></pre>
<p class="noindent">Comparing the columns to the importances, we see that <span class="pd_green-d"><code><strong>"Age"</strong></code></span> was the biggest factor, followed closely by <span class="pd_green-d"><code><strong>"Sex"</strong></code></span>, with <span class="pd_green-d"><code><strong>"Pclass"</strong></code></span> being a distant third (half as important as the second-highest factor). We can visualize the result as a bar chart as well:</p>
<p class="codelink"><a href="ch11_images.xhtml#f398-02" id="f398-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_gray">&gt;&gt;&gt;</span> fig, ax <span class="pd_gray">=</span> plt<span class="pd_gray">.</span>subplots()
<span class="pd_gray">&gt;&gt;&gt;</span> ax<span class="pd_gray">.</span>bar(X_train<span class="pd_gray">.</span>columns, random_forest<span class="pd_gray">.</span>feature_importances_)
<span class="pd_gray">&lt;</span>BarContainer <span class="pd_green">object</span> of <span class="pd_gray">3</span> artists<span class="pd_gray">&gt;
&gt;&gt;&gt;</span> plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent">Previous examples of <span class="pd_green-d"><code><strong>bar()</strong></code></span> have gone through the pandas interface, but here we see that Matplotlib also supports bar charts directly. (This isn’t surprising since, as noted in <a href="ch11.xhtml#sec11_4">Section 11.4</a>, pandas uses Matplotlib under the hood.) The result appears in <a href="ch11.xhtml#ch11fig35">Figure 11.35</a>.</p>
<figure class="image-c" id="ch11fig35">
<img src="graphics/11fig35.jpg" alt="images" width="601" height="448"/>
<figcaption>
<p class="title-f"><strong>Figure 11.35:</strong> The importance of each factor in <em>Titanic</em> survival rates.</p>
</figcaption>
</figure>
<section>
<h5 class="h5" id="sec11_1_1_5">Cross-Validation</h5>
<p class="noindent">As previously noted, we split the data into training and test datasets as one guard against overfitting. Another common technique to avoid “fitting an elephant” (per von Neumann’s quip) is known as <em>cross-validation</em>. The basic idea is to artificially break the original training data into new training and test datasets, train the model on the training data, and then use the model to predict the test data. If doing this on several different random choices for training and test subsets yields fairly consistent results, we can be more confident that the model actually works.</p>
<p class="indent">Because this is such a common technique, scikit-learn comes with a predefined routine for performing cross-validations called <span class="pd_green-d"><code><strong>cross_val_score</strong></code></span>:</p>
<span epub:type="pagebreak" id="page_399"></span>
<p class="codelink"><a href="ch11_images.xhtml#f399-01" id="f399-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_gray">&gt;&gt;&gt;</span> <span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>sklearn.model_selection</strong></span> <span class="pd_green"><strong>import</strong></span> cross_val_score</code></pre>
<p class="noindent">This method implements so-called <em>K-fold cross-validation</em>, which involves breaking the data into <em>K</em> pieces, or “folds”, using <em>K</em>−1 folds to train the model, and then predicting the values of the final fold to assess accuracy. The default value is <span class="pd_green-d"><code><strong>5</strong></code></span>, which is fine for our purposes, so we need only pass the function the classifier instance and the training data. We’ll use Random Forest since it was tied for first (and, as previously noted, has an especially cool name):</p>
<p class="codelink"><a href="ch11_images.xhtml#f399-02" id="f399-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> random_forest <span class="pd_gray">=</span> RandomForestClassifier(random_state<span class="pd_gray">=1</span>)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> scores <span class="pd_gray">=</span> cross_val_score(random_forest, X, Y)
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> scores
<span class="pd_green-d">array([0.75524476, 0.8041958, 0.82517483, 0.83216783, 0.83098592])</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> scores<span class="pd_gray">.</span>mean()
<span class="pd_green-d">0.8095538264552349</span>
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> scores<span class="pd_gray">.</span>std()
<span class="pd_green-d">0.028958338744358988</span></code></pre>
<p class="noindent"><span epub:type="pagebreak" id="page_400"></span>With scores that are nearly 81% accurate on average with a standard deviation of just under 3%, we can reasonably conclude the Random Forest model is an accurate predictor of <em>Titanic</em> survival data.</p>
</section>
</section>
<section>
<h4 class="h4" id="sec11_7_3">11.7.3 <em>k</em>-Means Clustering</h4>
<p class="noindent">As a final example, we’ll take a look at a <em>clustering algorithm</em>, which is but one of the many amazing things that scikit-learn can do.<sup><a id="fn11_35a" href="ch11.xhtml#fn11_35">35</a></sup> We’ll start by importing a utility method commonly used when demonstrating clustering algorithms called <span class="pd_green-d"><code><strong>make_blobs()</strong></code></span>, in this case consisting of <span class="pd_green-d"><code><strong>300</strong></code></span> points divided into <span class="pd_green-d"><code><strong>4</strong></code></span> blobs:</p>
<p class="footnote"><a id="fn11_35" href="ch11.xhtml#fn11_35a">35.</a> See <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html">https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html</a> for more details.</p>
<p class="codelink"><a href="ch11_images.xhtml#f400-01" id="f400-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> <span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>sklearn.datasets</strong></span> <span class="pd_green"><strong>import</strong></span> make_blobs
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> X, _ <span class="pd_gray">=</span> make_blobs(n_samples<span class="pd_gray">=300</span>, centers<span class="pd_gray">=4</span>, random_state<span class="pd_gray">=42</span>)</code></pre>
<p class="noindent">Note that we’ve also passed a <span class="pd_green-d"><code><strong>random_state</strong></code></span> parameter, which serves as the seed for the blobs and ensures consistent results (which can vary quite a lot).</p>
<p class="indent">We can see what’s “bloblike” about the data created by <span class="pd_green-d"><code><strong>make_blobs()</strong></code></span> by plotting the second column against the first:</p>
<p class="codelink"><a href="ch11_images.xhtml#f400-02" id="f400-02a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> fig, ax <span class="pd_gray">=</span> plt<span class="pd_gray">.</span>subplots()
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> ax<span class="pd_gray">.</span>scatter(X[:, <span class="pd_gray">0</span>], X[:, <span class="pd_gray">1</span>])
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent">The result appears in <a href="ch11.xhtml#ch11fig36">Figure 11.36</a>.</p>
<figure class="image-c" id="ch11fig36">
<img src="graphics/11fig36.jpg" alt="images" width="600" height="435"/>
<figcaption>
<p class="title-f"><strong>Figure 11.36:</strong> Some random blobs.</p>
</figcaption>
</figure>
<p class="indent">We can find a good fit for the <span class="pd_green-d"><code><strong>4</strong></code></span> blobs using an algorithm called <em>k-means clustering</em>:</p>
<p class="codelink"><a href="ch11_images.xhtml#f400-03" id="f400-03a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_gray">&gt;&gt;&gt;</span> <span class="pd_green"><strong>from</strong></span> <span class="pd_nila"><strong>sklearn.cluster</strong></span> <span class="pd_green"><strong>import</strong></span> KMeans
<span class="pd_gray">&gt;&gt;&gt;</span> kmeans <span class="pd_gray">=</span> KMeans(n_clusters<span class="pd_gray">=4</span>)
<span class="pd_gray">&gt;&gt;&gt;</span> kmeans<span class="pd_gray">.</span>fit(X)</code></pre>
<p class="noindent">Note how similar the steps are to the model fits in <a href="ch11.xhtml#sec11_7_2">Section 11.7.2</a>. We can find the model’s estimate for the center of each cluster using the <span class="pd_green-d"><code><strong>cluster_centers_</strong></code></span> attribute:</p>
<span epub:type="pagebreak" id="page_401"></span>
<p class="codelink"><a href="ch11_images.xhtml#f401-01" id="f401-01a">Click here to view code image</a></p>
<pre class="pre1"><code><span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> centers <span class="pd_gray">=</span> kmeans<span class="pd_gray">.</span>cluster_centers_
<span class="pd_blue"><strong>&gt;&gt;&gt;</strong></span> centers
<span class="pd_green-d">array([[ 4.7182049 ,  2.04179676],</span>
<span class="pd_green-d">       [-8.87357218,  7.17458342],
       [-6.83235205, -6.83045748],
       [-2.70981136,  8.97143336]])</span></code></pre>
<p class="noindent">(Note the same trailing-underscore convention mentioned in <a href="ch11.xhtml#sec11_7_1">Section 11.7.1</a> to indicate an attribute that is defined only after calling <span class="pd_green-d"><code><strong>fit()</strong></code></span>.) The result is an array of points whose meaning we can interpret by plotting the second column against the first as we did with the original blobs:</p>
<p class="codelink"><a href="ch11_images.xhtml#f401-02" id="f401-02a">Click here to view code image</a></p>
<pre class="pre1"><code>fig, ax <span class="pd_gray">=</span> plt<span class="pd_gray">.</span>subplots()
ax<span class="pd_gray">.</span>scatter(X[:, <span class="pd_gray">0</span>], X[:, <span class="pd_gray">1</span>])
centers <span class="pd_gray">=</span> kmeans<span class="pd_gray">.</span>cluster_centers_
ax<span class="pd_gray">.</span>scatter(centers[:, <span class="pd_gray">0</span>], centers[:, <span class="pd_gray">1</span>], s<span class="pd_gray">=200</span>, alpha<span class="pd_gray">=0.9</span>, color<span class="pd_gray">=</span><span class="pd_red">"orange"</span>)
plt<span class="pd_gray">.</span>show()</code></pre>
<p class="noindent">With the larger size, alpha transparency, and orange color, it’s easy to see the estimated centers of the respective clusters on a scatter plot (<a href="ch11.xhtml#ch11fig37">Figure 11.37</a>). The result is an <span epub:type="pagebreak" id="page_402"></span>excellent correspondence between the output of the clustering algorithm and what we expect based on an intuitive notion of “clusters”.</p>
<figure class="image-c" id="ch11fig37">
<img src="graphics/11fig37.jpg" alt="images" width="600" height="437"/>
<figcaption>
<p class="title-f"><strong>Figure 11.37:</strong> Clusters with their predicted centers.</p>
</figcaption>
</figure>
</section>
<section>
<h4 class="h4" id="sec11_7_4">11.7.4 Exercises</h4>
<ol class="number">
<li><p class="number">The <span class="pd_green-d"><code><strong>RandomForestClassifier()</strong></code></span> function takes a keyword argument called <span class="pd_green-d"><code><strong>n_estimators</strong></code></span> that represents the “number of trees in the forest”. According to the documentation, what is the default value for <span class="pd_green-d"><code><strong>n_estimators</strong></code></span>? Use <span class="pd_green-d"><code><strong>random_state=1</strong></code></span>.</p></li>
<li><p class="number">By varying <span class="pd_green-d"><code><strong>n_estimators</strong></code></span> in the call to <span class="pd_green-d"><code><strong>RandomForestClassifier()</strong></code></span>, determine the approximate value where the Random Forest classifier is less accurate than Decision Tree. Use <span class="pd_green-d"><code><strong>random_state=1</strong></code></span>.</p></li>
<li><p class="number">By rerunning the steps in <a href="ch11.xhtml#sec11_7_2">Section 11.7.2</a> using a few different values of <span class="pd_green-d"><code><strong>random_state</strong></code></span>, verify that the ordering is not always the same as shown in <a href="ch11.xhtml#ch11list25">Listing 11.25</a>. <em>Hint</em>: Try values like <span class="pd_green-d"><code><strong>0</strong></code></span>, <span class="pd_green-d"><code><strong>2</strong></code></span>, <span class="pd_green-d"><code><strong>3</strong></code></span>, and <span class="pd_green-d"><code><strong>4</strong></code></span>.</p></li>
<li><p class="number">Repeat the clustering steps in <a href="ch11.xhtml#sec11_7_3">Section 11.7.3</a> using two clusters and eight clusters. Does the algorithm still work well in both cases?</p></li>
</ol>
<span epub:type="pagebreak" id="page_403"></span>
</section>
</section>
<section>
<h3 class="h3" id="sec11_8">11.8 Further Resources and Conclusion</h3>
<p class="noindent">Congratulations—now you <em>really</em> know enough Python to be dangerous! In addition to the core material, you now have a good grounding in some of the most important tools for data science with Python.</p>
<p class="indent">There are a million directions to go from here; here are a few of the possibilities:</p>
<ul class="sq">
<li><p class="bull">The official pandas documentation includes 10 minutes to pandas followed by a large amount of additional tutorial material. The official documentation for NumPy, Matplotlib, and scikit-learn are also excellent resources. Finally, the SciPy and SageMath projects are worth knowing about; Sage in particular includes the ability to do symbolic as well as numerical computations (much like <em>Mathematica</em> or Maple).</p></li>
<li><p class="bull">“Python for Scientific Computing”: Although not for data science per se, this resource covers a lot of the same material needed for the subject. Among other things, “Python for Scientific Computing” was the inspiration for the section using Nobel Prize data (<a href="ch11.xhtml#sec11_5">Section 11.5</a>).</p></li>
<li><p class="bull"><em>Python Data Science Handbook</em> by Jake VanderPlas: This book takes a similar approach to this chapter and is available for free online.</p></li>
<li><p class="bull"><em>Data Science from Scratch</em> by Joel Grus: This book is basically the polar opposite of this chapter, taking a first-principles approach to data science that focuses on the foundational ideas of the subject. This approach is impossible in a space as short as ours but is an excellent way to go if you’re interested in becoming a professional data scientist.</p></li>
<li><p class="bull">Bloom Institute of Technology’s Data Science Course: This online course is aimed at serious students interested in a career in data science.</p></li>
<li><p class="bull"><em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> by Aurélien Géron: A much more advanced introduction to machine learning than <a href="ch11.xhtml#sec11_7">Section 11.7</a>, including Keras, a Python interface to Google’s TensorFlow library.</p></li>
</ul>
<p class="noindent">For completeness, here are the general Python resources recommended in <a href="ch10.xhtml#sec10_6">Section 10.6</a>:</p>
<ul class="sq">
<li><p class="bull">Replit’s 100 days of code: This is a guided introduction to Python programming using Replit’s amazing collaborative browser-based IDE.</p></li>
<li><p class="bull"><span epub:type="pagebreak" id="page_404"></span>Practical Python Programming by Dave Beazley: I’ve long been a huge fan of Beazely’s <em>Python Essential Reference</em> and highly recommend his (free) online course.</p></li>
<li><p class="bull"><em>Learn Python the Hard Way</em> by Zed Shaw: This exercise- and syntax-heavy approach is an excellent complement to the breadth-first, narrative approach taken in this tutorial. <em>Fun fact</em>: Zed Shaw’s “Learn Code the Hard Way” brand was a direct inspiration for “Learn Enough to Be Dangerous” (<a href="https://www.learnenough.com/">https://www.learnenough.com/</a>).</p></li>
<li><p class="bull"><em>Python Crash Course</em> and <em>Automate the Boring Stuff with Python</em> from No Starch Press: Both of these books are good follow-ons to <em>Learn Enough Python to Be Dangerous</em>; the former (by Eric Matthes) has more detailed coverage of Python syntax while the latter (by Al Sweigart) includes a great many applications of Python programming to everyday computer tasks.</p></li>
<li><p class="bull"><em>Captain Code</em> by Ben Forta and Shmuel Forta: Although this book is principally aimed at children, many adult readers have reported enjoying it as well.</p></li>
<li><p class="bull">Finally, for people who want the most solid foundation possible in technical sophistication, Learn Enough All Access (<a href="https://www.learnenough.com/all-access">https://www.learnenough.com/all-access</a>) is a subscription service that has special online versions of all the Learn Enough books and over 40 hours of streaming video tutorials, including <em>Learn Enough Python to Be Dangerous</em>, <em>Learn Enough Ruby to Be Dangerous</em> (<a href="https://www.learnenough.com/ruby">https://www.learnenough.com/ruby</a>), and the full <em>Ruby on Rails Tutorial</em> (<a href="https://www.railstutorial.org/">https://www.railstutorial.org/</a>). We hope you’ll check it out!</p></li>
</ul>
<p class="indent">Those are just a few of the incredible variety of options available to you now that you have learned the basics of Python and have developed your technical sophistication. Good luck!</p>
</section>
</section>
</div>
</div>
</body>
</html>